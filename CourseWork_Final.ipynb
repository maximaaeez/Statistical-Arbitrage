{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Jandsy/ml_finance_imperial/blob/main/Coursework/CourseWork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSIsUTeyXNr_"
   },
   "source": [
    "# **<center>Machine Learning and Finance </center>**\n",
    "\n",
    "\n",
    "## <center> CourseWork 2024 - StatArb </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBDj0uD44dQL"
   },
   "source": [
    "\n",
    "In this coursework, you will delve into and replicate selected elements of the research detailed in the paper **[End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture](https://arxiv.org/pdf/2402.08233.pdf)**. **However, we will not reproduce the entire study.**\n",
    "\n",
    "## Overview\n",
    "\n",
    "This study redefines Statistical Arbitrage (StatArb) by combining Autoencoder architectures and policy learning to generate trading strategies. Traditionally, StatArb involves finding the mean of a synthetic asset through classical or PCA-based methods before developing a mean reversion strategy. However, this paper proposes a data-driven approach using an Autoencoder trained on US stock returns, integrated into a neural network representing portfolio trading policies to output portfolio allocations directly.\n",
    "\n",
    "\n",
    "## Coursework Goal\n",
    "\n",
    "This coursework will replicate these results, providing hands-on experience in implementing and evaluating this innovative end-to-end policy learning Autoencoder within financial trading strategies.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- [Data Preparation and Exploration](#Data-Preparation-and-Exploration)\n",
    "- [Fama French Analysis](#Fama-French-Analysis)\n",
    "- [PCA Analysis](#PCA-Analysis)\n",
    "- [Ornstein Uhlenbeck](#Ornstein-Uhlenbeck)\n",
    "- [Autoencoder Analysis](#Autoencoder-Analysis)\n",
    "\n",
    "\n",
    "\n",
    "**Description:**\n",
    "The Coursework is graded on a 100 point scale and is divided into five  parts. Below is the mark distribution for each question:\n",
    "\n",
    "| **Problem**  | **Question**          | **Number of Marks** |\n",
    "|--------------|-----------------------|---------------------|\n",
    "| **Part A**   | Question 1            | 4                   |\n",
    "|              | Question 2            | 1                   |\n",
    "|              | Question 3            | 3                   |\n",
    "|              | Question 4            | 3                   |\n",
    "|              | Question 5            | 1                   |\n",
    "|              | Question 6            | 3                   |\n",
    "|**Part  B**    | Question 7           | 1                   |\n",
    "|              | Question 8            | 5                   |\n",
    "|              | Question 9            | 4                   |\n",
    "|              | Question 10           | 5                   |\n",
    "|              | Question 11           | 2                   |\n",
    "|              | Question 12           | 3                   |\n",
    "|**Part  C**    | Question 13          | 3                   |\n",
    "|              | Question 14           | 1                   |\n",
    "|              | Question 15           | 3                   |\n",
    "|              | Question 16           | 2                   |\n",
    "|              | Question 17           | 7                   |\n",
    "|              | Question 18           | 6                   |\n",
    "|              | Question 19           | 3                   |\n",
    "|  **Part  D** | Question 20           | 3                   |\n",
    "|              | Question 21           | 5                   |\n",
    "|              | Question 22           | 2                   |\n",
    "|  **Part  E** | Question 23           | 2                   |\n",
    "|              | Question 24           | 1                   |\n",
    "|              | Question 25           | 3                   |\n",
    "|              | Question 26           | 10                  |\n",
    "|              | Question 27           | 1                   |\n",
    "|              | Question 28           | 3                   |\n",
    "|              | Question 29           | 3                   |\n",
    "|              | Question 30           | 7                   |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Please read the questions carefully and do your best. Good luck!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "\n",
    "\n",
    "## 1. Data Preparation and Exploration\n",
    "Collect, clean, and prepare US stock return data for analysis.\n",
    "\n",
    "## 2. Fama French Analysis\n",
    "Utilize Fama French Factors to isolate the idiosyncratic components of stock returns, differentiating them from market-wide effects. This analysis helps in understanding the unique characteristics of individual stocks relative to broader market trends.\n",
    "\n",
    "## 3. PCA Analysis\n",
    "Employ Principal Component Analysis (PCA) to identify hidden structures and reduce dimensionality in the data. This method helps in extracting significant patterns that might be obscured in high-dimensional datasets.\n",
    "\n",
    "## 4. Ornstein-Uhlenbeck Process\n",
    "Analyze mean-reverting behavior in stock prices using the Ornstein-Uhlenbeck process. This stochastic process is useful for modeling and forecasting based on the assumption that prices will revert to a long-term mean.\n",
    "\n",
    "## 5. Building a Basic Autoencoder Model\n",
    "Construct and train a standard Autoencoder to extract residual idiosyncratic risk.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFU9ckGplGDf"
   },
   "source": [
    "# Data Preparation and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiqyU4DQlxsV"
   },
   "source": [
    "\n",
    "---\n",
    "<font color=green>Q1: (4 Marks)</font>\n",
    "<br><font color='green'>\n",
    "Write a Python function that accepts a URL parameter and retrieves the NASDAQ-100 companies and their ticker symbols by scraping the relevant Wikipedia page using **[Requests](https://pypi.org/project/requests/)** and **[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)**. Your function should return the data as a list of tuples, with each tuple containing the company name and its ticker symbol. Then, call your function with the appropriate Wikipedia page URL and print the data in a 'Company: Ticker' format.\n",
    "\n",
    "</font>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adobe Inc.: ADBE\n",
      "ADP: ADP\n",
      "Airbnb: ABNB\n",
      "Alphabet Inc. (Class A): GOOGL\n",
      "Alphabet Inc. (Class C): GOOG\n",
      "Amazon: AMZN\n",
      "Advanced Micro Devices Inc.: AMD\n",
      "American Electric Power: AEP\n",
      "Amgen: AMGN\n",
      "Analog Devices: ADI\n",
      "Ansys: ANSS\n",
      "Apple Inc.: AAPL\n",
      "Applied Materials: AMAT\n",
      "ASML Holding: ASML\n",
      "AstraZeneca: AZN\n",
      "Atlassian: TEAM\n",
      "Autodesk: ADSK\n",
      "Baker Hughes: BKR\n",
      "Biogen: BIIB\n",
      "Booking Holdings: BKNG\n",
      "Broadcom Inc.: AVGO\n",
      "Cadence Design Systems: CDNS\n",
      "CDW Corporation: CDW\n",
      "Charter Communications: CHTR\n",
      "Cintas: CTAS\n",
      "Cisco: CSCO\n",
      "Coca-Cola Europacific Partners: CCEP\n",
      "Cognizant: CTSH\n",
      "Comcast: CMCSA\n",
      "Constellation Energy: CEG\n",
      "Copart: CPRT\n",
      "CoStar Group: CSGP\n",
      "Costco: COST\n",
      "CrowdStrike: CRWD\n",
      "CSX Corporation: CSX\n",
      "Datadog: DDOG\n",
      "DexCom: DXCM\n",
      "Diamondback Energy: FANG\n",
      "Dollar Tree: DLTR\n",
      "DoorDash: DASH\n",
      "Electronic Arts: EA\n",
      "Exelon: EXC\n",
      "Fastenal: FAST\n",
      "Fortinet: FTNT\n",
      "GE HealthCare: GEHC\n",
      "Gilead Sciences: GILD\n",
      "GlobalFoundries: GFS\n",
      "Honeywell: HON\n",
      "Idexx Laboratories: IDXX\n",
      "Illumina, Inc.: ILMN\n",
      "Intel: INTC\n",
      "Intuit: INTU\n",
      "Intuitive Surgical: ISRG\n",
      "Keurig Dr Pepper: KDP\n",
      "KLA Corporation: KLAC\n",
      "Kraft Heinz: KHC\n",
      "Lam Research: LRCX\n",
      "Linde plc: LIN\n",
      "Lululemon: LULU\n",
      "Marriott International: MAR\n",
      "Marvell Technology: MRVL\n",
      "MercadoLibre: MELI\n",
      "Meta Platforms: META\n",
      "Microchip Technology: MCHP\n",
      "Micron Technology: MU\n",
      "Microsoft: MSFT\n",
      "Moderna: MRNA\n",
      "Mondelēz International: MDLZ\n",
      "MongoDB Inc.: MDB\n",
      "Monster Beverage: MNST\n",
      "Netflix: NFLX\n",
      "Nvidia: NVDA\n",
      "NXP: NXPI\n",
      "O'Reilly Automotive: ORLY\n",
      "Old Dominion Freight Line: ODFL\n",
      "Onsemi: ON\n",
      "Paccar: PCAR\n",
      "Palo Alto Networks: PANW\n",
      "Paychex: PAYX\n",
      "PayPal: PYPL\n",
      "PDD Holdings: PDD\n",
      "PepsiCo: PEP\n",
      "Qualcomm: QCOM\n",
      "Regeneron: REGN\n",
      "Roper Technologies: ROP\n",
      "Ross Stores: ROST\n",
      "Sirius XM: SIRI\n",
      "Starbucks: SBUX\n",
      "Synopsys: SNPS\n",
      "Take-Two Interactive: TTWO\n",
      "T-Mobile US: TMUS\n",
      "Tesla, Inc.: TSLA\n",
      "Texas Instruments: TXN\n",
      "The Trade Desk: TTD\n",
      "Verisk: VRSK\n",
      "Vertex Pharmaceuticals: VRTX\n",
      "Walgreens Boots Alliance: WBA\n",
      "Warner Bros. Discovery: WBD\n",
      "Workday, Inc.: WDAY\n",
      "Xcel Energy: XEL\n",
      "Zscaler: ZS\n"
     ]
    }
   ],
   "source": [
    "# Importing the requests module for making HTTP requests\n",
    "import requests\n",
    "\n",
    "# Importing BeautifulSoup for web scraping\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "def nasdaq_companies(url, table_id):\n",
    "    website = requests.get(url)  # Sending a GET request to the specified URL\n",
    "    soup = BeautifulSoup(website.text, 'html.parser')  # Parsing the HTML content using BeautifulSoup\n",
    "    table = soup.find(id=table_id)  # Finding the table element with the specified ID\n",
    "    table_rows = table.find_all('tr')[1:]  # Extracting all table rows except the header row\n",
    "    companies_data = []\n",
    "    for row in table_rows:\n",
    "        cells = row.find_all('td')  # Extracting all cells in the row\n",
    "        if len(cells) >= 2:\n",
    "            company_name = cells[0].text  # Extracting the company name from the first cell\n",
    "            ticker_symbol = cells[1].text  # Extracting the ticker symbol from the second cell\n",
    "            companies_data.append((company_name, ticker_symbol))  # Appending the company data to the list\n",
    "    return tuple(companies_data)\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/NASDAQ-100'\n",
    "nasdaq = nasdaq_companies(url, table_id=\"constituents\")  # Calling the function to get NASDAQ companies\n",
    "\n",
    "for name in nasdaq:\n",
    "    print(f'{name[0]}: {name[1]}')  # Printing the company name and ticker symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "def get_nasdaq_companies(url, table_id):\n",
    "    \"\"\"\n",
    "    Retrieves the list of NASDAQ companies from the specified URL and table ID.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the webpage containing the NASDAQ companies table.\n",
    "        table_id (str): The ID of the HTML table containing the NASDAQ companies.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of NASDAQ companies.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error retrieving the NASDAQ companies.\n",
    "\n",
    "    \"\"\"\n",
    "    output = nasdaq_companies(url, table_id=\"constituents\")\n",
    "    return output\n",
    "\n",
    "output = get_nasdaq_companies(url, table_id=\"constituents\")\n",
    "data_type = type(output)\n",
    "print(data_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VvyNAF9sE7u"
   },
   "source": [
    "---\n",
    "<font color=green>Q2: (1 Mark)</font>\n",
    "<br><font color='green'>\n",
    "Given a list of tuples representing NASDAQ-100 companies (where each tuple contains a company name and its ticker symbol), write a Python script to extract all ticker symbols into a separate list called `tickers_list`.\n",
    "</font>\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "yQaWckkXxDLP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADBE',\n",
       " 'ADP',\n",
       " 'ABNB',\n",
       " 'GOOGL',\n",
       " 'GOOG',\n",
       " 'AMZN',\n",
       " 'AMD',\n",
       " 'AEP',\n",
       " 'AMGN',\n",
       " 'ADI',\n",
       " 'ANSS',\n",
       " 'AAPL',\n",
       " 'AMAT',\n",
       " 'ASML',\n",
       " 'AZN',\n",
       " 'TEAM',\n",
       " 'ADSK',\n",
       " 'BKR',\n",
       " 'BIIB',\n",
       " 'BKNG',\n",
       " 'AVGO',\n",
       " 'CDNS',\n",
       " 'CDW',\n",
       " 'CHTR',\n",
       " 'CTAS',\n",
       " 'CSCO',\n",
       " 'CCEP',\n",
       " 'CTSH',\n",
       " 'CMCSA',\n",
       " 'CEG',\n",
       " 'CPRT',\n",
       " 'CSGP',\n",
       " 'COST',\n",
       " 'CRWD',\n",
       " 'CSX',\n",
       " 'DDOG',\n",
       " 'DXCM',\n",
       " 'FANG',\n",
       " 'DLTR',\n",
       " 'DASH',\n",
       " 'EA',\n",
       " 'EXC',\n",
       " 'FAST',\n",
       " 'FTNT',\n",
       " 'GEHC',\n",
       " 'GILD',\n",
       " 'GFS',\n",
       " 'HON',\n",
       " 'IDXX',\n",
       " 'ILMN',\n",
       " 'INTC',\n",
       " 'INTU',\n",
       " 'ISRG',\n",
       " 'KDP',\n",
       " 'KLAC',\n",
       " 'KHC',\n",
       " 'LRCX',\n",
       " 'LIN',\n",
       " 'LULU',\n",
       " 'MAR',\n",
       " 'MRVL',\n",
       " 'MELI',\n",
       " 'META',\n",
       " 'MCHP',\n",
       " 'MU',\n",
       " 'MSFT',\n",
       " 'MRNA',\n",
       " 'MDLZ',\n",
       " 'MDB',\n",
       " 'MNST',\n",
       " 'NFLX',\n",
       " 'NVDA',\n",
       " 'NXPI',\n",
       " 'ORLY',\n",
       " 'ODFL',\n",
       " 'ON',\n",
       " 'PCAR',\n",
       " 'PANW',\n",
       " 'PAYX',\n",
       " 'PYPL',\n",
       " 'PDD',\n",
       " 'PEP',\n",
       " 'QCOM',\n",
       " 'REGN',\n",
       " 'ROP',\n",
       " 'ROST',\n",
       " 'SIRI',\n",
       " 'SBUX',\n",
       " 'SNPS',\n",
       " 'TTWO',\n",
       " 'TMUS',\n",
       " 'TSLA',\n",
       " 'TXN',\n",
       " 'TTD',\n",
       " 'VRSK',\n",
       " 'VRTX',\n",
       " 'WBA',\n",
       " 'WBD',\n",
       " 'WDAY',\n",
       " 'XEL',\n",
       " 'ZS']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers_list = []\n",
    "for company in nasdaq:\n",
    "    ticker_symbol = company[1]\n",
    "    tickers_list.append(ticker_symbol)\n",
    "\n",
    "tickers_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KEkAPUxsW4s"
   },
   "source": [
    "---\n",
    "<font color=green>Q3: (3 Marks)</font>\n",
    "<br><font color='green'>\n",
    "Using **[yfinance](https://pypi.org/project/yfinance/)** library, write a Python script that accepts a list of stock ticker symbols. For each symbol, download the adjusted closing price data, store it in a dictionary with the ticker symbol as the key, and then convert the final dictionary into a Pandas DataFrame. Handle any errors encountered during data retrieval by printing a message indicating which symbol failed\n",
    "</font>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ABNB</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AMD</th>\n",
       "      <th>AEP</th>\n",
       "      <th>AMGN</th>\n",
       "      <th>ADI</th>\n",
       "      <th>...</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>TXN</th>\n",
       "      <th>TTD</th>\n",
       "      <th>VRSK</th>\n",
       "      <th>VRTX</th>\n",
       "      <th>WBA</th>\n",
       "      <th>WBD</th>\n",
       "      <th>WDAY</th>\n",
       "      <th>XEL</th>\n",
       "      <th>ZS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1962-01-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.950191</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.948460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.934614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-01-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.913845</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-01-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.906922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-03</th>\n",
       "      <td>439.019989</td>\n",
       "      <td>244.020004</td>\n",
       "      <td>146.250000</td>\n",
       "      <td>173.169998</td>\n",
       "      <td>174.419998</td>\n",
       "      <td>178.339996</td>\n",
       "      <td>163.550003</td>\n",
       "      <td>90.080002</td>\n",
       "      <td>307.420013</td>\n",
       "      <td>231.290009</td>\n",
       "      <td>...</td>\n",
       "      <td>176.289993</td>\n",
       "      <td>193.720001</td>\n",
       "      <td>93.110001</td>\n",
       "      <td>253.750000</td>\n",
       "      <td>470.179993</td>\n",
       "      <td>15.920000</td>\n",
       "      <td>8.330</td>\n",
       "      <td>210.830002</td>\n",
       "      <td>55.279999</td>\n",
       "      <td>169.020004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-04</th>\n",
       "      <td>448.369995</td>\n",
       "      <td>245.669998</td>\n",
       "      <td>147.080002</td>\n",
       "      <td>173.789993</td>\n",
       "      <td>175.130005</td>\n",
       "      <td>179.339996</td>\n",
       "      <td>159.990005</td>\n",
       "      <td>90.379997</td>\n",
       "      <td>307.369995</td>\n",
       "      <td>230.630005</td>\n",
       "      <td>...</td>\n",
       "      <td>174.770004</td>\n",
       "      <td>193.300003</td>\n",
       "      <td>94.480003</td>\n",
       "      <td>258.250000</td>\n",
       "      <td>474.950012</td>\n",
       "      <td>16.110001</td>\n",
       "      <td>8.240</td>\n",
       "      <td>211.119995</td>\n",
       "      <td>56.029999</td>\n",
       "      <td>169.139999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-05</th>\n",
       "      <td>455.799988</td>\n",
       "      <td>245.779999</td>\n",
       "      <td>145.779999</td>\n",
       "      <td>175.410004</td>\n",
       "      <td>177.070007</td>\n",
       "      <td>181.279999</td>\n",
       "      <td>166.169998</td>\n",
       "      <td>88.949997</td>\n",
       "      <td>307.380005</td>\n",
       "      <td>235.679993</td>\n",
       "      <td>...</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>196.080002</td>\n",
       "      <td>97.410004</td>\n",
       "      <td>261.279999</td>\n",
       "      <td>483.040009</td>\n",
       "      <td>15.940000</td>\n",
       "      <td>8.300</td>\n",
       "      <td>212.460007</td>\n",
       "      <td>55.160000</td>\n",
       "      <td>174.570007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-06</th>\n",
       "      <td>458.130005</td>\n",
       "      <td>247.970001</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>176.729996</td>\n",
       "      <td>178.350006</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>166.779999</td>\n",
       "      <td>88.699997</td>\n",
       "      <td>305.690002</td>\n",
       "      <td>237.410004</td>\n",
       "      <td>...</td>\n",
       "      <td>177.940002</td>\n",
       "      <td>196.240005</td>\n",
       "      <td>97.190002</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>485.529999</td>\n",
       "      <td>15.860000</td>\n",
       "      <td>8.340</td>\n",
       "      <td>214.949997</td>\n",
       "      <td>54.820000</td>\n",
       "      <td>178.929993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-07</th>\n",
       "      <td>464.649994</td>\n",
       "      <td>252.641098</td>\n",
       "      <td>146.229996</td>\n",
       "      <td>177.100006</td>\n",
       "      <td>178.690002</td>\n",
       "      <td>185.339996</td>\n",
       "      <td>168.116196</td>\n",
       "      <td>88.930099</td>\n",
       "      <td>307.279999</td>\n",
       "      <td>236.960007</td>\n",
       "      <td>...</td>\n",
       "      <td>178.551193</td>\n",
       "      <td>196.679993</td>\n",
       "      <td>96.739998</td>\n",
       "      <td>262.630005</td>\n",
       "      <td>486.015015</td>\n",
       "      <td>15.845000</td>\n",
       "      <td>8.295</td>\n",
       "      <td>215.470001</td>\n",
       "      <td>54.689999</td>\n",
       "      <td>179.294998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15716 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ADBE         ADP        ABNB       GOOGL        GOOG  \\\n",
       "1962-01-02         NaN         NaN         NaN         NaN         NaN   \n",
       "1962-01-03         NaN         NaN         NaN         NaN         NaN   \n",
       "1962-01-04         NaN         NaN         NaN         NaN         NaN   \n",
       "1962-01-05         NaN         NaN         NaN         NaN         NaN   \n",
       "1962-01-08         NaN         NaN         NaN         NaN         NaN   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2024-06-03  439.019989  244.020004  146.250000  173.169998  174.419998   \n",
       "2024-06-04  448.369995  245.669998  147.080002  173.789993  175.130005   \n",
       "2024-06-05  455.799988  245.779999  145.779999  175.410004  177.070007   \n",
       "2024-06-06  458.130005  247.970001  147.000000  176.729996  178.350006   \n",
       "2024-06-07  464.649994  252.641098  146.229996  177.100006  178.690002   \n",
       "\n",
       "                  AMZN         AMD        AEP        AMGN         ADI  ...  \\\n",
       "1962-01-02         NaN         NaN   0.950191         NaN         NaN  ...   \n",
       "1962-01-03         NaN         NaN   0.948460         NaN         NaN  ...   \n",
       "1962-01-04         NaN         NaN   0.934614         NaN         NaN  ...   \n",
       "1962-01-05         NaN         NaN   0.913845         NaN         NaN  ...   \n",
       "1962-01-08         NaN         NaN   0.906922         NaN         NaN  ...   \n",
       "...                ...         ...        ...         ...         ...  ...   \n",
       "2024-06-03  178.339996  163.550003  90.080002  307.420013  231.290009  ...   \n",
       "2024-06-04  179.339996  159.990005  90.379997  307.369995  230.630005  ...   \n",
       "2024-06-05  181.279999  166.169998  88.949997  307.380005  235.679993  ...   \n",
       "2024-06-06  185.000000  166.779999  88.699997  305.690002  237.410004  ...   \n",
       "2024-06-07  185.339996  168.116196  88.930099  307.279999  236.960007  ...   \n",
       "\n",
       "                  TSLA         TXN        TTD        VRSK        VRTX  \\\n",
       "1962-01-02         NaN         NaN        NaN         NaN         NaN   \n",
       "1962-01-03         NaN         NaN        NaN         NaN         NaN   \n",
       "1962-01-04         NaN         NaN        NaN         NaN         NaN   \n",
       "1962-01-05         NaN         NaN        NaN         NaN         NaN   \n",
       "1962-01-08         NaN         NaN        NaN         NaN         NaN   \n",
       "...                ...         ...        ...         ...         ...   \n",
       "2024-06-03  176.289993  193.720001  93.110001  253.750000  470.179993   \n",
       "2024-06-04  174.770004  193.300003  94.480003  258.250000  474.950012   \n",
       "2024-06-05  175.000000  196.080002  97.410004  261.279999  483.040009   \n",
       "2024-06-06  177.940002  196.240005  97.190002  260.000000  485.529999   \n",
       "2024-06-07  178.551193  196.679993  96.739998  262.630005  486.015015   \n",
       "\n",
       "                  WBA    WBD        WDAY        XEL          ZS  \n",
       "1962-01-02        NaN    NaN         NaN        NaN         NaN  \n",
       "1962-01-03        NaN    NaN         NaN        NaN         NaN  \n",
       "1962-01-04        NaN    NaN         NaN        NaN         NaN  \n",
       "1962-01-05        NaN    NaN         NaN        NaN         NaN  \n",
       "1962-01-08        NaN    NaN         NaN        NaN         NaN  \n",
       "...               ...    ...         ...        ...         ...  \n",
       "2024-06-03  15.920000  8.330  210.830002  55.279999  169.020004  \n",
       "2024-06-04  16.110001  8.240  211.119995  56.029999  169.139999  \n",
       "2024-06-05  15.940000  8.300  212.460007  55.160000  174.570007  \n",
       "2024-06-06  15.860000  8.340  214.949997  54.820000  178.929993  \n",
       "2024-06-07  15.845000  8.295  215.470001  54.689999  179.294998  \n",
       "\n",
       "[15716 rows x 101 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def download_stock_data(tickers_list):\n",
    "    stock_data = {}  # Dictionary to store stock data\n",
    "    for ticker in tickers_list:\n",
    "        try:\n",
    "            stock = yf.Ticker(ticker)  # Fetch the stock data\n",
    "            hist = stock.history(period=\"max\")  # Download the historical data\n",
    "            stock_data[ticker] = hist['Close']  # Store the adjusted closing prices\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to retrieve data for {ticker}: {e}\")\n",
    "    return stock_data\n",
    "\n",
    "# Download stock data\n",
    "stock_data = download_stock_data(tickers_list)\n",
    "\n",
    "# Convert the dictionary to a Pandas DataFrame\n",
    "stock_df = pd.DataFrame(stock_data)\n",
    "stock_df.index = stock_df.index.date\n",
    "\n",
    "stock_df  # Print the DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Np3XHMtatQC_"
   },
   "source": [
    "---\n",
    "<font color=green>Q4: (3 Marks)</font>\n",
    "<br><font color='green'>\n",
    "Write a Python script to analyze stock data stored in a dictionary `stock_data` (where each key is a stock ticker symbol, and each value is a Pandas Series of adjusted closing prices). The script should:\n",
    "1. Convert the dictionary into a DataFrame.\n",
    "2. Calculate the daily returns for each stock.\n",
    "3. Identify columns (ticker symbols) with at least 2000 non-NaN values in their daily returns.\n",
    "4. Create a new DataFrame that only includes these filtered ticker symbols.\n",
    "5. Remove any remaining rows with NaN values in this new DataFrame.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wp/tw2jj_bx35gcj53hl67z_4nc0000gn/T/ipykernel_88595/4091264949.py:19: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Call ffill before calling pct_change to retain current behavior and silence this warning.\n",
      "  daily_returns = stock_df.pct_change()\n",
      "/var/folders/wp/tw2jj_bx35gcj53hl67z_4nc0000gn/T/ipykernel_88595/4091264949.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.dropna(inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADP</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AMD</th>\n",
       "      <th>AEP</th>\n",
       "      <th>AMGN</th>\n",
       "      <th>ADI</th>\n",
       "      <th>ANSS</th>\n",
       "      <th>...</th>\n",
       "      <th>TTWO</th>\n",
       "      <th>TMUS</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>TXN</th>\n",
       "      <th>VRSK</th>\n",
       "      <th>VRTX</th>\n",
       "      <th>WBA</th>\n",
       "      <th>WBD</th>\n",
       "      <th>WDAY</th>\n",
       "      <th>XEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-12-10</th>\n",
       "      <td>-0.006699</td>\n",
       "      <td>0.006004</td>\n",
       "      <td>-0.003292</td>\n",
       "      <td>-0.002861</td>\n",
       "      <td>-0.003715</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>-0.024355</td>\n",
       "      <td>0.011274</td>\n",
       "      <td>0.008826</td>\n",
       "      <td>0.004968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004777</td>\n",
       "      <td>0.007485</td>\n",
       "      <td>0.011358</td>\n",
       "      <td>0.002995</td>\n",
       "      <td>-0.002615</td>\n",
       "      <td>0.010279</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>-0.002109</td>\n",
       "      <td>0.005037</td>\n",
       "      <td>-0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-11</th>\n",
       "      <td>0.027653</td>\n",
       "      <td>-0.024924</td>\n",
       "      <td>-0.012657</td>\n",
       "      <td>-0.014130</td>\n",
       "      <td>-0.033473</td>\n",
       "      <td>-0.036735</td>\n",
       "      <td>-0.005831</td>\n",
       "      <td>-0.028308</td>\n",
       "      <td>-0.003849</td>\n",
       "      <td>-0.013951</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011575</td>\n",
       "      <td>-0.009356</td>\n",
       "      <td>-0.044259</td>\n",
       "      <td>-0.012647</td>\n",
       "      <td>-0.015995</td>\n",
       "      <td>-0.034709</td>\n",
       "      <td>-0.020499</td>\n",
       "      <td>-0.035928</td>\n",
       "      <td>-0.057630</td>\n",
       "      <td>0.004311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-14</th>\n",
       "      <td>0.020127</td>\n",
       "      <td>0.015241</td>\n",
       "      <td>0.016151</td>\n",
       "      <td>0.012045</td>\n",
       "      <td>0.027744</td>\n",
       "      <td>-0.008475</td>\n",
       "      <td>-0.000367</td>\n",
       "      <td>0.019078</td>\n",
       "      <td>-0.001932</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.014444</td>\n",
       "      <td>0.007188</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.014390</td>\n",
       "      <td>-0.016491</td>\n",
       "      <td>0.010402</td>\n",
       "      <td>-0.033613</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.010017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-15</th>\n",
       "      <td>0.008149</td>\n",
       "      <td>0.010284</td>\n",
       "      <td>-0.003213</td>\n",
       "      <td>-0.005844</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.027503</td>\n",
       "      <td>0.028524</td>\n",
       "      <td>-0.004752</td>\n",
       "      <td>0.009544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023018</td>\n",
       "      <td>0.044907</td>\n",
       "      <td>0.011483</td>\n",
       "      <td>0.023657</td>\n",
       "      <td>0.013924</td>\n",
       "      <td>0.013656</td>\n",
       "      <td>-0.005451</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>-0.000380</td>\n",
       "      <td>0.007934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>0.016380</td>\n",
       "      <td>0.008541</td>\n",
       "      <td>0.021708</td>\n",
       "      <td>0.019761</td>\n",
       "      <td>0.026008</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.017309</td>\n",
       "      <td>0.012053</td>\n",
       "      <td>0.017330</td>\n",
       "      <td>0.008354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006389</td>\n",
       "      <td>0.025419</td>\n",
       "      <td>0.060699</td>\n",
       "      <td>0.009035</td>\n",
       "      <td>0.021117</td>\n",
       "      <td>0.010658</td>\n",
       "      <td>0.031665</td>\n",
       "      <td>0.024887</td>\n",
       "      <td>0.029378</td>\n",
       "      <td>0.023896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-03</th>\n",
       "      <td>-0.012906</td>\n",
       "      <td>-0.003675</td>\n",
       "      <td>0.003884</td>\n",
       "      <td>0.002644</td>\n",
       "      <td>0.010768</td>\n",
       "      <td>-0.020072</td>\n",
       "      <td>-0.001884</td>\n",
       "      <td>0.005133</td>\n",
       "      <td>-0.009723</td>\n",
       "      <td>-0.008505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020454</td>\n",
       "      <td>-0.010745</td>\n",
       "      <td>-0.010052</td>\n",
       "      <td>-0.006615</td>\n",
       "      <td>0.003837</td>\n",
       "      <td>0.032591</td>\n",
       "      <td>-0.018496</td>\n",
       "      <td>0.010922</td>\n",
       "      <td>-0.002932</td>\n",
       "      <td>-0.003066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-04</th>\n",
       "      <td>0.021297</td>\n",
       "      <td>0.006762</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>0.005607</td>\n",
       "      <td>-0.021767</td>\n",
       "      <td>0.003330</td>\n",
       "      <td>-0.000163</td>\n",
       "      <td>-0.002854</td>\n",
       "      <td>0.007466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014178</td>\n",
       "      <td>0.027906</td>\n",
       "      <td>-0.008622</td>\n",
       "      <td>-0.002168</td>\n",
       "      <td>0.017734</td>\n",
       "      <td>0.010145</td>\n",
       "      <td>0.011935</td>\n",
       "      <td>-0.010804</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.013567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-05</th>\n",
       "      <td>0.016571</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0.009322</td>\n",
       "      <td>0.011077</td>\n",
       "      <td>0.010817</td>\n",
       "      <td>0.038627</td>\n",
       "      <td>-0.015822</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.021896</td>\n",
       "      <td>0.031441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008857</td>\n",
       "      <td>0.013265</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.014382</td>\n",
       "      <td>0.011733</td>\n",
       "      <td>0.017033</td>\n",
       "      <td>-0.010553</td>\n",
       "      <td>0.007282</td>\n",
       "      <td>0.006347</td>\n",
       "      <td>-0.015527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-06</th>\n",
       "      <td>0.005112</td>\n",
       "      <td>0.008910</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>0.007229</td>\n",
       "      <td>0.020521</td>\n",
       "      <td>0.003671</td>\n",
       "      <td>-0.002811</td>\n",
       "      <td>-0.005498</td>\n",
       "      <td>0.007341</td>\n",
       "      <td>-0.007154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010512</td>\n",
       "      <td>-0.002774</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>-0.004899</td>\n",
       "      <td>0.005155</td>\n",
       "      <td>-0.005019</td>\n",
       "      <td>0.004819</td>\n",
       "      <td>0.011720</td>\n",
       "      <td>-0.006164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-07</th>\n",
       "      <td>0.014232</td>\n",
       "      <td>0.018837</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.001906</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.008012</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>-0.001895</td>\n",
       "      <td>0.003634</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001720</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.003435</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.010115</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>-0.000946</td>\n",
       "      <td>-0.005396</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>-0.002371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2137 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ADBE       ADP     GOOGL      GOOG      AMZN       AMD  \\\n",
       "2015-12-10 -0.006699  0.006004 -0.003292 -0.002861 -0.003715  0.042553   \n",
       "2015-12-11  0.027653 -0.024924 -0.012657 -0.014130 -0.033473 -0.036735   \n",
       "2015-12-14  0.020127  0.015241  0.016151  0.012045  0.027744 -0.008475   \n",
       "2015-12-15  0.008149  0.010284 -0.003213 -0.005844  0.001110  0.008547   \n",
       "2015-12-16  0.016380  0.008541  0.021708  0.019761  0.026008  0.076271   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2024-06-03 -0.012906 -0.003675  0.003884  0.002644  0.010768 -0.020072   \n",
       "2024-06-04  0.021297  0.006762  0.003580  0.004071  0.005607 -0.021767   \n",
       "2024-06-05  0.016571  0.000448  0.009322  0.011077  0.010817  0.038627   \n",
       "2024-06-06  0.005112  0.008910  0.007525  0.007229  0.020521  0.003671   \n",
       "2024-06-07  0.014232  0.018837  0.002094  0.001906  0.001838  0.008012   \n",
       "\n",
       "                 AEP      AMGN       ADI      ANSS  ...      TTWO      TMUS  \\\n",
       "2015-12-10 -0.024355  0.011274  0.008826  0.004968  ... -0.004777  0.007485   \n",
       "2015-12-11 -0.005831 -0.028308 -0.003849 -0.013951  ... -0.011575 -0.009356   \n",
       "2015-12-14 -0.000367  0.019078 -0.001932  0.003899  ...  0.005141  0.014444   \n",
       "2015-12-15  0.027503  0.028524 -0.004752  0.009544  ...  0.023018  0.044907   \n",
       "2015-12-16  0.017309  0.012053  0.017330  0.008354  ...  0.006389  0.025419   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "2024-06-03 -0.001884  0.005133 -0.009723 -0.008505  ...  0.020454 -0.010745   \n",
       "2024-06-04  0.003330 -0.000163 -0.002854  0.007466  ...  0.014178  0.027906   \n",
       "2024-06-05 -0.015822  0.000033  0.021896  0.031441  ...  0.008857  0.013265   \n",
       "2024-06-06 -0.002811 -0.005498  0.007341 -0.007154  ... -0.010512 -0.002774   \n",
       "2024-06-07  0.002594  0.005201 -0.001895  0.003634  ... -0.001720  0.000946   \n",
       "\n",
       "                TSLA       TXN      VRSK      VRTX       WBA       WBD  \\\n",
       "2015-12-10  0.011358  0.002995 -0.002615  0.010279  0.000960 -0.002109   \n",
       "2015-12-11 -0.044259 -0.012647 -0.015995 -0.034709 -0.020499 -0.035928   \n",
       "2015-12-14  0.007188  0.000178  0.014390 -0.016491  0.010402 -0.033613   \n",
       "2015-12-15  0.011483  0.023657  0.013924  0.013656 -0.005451  0.002646   \n",
       "2015-12-16  0.060699  0.009035  0.021117  0.010658  0.031665  0.024887   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2024-06-03 -0.010052 -0.006615  0.003837  0.032591 -0.018496  0.010922   \n",
       "2024-06-04 -0.008622 -0.002168  0.017734  0.010145  0.011935 -0.010804   \n",
       "2024-06-05  0.001316  0.014382  0.011733  0.017033 -0.010553  0.007282   \n",
       "2024-06-06  0.016800  0.000816 -0.004899  0.005155 -0.005019  0.004819   \n",
       "2024-06-07  0.003435  0.002242  0.010115  0.000999 -0.000946 -0.005396   \n",
       "\n",
       "                WDAY       XEL  \n",
       "2015-12-10  0.005037 -0.013889  \n",
       "2015-12-11 -0.057630  0.004311  \n",
       "2015-12-14  0.000253  0.010017  \n",
       "2015-12-15 -0.000380  0.007934  \n",
       "2015-12-16  0.029378  0.023896  \n",
       "...              ...       ...  \n",
       "2024-06-03 -0.002932 -0.003066  \n",
       "2024-06-04  0.001375  0.013567  \n",
       "2024-06-05  0.006347 -0.015527  \n",
       "2024-06-06  0.011720 -0.006164  \n",
       "2024-06-07  0.002419 -0.002371  \n",
       "\n",
       "[2137 rows x 89 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def analyze_stock_data(stock_data):\n",
    "    \"\"\"\n",
    "    Analyzes the stock data by calculating daily returns, filtering columns with at least 2000 non-NaN values,\n",
    "    removing rows with NaN values, and converting the index to date format.\n",
    "    \n",
    "    Parameters:\n",
    "    stock_data (dict): A dictionary containing stock data with ticker symbols as keys and price data as values.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the filtered stock data with daily returns, filtered columns, and date index.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    stock_df = pd.DataFrame(stock_data)\n",
    "    \n",
    "    # Calculate daily returns for each stock\n",
    "    daily_returns = stock_df.pct_change()\n",
    "    \n",
    "    # Identify columns with at least 2000 non-NaN values in their daily returns\n",
    "    accepted_columns = daily_returns.columns[daily_returns.count() >= 2000]\n",
    "    \n",
    "    # Create a new DataFrame with filtered ticker symbols\n",
    "    filtered_df = daily_returns[accepted_columns]\n",
    "    \n",
    "    # Remove remaining rows with NaN values\n",
    "    filtered_df.dropna(inplace=True)\n",
    "    \n",
    "    filtered_df.index = filtered_df.index.date\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "filtered_stock_data = analyze_stock_data(stock_data)\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "filtered_stock_data  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3xAawJ8nPbH"
   },
   "source": [
    "---\n",
    "<font color=green>Q5: (1 Mark)</font>\n",
    "<br><font color='green'>\n",
    "Download the dataset named `df_filtered_nasdaq_100` from the GitHub repository of the course.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Ko4juu_HxHnT"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADP</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AMD</th>\n",
       "      <th>AEP</th>\n",
       "      <th>AMGN</th>\n",
       "      <th>ADI</th>\n",
       "      <th>ANSS</th>\n",
       "      <th>...</th>\n",
       "      <th>TTWO</th>\n",
       "      <th>TMUS</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>TXN</th>\n",
       "      <th>VRSK</th>\n",
       "      <th>VRTX</th>\n",
       "      <th>WBA</th>\n",
       "      <th>WBD</th>\n",
       "      <th>WDAY</th>\n",
       "      <th>XEL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-12-10</th>\n",
       "      <td>-0.006699</td>\n",
       "      <td>0.006004</td>\n",
       "      <td>-0.003292</td>\n",
       "      <td>-0.002861</td>\n",
       "      <td>-0.003715</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>-0.024356</td>\n",
       "      <td>0.011274</td>\n",
       "      <td>0.008826</td>\n",
       "      <td>0.004968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004777</td>\n",
       "      <td>0.007485</td>\n",
       "      <td>0.011358</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>-0.002616</td>\n",
       "      <td>0.010279</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>-0.002109</td>\n",
       "      <td>0.005037</td>\n",
       "      <td>-0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-11</th>\n",
       "      <td>0.027653</td>\n",
       "      <td>-0.024924</td>\n",
       "      <td>-0.012657</td>\n",
       "      <td>-0.014130</td>\n",
       "      <td>-0.033473</td>\n",
       "      <td>-0.036735</td>\n",
       "      <td>-0.005831</td>\n",
       "      <td>-0.028308</td>\n",
       "      <td>-0.003850</td>\n",
       "      <td>-0.013951</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011575</td>\n",
       "      <td>-0.009356</td>\n",
       "      <td>-0.044259</td>\n",
       "      <td>-0.012648</td>\n",
       "      <td>-0.015996</td>\n",
       "      <td>-0.034709</td>\n",
       "      <td>-0.020499</td>\n",
       "      <td>-0.035928</td>\n",
       "      <td>-0.057630</td>\n",
       "      <td>0.004311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-14</th>\n",
       "      <td>0.020127</td>\n",
       "      <td>0.015241</td>\n",
       "      <td>0.016151</td>\n",
       "      <td>0.012045</td>\n",
       "      <td>0.027744</td>\n",
       "      <td>-0.008475</td>\n",
       "      <td>-0.000366</td>\n",
       "      <td>0.019078</td>\n",
       "      <td>-0.001932</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.014444</td>\n",
       "      <td>0.007188</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.014390</td>\n",
       "      <td>-0.016491</td>\n",
       "      <td>0.010402</td>\n",
       "      <td>-0.033613</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.010017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-15</th>\n",
       "      <td>0.008149</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>-0.003213</td>\n",
       "      <td>-0.005844</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.027503</td>\n",
       "      <td>0.028525</td>\n",
       "      <td>-0.004752</td>\n",
       "      <td>0.009544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023018</td>\n",
       "      <td>0.044907</td>\n",
       "      <td>0.011483</td>\n",
       "      <td>0.023657</td>\n",
       "      <td>0.013923</td>\n",
       "      <td>0.013656</td>\n",
       "      <td>-0.005450</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>-0.000380</td>\n",
       "      <td>0.007934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>0.016380</td>\n",
       "      <td>0.008541</td>\n",
       "      <td>0.021708</td>\n",
       "      <td>0.019761</td>\n",
       "      <td>0.026008</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.017309</td>\n",
       "      <td>0.012053</td>\n",
       "      <td>0.017330</td>\n",
       "      <td>0.008354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006389</td>\n",
       "      <td>0.025419</td>\n",
       "      <td>0.060699</td>\n",
       "      <td>0.009036</td>\n",
       "      <td>0.021117</td>\n",
       "      <td>0.010658</td>\n",
       "      <td>0.031664</td>\n",
       "      <td>0.024887</td>\n",
       "      <td>0.029378</td>\n",
       "      <td>0.023897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ADBE       ADP     GOOGL      GOOG      AMZN       AMD  \\\n",
       "Date                                                                     \n",
       "2015-12-10 -0.006699  0.006004 -0.003292 -0.002861 -0.003715  0.042553   \n",
       "2015-12-11  0.027653 -0.024924 -0.012657 -0.014130 -0.033473 -0.036735   \n",
       "2015-12-14  0.020127  0.015241  0.016151  0.012045  0.027744 -0.008475   \n",
       "2015-12-15  0.008149  0.010283 -0.003213 -0.005844  0.001110  0.008547   \n",
       "2015-12-16  0.016380  0.008541  0.021708  0.019761  0.026008  0.076271   \n",
       "\n",
       "                 AEP      AMGN       ADI      ANSS  ...      TTWO      TMUS  \\\n",
       "Date                                                ...                       \n",
       "2015-12-10 -0.024356  0.011274  0.008826  0.004968  ... -0.004777  0.007485   \n",
       "2015-12-11 -0.005831 -0.028308 -0.003850 -0.013951  ... -0.011575 -0.009356   \n",
       "2015-12-14 -0.000366  0.019078 -0.001932  0.003899  ...  0.005141  0.014444   \n",
       "2015-12-15  0.027503  0.028525 -0.004752  0.009544  ...  0.023018  0.044907   \n",
       "2015-12-16  0.017309  0.012053  0.017330  0.008354  ...  0.006389  0.025419   \n",
       "\n",
       "                TSLA       TXN      VRSK      VRTX       WBA       WBD  \\\n",
       "Date                                                                     \n",
       "2015-12-10  0.011358  0.002996 -0.002616  0.010279  0.000960 -0.002109   \n",
       "2015-12-11 -0.044259 -0.012648 -0.015996 -0.034709 -0.020499 -0.035928   \n",
       "2015-12-14  0.007188  0.000178  0.014390 -0.016491  0.010402 -0.033613   \n",
       "2015-12-15  0.011483  0.023657  0.013923  0.013656 -0.005450  0.002646   \n",
       "2015-12-16  0.060699  0.009036  0.021117  0.010658  0.031664  0.024887   \n",
       "\n",
       "                WDAY       XEL  \n",
       "Date                            \n",
       "2015-12-10  0.005037 -0.013889  \n",
       "2015-12-11 -0.057630  0.004311  \n",
       "2015-12-14  0.000253  0.010017  \n",
       "2015-12-15 -0.000380  0.007934  \n",
       "2015-12-16  0.029378  0.023897  \n",
       "\n",
       "[5 rows x 89 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import ssl\n",
    "\n",
    "\n",
    "url = 'http://raw.githubusercontent.com/Jandsy/ml_finance_imperial/main/Coursework/df_filtered_nasdaq_100.csv'\n",
    "\n",
    "# Visual Studio Code made me import ssl to ensure security, may not be needed when run in Google Colab\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Read the file and call it a dataframe\n",
    "df_filtered_nasdaq_100 = pd.read_csv(url,index_col='Date', parse_dates=True)\n",
    "\n",
    "# Print out the data\n",
    "df_filtered_nasdaq_100.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WbCUDabWQoZ"
   },
   "source": [
    "---\n",
    "<font color=green>Q6: (3 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Conduct an in-depth analysis of the `df_filtered_nasdaq_100` dataset from GitHub. Answer the following questions:\n",
    "- Which stock had the best performance over the entire period?\n",
    "- What is the average daily return of 'AAPL'?\n",
    "- What is the worst daily return? Provide the stock name and the date it occurred.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "kPfiDWWlxI0Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performing stock over the entire period: NVDA with a cumulative return of 11158.84%\n",
      "Average daily return of 'AAPL': 0.11%\n",
      "Worst daily return occurred for FANG on 2020-03-09, return: -44.65%\n",
      "111.58842785050534\n"
     ]
    }
   ],
   "source": [
    "# Calculate the cumulative return of each stock to determine the best performer over the entire period\n",
    "cumulative_returns = df_filtered_nasdaq_100.add(1).cumprod().iloc[-1] - 1\n",
    "best_performer = cumulative_returns.idxmax()\n",
    "best_performance = cumulative_returns.max()\n",
    "\n",
    "# Calculate the average daily return of 'AAPL'\n",
    "average_daily_return_aapl = df_filtered_nasdaq_100['AAPL'].mean()\n",
    "\n",
    "# Find the worst daily return across all stocks and the corresponding date and stock\n",
    "worst_daily_return = df_filtered_nasdaq_100.min().min()\n",
    "worst_stock = df_filtered_nasdaq_100.min().idxmin()\n",
    "worst_date = df_filtered_nasdaq_100.idxmin()[worst_stock].date()\n",
    "\n",
    "# Print out the results\n",
    "print(f\"Best performing stock over the entire period: {best_performer} with a cumulative return of {best_performance*100:.2f}%\")\n",
    "print(f\"Average daily return of 'AAPL': {average_daily_return_aapl*100:.2f}%\")\n",
    "print(f\"Worst daily return occurred for {worst_stock} on {worst_date}, return: {worst_daily_return*100:.2f}%\")\n",
    "print(cumulative_returns['NVDA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcY-V82zXGqc"
   },
   "source": [
    "# Fama French Analysis\n",
    "\n",
    "The Fama-French five-factor model is an extension of the classic three-factor model used in finance to describe stock returns. It is designed to better capture the risk associated with stocks and explain differences in returns. This model includes the following factors:\n",
    "\n",
    "1. **Market Risk (MKT)**: The excess return of the market over the risk-free rate. It captures the overall market's premium.\n",
    "2. **Size (SMB, \"Small Minus Big\")**: The performance of small-cap stocks relative to large-cap stocks.\n",
    "3. **Value (HML, \"High Minus Low\")**: The performance of stocks with high book-to-market values relative to those with low book-to-market values.\n",
    "4. **Profitability (RMW, \"Robust Minus Weak\")**: The difference in returns between companies with robust (high) and weak (low) profitability.\n",
    "5. **Investment (CMA, \"Conservative Minus Aggressive\")**: The difference in returns between companies that invest conservatively and those that invest aggressively.\n",
    "\n",
    "## Additional Factor\n",
    "\n",
    "6. **Momentum (MOM)**: This factor represents the tendency of stocks that have performed well in the past to continue performing well, and the reverse for stocks that have performed poorly.\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "The return of a stock $R_i^t$ at time $t$ can be modeled as follows :\n",
    "\n",
    "$$\n",
    "R_i^t - R_f^t = \\alpha_i^t + \\beta_{i,MKT}^t(R_M^t - R_f^t) + \\beta_{i,SMB}^t \\cdot SMB^t + \\beta_{i,HML}^t \\cdot HML^t + \\beta_{i,RMW}^t \\cdot RMW^t + \\beta_{i,CMA}^t \\cdot CMA^t + \\beta_{i,MOM}^t \\cdot MOM^t + \\epsilon_i^t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ R_i^t $ is the return of stock $i$ at time $t$\n",
    "- $R_f^t $is the risk-free rate at time $t$\n",
    "- $ R_M^t $ is the market return at time $t$\n",
    "- $\\alpha_i^t $ is the abnormal return or alpha of stock $ i $ at time $t$\n",
    "- $\\beta^t $ coefficients represent the sensitivity of the stock returns to each factor at time $t$\n",
    "- $\\epsilon_i^t $ is the error term or idiosyncratic risk unique to stock $ i $ at time $t$\n",
    "\n",
    "This model is particularly useful for identifying which factors significantly impact stock returns and for constructing a diversified portfolio that is optimized for given risk preferences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtFgmpsKQc8B"
   },
   "source": [
    "---\n",
    "<font color=green>Q7: (1 Mark) </font>\n",
    "<br><font color='green'>\n",
    "Download the `fama_french_dataset` from the course's GitHub account.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "iJVPHhTSxKuk"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>RF</th>\n",
       "      <th>Mom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1963-07-01</th>\n",
       "      <td>-0.67</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-07-02</th>\n",
       "      <td>0.79</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-07-03</th>\n",
       "      <td>0.63</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-07-05</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-07-08</th>\n",
       "      <td>-0.63</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Mkt-RF   SMB   HML   RMW   CMA     RF   Mom\n",
       "1963-07-01   -0.67  0.02 -0.35  0.03  0.13  0.012 -0.21\n",
       "1963-07-02    0.79 -0.28  0.28 -0.08 -0.21  0.012  0.42\n",
       "1963-07-03    0.63 -0.18 -0.10  0.13 -0.25  0.012  0.41\n",
       "1963-07-05    0.40  0.09 -0.28  0.07 -0.30  0.012  0.07\n",
       "1963-07-08   -0.63  0.07 -0.20 -0.27  0.06  0.012 -0.45"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import ssl\n",
    "\n",
    "url = 'http://raw.githubusercontent.com/Jandsy/ml_finance_imperial/main/Coursework/fama_french_dataset.csv'\n",
    "# Visual Studio Code made me import ssl to ensure security, may not be needed when run in Google Colab\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Read the file of the fama french data\n",
    "fama_french_data = pd.read_csv(url, index_col=0)\n",
    "\n",
    "# Print the first few rows of the dataset\n",
    "fama_french_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>RF</th>\n",
       "      <th>Mom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-03-22</th>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-25</th>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.88</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-26</th>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-27</th>\n",
       "      <td>0.88</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.91</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-1.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-28</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.48</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Mkt-RF   SMB   HML   RMW   CMA     RF   Mom\n",
       "2024-03-22   -0.23 -0.98 -0.53  0.29 -0.37  0.021  0.43\n",
       "2024-03-25   -0.26 -0.10  0.88 -0.22 -0.17  0.021 -0.34\n",
       "2024-03-26   -0.26  0.10 -0.13 -0.50  0.23  0.021  0.09\n",
       "2024-03-27    0.88  1.29  0.91 -0.14  0.58  0.021 -1.34\n",
       "2024-03-28    0.10  0.45  0.48 -0.07  0.09  0.021 -0.44"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the last few rows of the dataset\n",
    "fama_french_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5FJ362KW-wh"
   },
   "source": [
    "---\n",
    "<font color=green>Q8: (5 Marks)</font>\n",
    "<br><font color='green'>\n",
    "\n",
    "Write a Python function called `get_sub_df_ticker(ticker, date, df_filtered, length_history)` that extracts a historical sub-dataframe for a given `ticker` from `df_filtered`. The function should use `length_history` to determine the number of trading days to include, ending at the specified `date`. Return the sub-dataframe for the specified `ticker`.\n",
    "</font>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import ssl\n",
    "\n",
    "url = 'http://raw.githubusercontent.com/Jandsy/ml_finance_imperial/main/Coursework/df_filtered_nasdaq_100.csv'\n",
    "\n",
    "# Visual Studio Code made me import ssl to ensure security, may not be needed when run in Google Colab\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Read the file from url\n",
    "df_filtered = pd.read_csv(url, index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FANG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-03-05</th>\n",
       "      <td>-0.046831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-06</th>\n",
       "      <td>-0.165779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-09</th>\n",
       "      <td>-0.446458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-10</th>\n",
       "      <td>0.104167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                FANG\n",
       "Date                \n",
       "2020-03-05 -0.046831\n",
       "2020-03-06 -0.165779\n",
       "2020-03-09 -0.446458\n",
       "2020-03-10  0.104167"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from datetime import datetime,timedelta\n",
    "import pandas as pd\n",
    "\n",
    "def get_sub_df_ticker(ticker, date, df_filtered, length_history):\n",
    "\n",
    "    # Convert index to datetime if not already\n",
    "    if not isinstance(df_filtered.index, pd.DatetimeIndex):\n",
    "        df_filtered.index = pd.to_datetime(df_filtered.index)\n",
    "\n",
    "    # Validate input date\n",
    "    try:\n",
    "        end_date = pd.to_datetime(date)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"The date provided is not in a recognisable format. Please use 'YYYY-MM-DD' format.\")\n",
    "\n",
    "    # Check if the date is within the range\n",
    "    if end_date not in df_filtered.index:\n",
    "        available_start = df_filtered.index.min()\n",
    "        available_end = df_filtered.index.max()\n",
    "        if end_date < available_start or end_date > available_end:\n",
    "            raise ValueError(f\"The date provided is out of the available data range ({available_start.date()} to {available_end.date()}).\")\n",
    "        end_date = df_filtered.index[df_filtered.index <= end_date].max()\n",
    "\n",
    "    # Find the position of the end_date in the index\n",
    "    end_date_index = df_filtered.index.get_loc(end_date)\n",
    "\n",
    "    # Determine the start_date_index by subtracting length_history\n",
    "    start_date_index = max(0, end_date_index - length_history + 1)\n",
    "\n",
    "    # Get the actual start_date from the index\n",
    "    start_date = df_filtered.index[start_date_index]\n",
    "\n",
    "    # Create a sub dataframe with the specified ticker and date range\n",
    "    sub_dataframe = df_filtered.loc[start_date:end_date, [ticker]].copy()\n",
    "\n",
    "    return sub_dataframe\n",
    "\n",
    "# Testing to confirm if the function is working or not\n",
    "get_sub_df_ticker(\"FANG\", '2020-03-10', df_filtered, 4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-p7nCNYXYNr"
   },
   "source": [
    "---\n",
    "<font color=green>Q9: (4 Marks)</font>\n",
    "<br><font color='green'>\n",
    "Create a Python function named `df_ticker_with_fama_french(ticker, date, df_filtered, length_history, fama_french_data)` that uses `get_sub_df_ticker` to extract historical data for a specific `ticker`. Incorporate the Fama-French factors from `fama_french_data` into the extracted sub-dataframe. Adjust the ticker's returns by subtracting the risk-free rate ('RF') and add other relevant Fama-French factors ('Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', and 'Mom'). Return the resulting sub-dataframe.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FANG</th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>RF</th>\n",
       "      <th>Mom</th>\n",
       "      <th>Excess Returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-03-05</th>\n",
       "      <td>-0.046831</td>\n",
       "      <td>-3.38</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-1.40</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.052831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-06</th>\n",
       "      <td>-0.165779</td>\n",
       "      <td>-1.78</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-1.49</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.171779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-09</th>\n",
       "      <td>-0.446458</td>\n",
       "      <td>-7.78</td>\n",
       "      <td>-1.33</td>\n",
       "      <td>-4.75</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.006</td>\n",
       "      <td>3.22</td>\n",
       "      <td>-0.452458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-10</th>\n",
       "      <td>0.104167</td>\n",
       "      <td>4.74</td>\n",
       "      <td>-2.39</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.098167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                FANG  Mkt-RF   SMB   HML   RMW   CMA     RF   Mom  \\\n",
       "Date                                                                \n",
       "2020-03-05 -0.046831   -3.38 -0.14 -1.40 -0.88  0.05  0.006  0.46   \n",
       "2020-03-06 -0.165779   -1.78 -0.21 -1.49  0.69  0.34  0.006  0.46   \n",
       "2020-03-09 -0.446458   -7.78 -1.33 -4.75  0.08  0.46  0.006  3.22   \n",
       "2020-03-10  0.104167    4.74 -2.39  0.81  0.52  0.06  0.006  0.57   \n",
       "\n",
       "            Excess Returns  \n",
       "Date                        \n",
       "2020-03-05       -0.052831  \n",
       "2020-03-06       -0.171779  \n",
       "2020-03-09       -0.452458  \n",
       "2020-03-10        0.098167  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df_ticker_with_fama_french(ticker, date, df_filtered, length_history, fama_french_data):\n",
    "    \"\"\"\n",
    "    Merges the filtered DataFrame with Fama-French data based on ticker and date.\n",
    "\n",
    "    Args:\n",
    "        ticker (str): Ticker symbol of the stock.\n",
    "        date (str): Date in the format 'YYYY-MM-DD'.\n",
    "        df_filtered (pd.DataFrame): Filtered DataFrame containing stock data.\n",
    "        length_history (int): Number of historical data points to consider.\n",
    "        fama_french_data (pd.DataFrame): DataFrame containing Fama-French data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged DataFrame with Fama-French data.\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert index to datetime if not already\n",
    "    for df in [df_filtered, fama_french_data]:\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Get a sub DataFrame based on ticker, date, and length_history\n",
    "    sub_df = get_sub_df_ticker(ticker, date, df_filtered, length_history)\n",
    "\n",
    "    # Merge sub DataFrame with Fama-French data\n",
    "    merged_df = sub_df.join(fama_french_data, how='left') # or inner\n",
    "    merged_df[\"Excess Returns\"] = merged_df[ticker] - merged_df[\"RF\"] \n",
    "    return merged_df\n",
    "\n",
    "df_ticker_with_fama_french(\"FANG\", '2020-03-10', df_filtered, 4, fama_french_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykVqmW4PQe5T"
   },
   "source": [
    "---\n",
    "<font color=green>Q10: (5 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Write a Python function named `extract_beta_fama_french` to perform a rolling regression analysis for a given stock at a specific time point using the Fama-French model. The function should accept the following parameters:\n",
    "\n",
    "- `ticker`: A string indicating the stock symbol.\n",
    "- `date`: A string specifying the date for the analysis.\n",
    "- `length_history`: An integer representing the number of days of historical data to include.\n",
    "- `df_filtered`: A pandas DataFrame (assumed to be derived from question 5) containing filtered stock data.\n",
    "- `fama_french_data`: A pandas DataFrame (assumed to be from question 7) that includes Fama-French factors.\n",
    "\n",
    "Utilize the `statsmodels.api` library to conduct the regression.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "VGSUB3UDxN2B"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def extract_beta_fama_french(ticker, date, df_filtered, length_history, fama_french_data):\n",
    "    \"\"\"\n",
    "    Extracts the beta using Fama-French factors for a given ticker and date.\n",
    "\n",
    "    Parameters:\n",
    "    ticker (str): The ticker symbol of the stock.\n",
    "    date (str): The date for which the beta is calculated.\n",
    "    df_filtered (pandas.DataFrame): The filtered dataframe containing the stock data.\n",
    "    length_history (int): The length of historical data to consider for beta calculation.\n",
    "    fama_french_data (pandas.DataFrame): The Fama-French factors data.\n",
    "\n",
    "    Returns:\n",
    "    statsmodels.regression.linear_model.RegressionResultsWrapper: The regression model object.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the data for regression\n",
    "    data_for_regression = df_ticker_with_fama_french(ticker, date, df_filtered, length_history, fama_french_data)\n",
    "    \n",
    "    # Extract the independent variables\n",
    "    constant_x = data_for_regression[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA','Mom']] \n",
    "    \n",
    "    # Extract the dependent variable\n",
    "    predictor = data_for_regression['Excess Returns'] \n",
    "    \n",
    "    # Add a constant term to the independent variables\n",
    "    constant = sm.add_constant(constant_x)  \n",
    "    \n",
    "    # Fit the regression model\n",
    "    model = sm.OLS(predictor, constant).fit() \n",
    "    \n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY8FqrBjRDTz"
   },
   "source": [
    "---\n",
    "<font color=green>Q11: (2 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Apply the `extract_beta_fama_french` function to the stock symbol 'AAPL' for the date '2024-03-28', using a historical data length of 252 days. Ensure that the `df_filtered` and `fama_french_data` DataFrames are correctly prepared and available in your environment before executing this function. The parameters for the function call are set as follows:\n",
    "\n",
    "- **Ticker**: 'AAPL'\n",
    "- **Date**: '2024-03-28'\n",
    "- **Length of History**: 252 days\n",
    "</font>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "fo6C5-qXxPZO"
   },
   "outputs": [],
   "source": [
    "# Define the ticker, date, length history and the result\n",
    "\n",
    "ticker = 'AAPL'\n",
    "date = '2024-03-28'\n",
    "length_history = 252\n",
    "result = extract_beta_fama_french(ticker, date, df_filtered, length_history, fama_french_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DyA4G1d0HjY"
   },
   "source": [
    "---\n",
    "<font color=green>Q12: (2 Marks)</font>\n",
    "<br><font color='green'>\n",
    "Once the `extract_beta_fama_french` function has been applied to 'AAPL' with the specified parameters, the next step is to analyze the regression summary to identify which Fama-French factor explains the most variance in 'AAPL' returns during the specified period.\n",
    "\n",
    "Follow these steps to perform the analysis:\n",
    "\n",
    "1. **Review the Summary**: Examine the regression output, focusing on the coefficients and their statistical significance (p-values).\n",
    "2. **Identify Key Factor**: Determine which factor has the highest absolute coefficient value and is statistically significant (typically p < 0.05). This factor can be considered as having the strongest influence on 'AAPL' returns for the period.\n",
    "\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thaIsaiFWWNO"
   },
   "source": [
    "**Write your answers here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>Excess Returns</td>  <th>  R-squared:         </th> <td>   0.475</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.462</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   36.96</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 07 Jun 2024</td> <th>  Prob (F-statistic):</th> <td>9.04e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:44:53</td>     <th>  Log-Likelihood:    </th> <td>  827.28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   252</td>      <th>  AIC:               </th> <td>  -1641.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   245</td>      <th>  BIC:               </th> <td>  -1616.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>  <td>   -0.0207</td> <td>    0.001</td> <td>  -35.204</td> <td> 0.000</td> <td>   -0.022</td> <td>   -0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mkt-RF</th> <td>    0.0111</td> <td>    0.001</td> <td>   11.966</td> <td> 0.000</td> <td>    0.009</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SMB</th>    <td>    0.0008</td> <td>    0.001</td> <td>    0.690</td> <td> 0.491</td> <td>   -0.001</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HML</th>    <td>   -0.0076</td> <td>    0.001</td> <td>   -5.762</td> <td> 0.000</td> <td>   -0.010</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RMW</th>    <td>    0.0053</td> <td>    0.001</td> <td>    3.607</td> <td> 0.000</td> <td>    0.002</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CMA</th>    <td>    0.0019</td> <td>    0.002</td> <td>    1.052</td> <td> 0.294</td> <td>   -0.002</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mom</th>    <td>   -0.0015</td> <td>    0.001</td> <td>   -1.592</td> <td> 0.113</td> <td>   -0.003</td> <td>    0.000</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>36.898</td> <th>  Durbin-Watson:     </th> <td>   1.622</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  83.475</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.702</td> <th>  Prob(JB):          </th> <td>7.48e-19</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.445</td> <th>  Cond. No.          </th> <td>    4.05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &  Excess Returns  & \\textbf{  R-squared:         } &     0.475   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.462   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     36.96   \\\\\n",
       "\\textbf{Date:}             & Fri, 07 Jun 2024 & \\textbf{  Prob (F-statistic):} &  9.04e-32   \\\\\n",
       "\\textbf{Time:}             &     15:44:53     & \\textbf{  Log-Likelihood:    } &    827.28   \\\\\n",
       "\\textbf{No. Observations:} &         252      & \\textbf{  AIC:               } &    -1641.   \\\\\n",
       "\\textbf{Df Residuals:}     &         245      & \\textbf{  BIC:               } &    -1616.   \\\\\n",
       "\\textbf{Df Model:}         &           6      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}  &      -0.0207  &        0.001     &   -35.204  &         0.000        &       -0.022    &       -0.020     \\\\\n",
       "\\textbf{Mkt-RF} &       0.0111  &        0.001     &    11.966  &         0.000        &        0.009    &        0.013     \\\\\n",
       "\\textbf{SMB}    &       0.0008  &        0.001     &     0.690  &         0.491        &       -0.001    &        0.003     \\\\\n",
       "\\textbf{HML}    &      -0.0076  &        0.001     &    -5.762  &         0.000        &       -0.010    &       -0.005     \\\\\n",
       "\\textbf{RMW}    &       0.0053  &        0.001     &     3.607  &         0.000        &        0.002    &        0.008     \\\\\n",
       "\\textbf{CMA}    &       0.0019  &        0.002     &     1.052  &         0.294        &       -0.002    &        0.006     \\\\\n",
       "\\textbf{Mom}    &      -0.0015  &        0.001     &    -1.592  &         0.113        &       -0.003    &        0.000     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 36.898 & \\textbf{  Durbin-Watson:     } &    1.622  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.000 & \\textbf{  Jarque-Bera (JB):  } &   83.475  \\\\\n",
       "\\textbf{Skew:}          & -0.702 & \\textbf{  Prob(JB):          } & 7.48e-19  \\\\\n",
       "\\textbf{Kurtosis:}      &  5.445 & \\textbf{  Cond. No.          } &     4.05  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:         Excess Returns   R-squared:                       0.475\n",
       "Model:                            OLS   Adj. R-squared:                  0.462\n",
       "Method:                 Least Squares   F-statistic:                     36.96\n",
       "Date:                Fri, 07 Jun 2024   Prob (F-statistic):           9.04e-32\n",
       "Time:                        15:44:53   Log-Likelihood:                 827.28\n",
       "No. Observations:                 252   AIC:                            -1641.\n",
       "Df Residuals:                     245   BIC:                            -1616.\n",
       "Df Model:                           6                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -0.0207      0.001    -35.204      0.000      -0.022      -0.020\n",
       "Mkt-RF         0.0111      0.001     11.966      0.000       0.009       0.013\n",
       "SMB            0.0008      0.001      0.690      0.491      -0.001       0.003\n",
       "HML           -0.0076      0.001     -5.762      0.000      -0.010      -0.005\n",
       "RMW            0.0053      0.001      3.607      0.000       0.002       0.008\n",
       "CMA            0.0019      0.002      1.052      0.294      -0.002       0.006\n",
       "Mom           -0.0015      0.001     -1.592      0.113      -0.003       0.000\n",
       "==============================================================================\n",
       "Omnibus:                       36.898   Durbin-Watson:                   1.622\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               83.475\n",
       "Skew:                          -0.702   Prob(JB):                     7.48e-19\n",
       "Kurtosis:                       5.445   Cond. No.                         4.05\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the summaries\n",
    "summary =result.summary()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regression Summary:\n",
      "                             OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:         Excess Returns   R-squared:                       0.475\n",
      "Model:                            OLS   Adj. R-squared:                  0.462\n",
      "Method:                 Least Squares   F-statistic:                     36.96\n",
      "Date:                Fri, 07 Jun 2024   Prob (F-statistic):           9.04e-32\n",
      "Time:                        15:44:53   Log-Likelihood:                 827.28\n",
      "No. Observations:                 252   AIC:                            -1641.\n",
      "Df Residuals:                     245   BIC:                            -1616.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0207      0.001    -35.204      0.000      -0.022      -0.020\n",
      "Mkt-RF         0.0111      0.001     11.966      0.000       0.009       0.013\n",
      "SMB            0.0008      0.001      0.690      0.491      -0.001       0.003\n",
      "HML           -0.0076      0.001     -5.762      0.000      -0.010      -0.005\n",
      "RMW            0.0053      0.001      3.607      0.000       0.002       0.008\n",
      "CMA            0.0019      0.002      1.052      0.294      -0.002       0.006\n",
      "Mom           -0.0015      0.001     -1.592      0.113      -0.003       0.000\n",
      "==============================================================================\n",
      "Omnibus:                       36.898   Durbin-Watson:                   1.622\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               83.475\n",
      "Skew:                          -0.702   Prob(JB):                     7.48e-19\n",
      "Kurtosis:                       5.445   Cond. No.                         4.05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRegression Summary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-Values:\n",
      " const     8.072684e-98\n",
      "Mkt-RF    2.723353e-26\n",
      "SMB       4.908369e-01\n",
      "HML       2.481186e-08\n",
      "RMW       3.759857e-04\n",
      "CMA       2.940650e-01\n",
      "Mom       1.126391e-01\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "p_values = result.pvalues\n",
    "print(\"P-Values:\\n\", p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const    -0.020742\n",
       "Mkt-RF    0.011080\n",
       "SMB       0.000766\n",
       "HML      -0.007594\n",
       "RMW       0.005255\n",
       "CMA       0.001945\n",
       "Mom      -0.001470\n",
       "dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients = result.params\n",
    "coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor: Mkt-RF Coefficient: 0.011079574327502763\n"
     ]
    }
   ],
   "source": [
    "# Filtering factors that are statistically significant\n",
    "significant_factors = coefficients[p_values < 0.05]\n",
    "\n",
    "# Sorting the significant factors by their absolute coefficients in descending order\n",
    "sorted_factors = significant_factors.sort_values(ascending=False)\n",
    "\n",
    "top_two_factors = sorted_factors.head(1)\n",
    "\n",
    "# Print the top two factors and their coefficients\n",
    "for factor in top_two_factors.index:\n",
    "    print(\"Factor:\", factor, \"Coefficient:\", significant_factors[factor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As calculated above, for the ticker 'AAPL' on the 28th of March 2024, using 252 days as the data length, the most significant factor is the 'Mkt-RF' factor, representing the market risk premium, with a coefficient of 0.0111, indicating a positive and substantial impact on Excess Returns. This is statistically significant with a p-value less than 0.05. Additionally, the 'HML' factor, which accounts for the difference between high and low book-to-market stocks, also shows a significant negative influence with a coefficient of -0.0076. Despite its negative value, its impact is considerable, underscored by its statistical significance. These findings suggest that market risk and value factors are important in driving the Excess Returns, with the market risk premium being the most crucial determinant from this regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pj7qlAq-J8N2"
   },
   "source": [
    "# PCA Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qty6V05YXxti"
   },
   "source": [
    "\n",
    "In literature, another method exists for extracting residuals for each stock, utilizing the PCA approach to identify hidden factors in the data. Let's describe this method.\n",
    "\n",
    "The return of a stock $R_i^t$ at time $t$ can be modeled as follows :\n",
    "\n",
    "$$\n",
    "R_i^t  = \\sum_{j=1}^m\\beta_{i,j}^t F_j^t  + \\epsilon_i^t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ R_i^t $ is the return of stock $i$ at time $t$\n",
    "- $m$ is the number of factors selected from PCA\n",
    "-  $ F_j^t $ is the $j$-th hidden factor constructed from PCA at time $t$\n",
    "- $\\beta_{i,j}^t $ are the coefficients representing the sensitivity of the stock returns to each hidden factor.\n",
    "- $\\epsilon_i^t $  is the residual term for stock $i$ at time $t$, representing the portion of the return not explained by the PCA factors.\n",
    "\n",
    "### Representation of Stock Return Data\n",
    "\n",
    "Consider the return data for $N$ stocks over $T$ periods, represented by the matrix $R$ of size $T \\times N$:\n",
    "\n",
    "$$\n",
    "R = \\left[\n",
    "\\begin{array}{cccc}\n",
    "R_1^T & R_2^T & \\cdots & R_N^T \\\\\n",
    "R_1^{T-1} & R_2^{T-1} & \\cdots & R_N^{T-1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "R_1^1 & R_2^1 & \\cdots & R_N^1 \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Each element $R_i^k$ of the matrix represents the return of stock $i$ at time $k$ and is defined as:\n",
    "\n",
    "$$\n",
    "R_i^k = \\frac{S_{i,k} - S_{i, k-1}}{S_{i, k-1}}, \\quad k=1,\\cdots, T, \\quad i=1,\\cdots,N\n",
    "$$\n",
    "\n",
    "where $S_{i,k}$ denotes the adjusted close price of stock $i$ at time $k$.\n",
    "\n",
    "### Standardization of Returns\n",
    "\n",
    "To adjust for varying volatilities across stocks, we standardize the returns as follows:\n",
    "\n",
    "$$\n",
    "Z_i^t = \\frac{R_i^t - \\mu_i}{\\sigma_i}\n",
    "$$\n",
    "\n",
    "where $\\mu_i$ and $\\sigma_i$ are the mean and standard deviation of returns for stock $i$ over the period $[t-T, t]$, respectively.\n",
    "\n",
    "### Empirical Correlation Matrix\n",
    "\n",
    "The empirical correlation matrix $C$ is computed from the standardized returns:\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{T-1} Z^T Z\n",
    "$$\n",
    "\n",
    "where $Z^T$ is the transpose of matrix $Z$.\n",
    "\n",
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "We apply Singular Value Decomposition to the correlation matrix $C$:\n",
    "\n",
    "$$\n",
    "C = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "Here, $U$ and $V$ are orthogonal matrices representing the left and right singular vectors, respectively, and $\\Sigma$ is a diagonal matrix containing the singular values, which are the square roots of the eigenvalues.\n",
    "\n",
    "### Construction of Hidden Factors\n",
    "\n",
    "For each of the top $m$ components, we construct the selected hidden factors as follows:\n",
    "\n",
    "$$\n",
    "F_j^t = \\sum_{i=1}^N \\frac{\\lambda_{i,j}}{\\sigma_i} R_i^t\n",
    "$$\n",
    "\n",
    "where $\\lambda_{i,j}$ is the $i$-th component of the $j$-th eigenvector (ranked by eigenvalue magnitude).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIkNjUbFZ3Wy"
   },
   "source": [
    "---\n",
    "<font color=green>Q13 (3 Marks):\n",
    "\n",
    "For the specified period from March 29, 2023 ('2023-03-29'), to March 28, 2024 ('2024-03-28'), generate the matrix $Z$ by standardizing the stock returns using the DataFrame `df_filtered_new`\n",
    "</font>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "AqhmrY9xcbnk"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADP</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AMD</th>\n",
       "      <th>AEP</th>\n",
       "      <th>AMGN</th>\n",
       "      <th>ADI</th>\n",
       "      <th>ANSS</th>\n",
       "      <th>...</th>\n",
       "      <th>TTWO</th>\n",
       "      <th>TMUS</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>TXN</th>\n",
       "      <th>VRSK</th>\n",
       "      <th>VRTX</th>\n",
       "      <th>WBA</th>\n",
       "      <th>WBD</th>\n",
       "      <th>WDAY</th>\n",
       "      <th>XEL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-03-29</th>\n",
       "      <td>0.015426</td>\n",
       "      <td>0.026850</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.005328</td>\n",
       "      <td>0.030954</td>\n",
       "      <td>0.016180</td>\n",
       "      <td>0.013156</td>\n",
       "      <td>0.010693</td>\n",
       "      <td>0.030338</td>\n",
       "      <td>0.007428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009693</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.024790</td>\n",
       "      <td>0.020001</td>\n",
       "      <td>0.013639</td>\n",
       "      <td>0.003160</td>\n",
       "      <td>0.009758</td>\n",
       "      <td>0.012431</td>\n",
       "      <td>0.045181</td>\n",
       "      <td>0.017431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-30</th>\n",
       "      <td>0.007226</td>\n",
       "      <td>-0.001864</td>\n",
       "      <td>-0.004931</td>\n",
       "      <td>-0.005692</td>\n",
       "      <td>0.017456</td>\n",
       "      <td>0.018628</td>\n",
       "      <td>0.003662</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>0.026643</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000686</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>0.007221</td>\n",
       "      <td>0.017676</td>\n",
       "      <td>0.003732</td>\n",
       "      <td>-0.006619</td>\n",
       "      <td>0.014056</td>\n",
       "      <td>0.012960</td>\n",
       "      <td>0.009079</td>\n",
       "      <td>0.005561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-31</th>\n",
       "      <td>0.009086</td>\n",
       "      <td>0.014306</td>\n",
       "      <td>0.028150</td>\n",
       "      <td>0.026451</td>\n",
       "      <td>0.012647</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.015499</td>\n",
       "      <td>0.020734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023244</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.062372</td>\n",
       "      <td>0.009607</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>0.009322</td>\n",
       "      <td>-0.001444</td>\n",
       "      <td>0.016835</td>\n",
       "      <td>0.032442</td>\n",
       "      <td>0.007921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-03</th>\n",
       "      <td>-0.013727</td>\n",
       "      <td>-0.026142</td>\n",
       "      <td>0.006073</td>\n",
       "      <td>0.008750</td>\n",
       "      <td>-0.008520</td>\n",
       "      <td>-0.014794</td>\n",
       "      <td>-0.000989</td>\n",
       "      <td>0.011127</td>\n",
       "      <td>-0.004766</td>\n",
       "      <td>-0.010216</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005113</td>\n",
       "      <td>0.013256</td>\n",
       "      <td>-0.061168</td>\n",
       "      <td>-0.009946</td>\n",
       "      <td>-0.001459</td>\n",
       "      <td>0.003999</td>\n",
       "      <td>0.025448</td>\n",
       "      <td>-0.019205</td>\n",
       "      <td>-0.010506</td>\n",
       "      <td>0.001186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-04</th>\n",
       "      <td>0.013339</td>\n",
       "      <td>-0.012776</td>\n",
       "      <td>0.003450</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>-0.007146</td>\n",
       "      <td>0.002970</td>\n",
       "      <td>0.012641</td>\n",
       "      <td>-0.005910</td>\n",
       "      <td>-0.001943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024855</td>\n",
       "      <td>-0.003066</td>\n",
       "      <td>-0.011244</td>\n",
       "      <td>-0.020417</td>\n",
       "      <td>-0.005168</td>\n",
       "      <td>-0.006038</td>\n",
       "      <td>0.010998</td>\n",
       "      <td>0.022957</td>\n",
       "      <td>-0.008661</td>\n",
       "      <td>0.013774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-22</th>\n",
       "      <td>-0.022944</td>\n",
       "      <td>-0.005383</td>\n",
       "      <td>0.021477</td>\n",
       "      <td>0.020371</td>\n",
       "      <td>0.004042</td>\n",
       "      <td>0.005429</td>\n",
       "      <td>-0.001805</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>-0.008404</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001906</td>\n",
       "      <td>-0.001988</td>\n",
       "      <td>-0.011515</td>\n",
       "      <td>-0.000637</td>\n",
       "      <td>-0.004451</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>-0.010577</td>\n",
       "      <td>-0.032110</td>\n",
       "      <td>0.003653</td>\n",
       "      <td>-0.000574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-25</th>\n",
       "      <td>0.015435</td>\n",
       "      <td>-0.013772</td>\n",
       "      <td>-0.004643</td>\n",
       "      <td>-0.004085</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>-0.005678</td>\n",
       "      <td>-0.000964</td>\n",
       "      <td>0.016837</td>\n",
       "      <td>-0.014883</td>\n",
       "      <td>-0.004925</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041454</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.010537</td>\n",
       "      <td>-0.009450</td>\n",
       "      <td>-0.012177</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>-0.006559</td>\n",
       "      <td>0.004018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-26</th>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003998</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>-0.007846</td>\n",
       "      <td>-0.004255</td>\n",
       "      <td>-0.004706</td>\n",
       "      <td>0.003383</td>\n",
       "      <td>-0.008760</td>\n",
       "      <td>0.006676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>0.029195</td>\n",
       "      <td>-0.017208</td>\n",
       "      <td>-0.003405</td>\n",
       "      <td>0.005841</td>\n",
       "      <td>-0.005817</td>\n",
       "      <td>-0.009456</td>\n",
       "      <td>0.006167</td>\n",
       "      <td>-0.012957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-27</th>\n",
       "      <td>-0.006304</td>\n",
       "      <td>0.013302</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.008581</td>\n",
       "      <td>0.009670</td>\n",
       "      <td>0.028128</td>\n",
       "      <td>0.016077</td>\n",
       "      <td>0.023127</td>\n",
       "      <td>-0.005460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.005649</td>\n",
       "      <td>0.012157</td>\n",
       "      <td>0.029540</td>\n",
       "      <td>0.011764</td>\n",
       "      <td>-0.002724</td>\n",
       "      <td>0.024866</td>\n",
       "      <td>0.031026</td>\n",
       "      <td>-0.013555</td>\n",
       "      <td>0.030502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-28</th>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.005678</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>0.003058</td>\n",
       "      <td>0.005011</td>\n",
       "      <td>0.015330</td>\n",
       "      <td>-0.006916</td>\n",
       "      <td>0.023069</td>\n",
       "      <td>-0.002213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010686</td>\n",
       "      <td>0.007469</td>\n",
       "      <td>-0.022466</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.007695</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.031874</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>-0.003180</td>\n",
       "      <td>0.006931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ADBE       ADP     GOOGL      GOOG      AMZN       AMD  \\\n",
       "Date                                                                     \n",
       "2023-03-29  0.015426  0.026850  0.003563  0.005328  0.030954  0.016180   \n",
       "2023-03-30  0.007226 -0.001864 -0.004931 -0.005692  0.017456  0.018628   \n",
       "2023-03-31  0.009086  0.014306  0.028150  0.026451  0.012647  0.001328   \n",
       "2023-04-03 -0.013727 -0.026142  0.006073  0.008750 -0.008520 -0.014794   \n",
       "2023-04-04  0.013339 -0.012776  0.003450  0.002002  0.015038 -0.007146   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2024-03-22 -0.022944 -0.005383  0.021477  0.020371  0.004042  0.005429   \n",
       "2024-03-25  0.015435 -0.013772 -0.004643 -0.004085  0.004696 -0.005678   \n",
       "2024-03-26  0.000729  0.003563  0.003998  0.003639 -0.007846 -0.004255   \n",
       "2024-03-27 -0.006304  0.013302  0.001327  0.001582  0.008581  0.009670   \n",
       "2024-03-28  0.000397  0.005678  0.000398  0.002106  0.003058  0.005011   \n",
       "\n",
       "                 AEP      AMGN       ADI      ANSS  ...      TTWO      TMUS  \\\n",
       "Date                                                ...                       \n",
       "2023-03-29  0.013156  0.010693  0.030338  0.007428  ...  0.009693  0.007143   \n",
       "2023-03-30  0.003662  0.001950  0.026643  0.018652  ... -0.000686  0.005285   \n",
       "2023-03-31  0.006193  0.001035  0.015499  0.020734  ...  0.023244  0.001868   \n",
       "2023-04-03 -0.000989  0.011127 -0.004766 -0.010216  ... -0.005113  0.013256   \n",
       "2023-04-04  0.002970  0.012641 -0.005910 -0.001943  ...  0.024855 -0.003066   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "2024-03-22 -0.001805 -0.002816 -0.008404  0.002987  ...  0.001906 -0.001988   \n",
       "2024-03-25 -0.000964  0.016837 -0.014883 -0.004925  ... -0.041454  0.003175   \n",
       "2024-03-26 -0.004706  0.003383 -0.008760  0.006676  ...  0.003627 -0.000124   \n",
       "2024-03-27  0.028128  0.016077  0.023127 -0.005460  ...  0.001705  0.005649   \n",
       "2024-03-28  0.015330 -0.006916  0.023069 -0.002213  ...  0.010686  0.007469   \n",
       "\n",
       "                TSLA       TXN      VRSK      VRTX       WBA       WBD  \\\n",
       "Date                                                                     \n",
       "2023-03-29  0.024790  0.020001  0.013639  0.003160  0.009758  0.012431   \n",
       "2023-03-30  0.007221  0.017676  0.003732 -0.006619  0.014056  0.012960   \n",
       "2023-03-31  0.062372  0.009607  0.004713  0.009322 -0.001444  0.016835   \n",
       "2023-04-03 -0.061168 -0.009946 -0.001459  0.003999  0.025448 -0.019205   \n",
       "2023-04-04 -0.011244 -0.020417 -0.005168 -0.006038  0.010998  0.022957   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2024-03-22 -0.011515 -0.000637 -0.004451 -0.000120 -0.010577 -0.032110   \n",
       "2024-03-25  0.010537 -0.009450 -0.012177  0.000890  0.002430  0.002370   \n",
       "2024-03-26  0.029195 -0.017208 -0.003405  0.005841 -0.005817 -0.009456   \n",
       "2024-03-27  0.012157  0.029540  0.011764 -0.002724  0.024866  0.031026   \n",
       "2024-03-28 -0.022466  0.007752  0.007695  0.001653  0.031874  0.010417   \n",
       "\n",
       "                WDAY       XEL  \n",
       "Date                            \n",
       "2023-03-29  0.045181  0.017431  \n",
       "2023-03-30  0.009079  0.005561  \n",
       "2023-03-31  0.032442  0.007921  \n",
       "2023-04-03 -0.010506  0.001186  \n",
       "2023-04-04 -0.008661  0.013774  \n",
       "...              ...       ...  \n",
       "2024-03-22  0.003653 -0.000574  \n",
       "2024-03-25 -0.006559  0.004018  \n",
       "2024-03-26  0.006167 -0.012957  \n",
       "2024-03-27 -0.013555  0.030502  \n",
       "2024-03-28 -0.003180  0.006931  \n",
       "\n",
       "[252 rows x 89 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the new dataframe within the specified dated\n",
    "df_filtered_new = df_filtered.loc['2023-03-29':'2024-03-28']\n",
    "df_filtered_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADP</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AMD</th>\n",
       "      <th>AEP</th>\n",
       "      <th>AMGN</th>\n",
       "      <th>ADI</th>\n",
       "      <th>ANSS</th>\n",
       "      <th>...</th>\n",
       "      <th>TTWO</th>\n",
       "      <th>TMUS</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>TXN</th>\n",
       "      <th>VRSK</th>\n",
       "      <th>VRTX</th>\n",
       "      <th>WBA</th>\n",
       "      <th>WBD</th>\n",
       "      <th>WDAY</th>\n",
       "      <th>XEL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-03-29</th>\n",
       "      <td>0.658925</td>\n",
       "      <td>2.189521</td>\n",
       "      <td>0.106285</td>\n",
       "      <td>0.207821</td>\n",
       "      <td>1.503570</td>\n",
       "      <td>0.444433</td>\n",
       "      <td>1.020467</td>\n",
       "      <td>0.727398</td>\n",
       "      <td>1.860839</td>\n",
       "      <td>0.356414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517876</td>\n",
       "      <td>0.615242</td>\n",
       "      <td>0.814986</td>\n",
       "      <td>1.344650</td>\n",
       "      <td>1.105776</td>\n",
       "      <td>0.127319</td>\n",
       "      <td>0.497115</td>\n",
       "      <td>0.429736</td>\n",
       "      <td>2.277783</td>\n",
       "      <td>1.269711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-30</th>\n",
       "      <td>0.273051</td>\n",
       "      <td>-0.221306</td>\n",
       "      <td>-0.389329</td>\n",
       "      <td>-0.434766</td>\n",
       "      <td>0.787034</td>\n",
       "      <td>0.526996</td>\n",
       "      <td>0.277291</td>\n",
       "      <td>0.076716</td>\n",
       "      <td>1.630406</td>\n",
       "      <td>0.936497</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109884</td>\n",
       "      <td>0.439972</td>\n",
       "      <td>0.233493</td>\n",
       "      <td>1.187056</td>\n",
       "      <td>0.239468</td>\n",
       "      <td>-0.525980</td>\n",
       "      <td>0.691243</td>\n",
       "      <td>0.446100</td>\n",
       "      <td>0.389915</td>\n",
       "      <td>0.430715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-31</th>\n",
       "      <td>0.360570</td>\n",
       "      <td>1.136308</td>\n",
       "      <td>1.540732</td>\n",
       "      <td>1.439604</td>\n",
       "      <td>0.531735</td>\n",
       "      <td>-0.056439</td>\n",
       "      <td>0.475365</td>\n",
       "      <td>0.008639</td>\n",
       "      <td>0.935425</td>\n",
       "      <td>1.044069</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337544</td>\n",
       "      <td>0.117646</td>\n",
       "      <td>2.058867</td>\n",
       "      <td>0.640237</td>\n",
       "      <td>0.325258</td>\n",
       "      <td>0.538943</td>\n",
       "      <td>-0.008816</td>\n",
       "      <td>0.565842</td>\n",
       "      <td>1.611597</td>\n",
       "      <td>0.597545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-03</th>\n",
       "      <td>-0.713053</td>\n",
       "      <td>-2.259591</td>\n",
       "      <td>0.252735</td>\n",
       "      <td>0.407399</td>\n",
       "      <td>-0.591892</td>\n",
       "      <td>-0.600163</td>\n",
       "      <td>-0.086824</td>\n",
       "      <td>0.759731</td>\n",
       "      <td>-0.328378</td>\n",
       "      <td>-0.555499</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.377687</td>\n",
       "      <td>1.191774</td>\n",
       "      <td>-2.030038</td>\n",
       "      <td>-0.684867</td>\n",
       "      <td>-0.214460</td>\n",
       "      <td>0.183344</td>\n",
       "      <td>1.205798</td>\n",
       "      <td>-0.547975</td>\n",
       "      <td>-0.634285</td>\n",
       "      <td>0.121519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-04</th>\n",
       "      <td>0.560730</td>\n",
       "      <td>-1.137430</td>\n",
       "      <td>0.099652</td>\n",
       "      <td>0.013877</td>\n",
       "      <td>0.658632</td>\n",
       "      <td>-0.342217</td>\n",
       "      <td>0.223111</td>\n",
       "      <td>0.872403</td>\n",
       "      <td>-0.399698</td>\n",
       "      <td>-0.127907</td>\n",
       "      <td>...</td>\n",
       "      <td>1.434972</td>\n",
       "      <td>-0.347689</td>\n",
       "      <td>-0.377655</td>\n",
       "      <td>-1.394525</td>\n",
       "      <td>-0.538698</td>\n",
       "      <td>-0.487155</td>\n",
       "      <td>0.553153</td>\n",
       "      <td>0.755053</td>\n",
       "      <td>-0.537767</td>\n",
       "      <td>1.011197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-22</th>\n",
       "      <td>-1.146803</td>\n",
       "      <td>-0.516681</td>\n",
       "      <td>1.151431</td>\n",
       "      <td>1.085070</td>\n",
       "      <td>0.074915</td>\n",
       "      <td>0.081847</td>\n",
       "      <td>-0.150701</td>\n",
       "      <td>-0.278013</td>\n",
       "      <td>-0.555224</td>\n",
       "      <td>0.126867</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046870</td>\n",
       "      <td>-0.246035</td>\n",
       "      <td>-0.386620</td>\n",
       "      <td>-0.054031</td>\n",
       "      <td>-0.476030</td>\n",
       "      <td>-0.091837</td>\n",
       "      <td>-0.421327</td>\n",
       "      <td>-0.946797</td>\n",
       "      <td>0.106149</td>\n",
       "      <td>-0.002851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-25</th>\n",
       "      <td>0.659349</td>\n",
       "      <td>-1.221008</td>\n",
       "      <td>-0.372489</td>\n",
       "      <td>-0.341074</td>\n",
       "      <td>0.109668</td>\n",
       "      <td>-0.292706</td>\n",
       "      <td>-0.084892</td>\n",
       "      <td>1.184710</td>\n",
       "      <td>-0.959285</td>\n",
       "      <td>-0.282029</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.575803</td>\n",
       "      <td>0.240999</td>\n",
       "      <td>0.343241</td>\n",
       "      <td>-0.651297</td>\n",
       "      <td>-1.151639</td>\n",
       "      <td>-0.024341</td>\n",
       "      <td>0.166127</td>\n",
       "      <td>0.118796</td>\n",
       "      <td>-0.427841</td>\n",
       "      <td>0.321648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-26</th>\n",
       "      <td>-0.032708</td>\n",
       "      <td>0.234343</td>\n",
       "      <td>0.131651</td>\n",
       "      <td>0.109342</td>\n",
       "      <td>-0.556130</td>\n",
       "      <td>-0.244717</td>\n",
       "      <td>-0.377797</td>\n",
       "      <td>0.183364</td>\n",
       "      <td>-0.577464</td>\n",
       "      <td>0.317533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150958</td>\n",
       "      <td>-0.070195</td>\n",
       "      <td>0.960796</td>\n",
       "      <td>-1.177043</td>\n",
       "      <td>-0.384593</td>\n",
       "      <td>0.306386</td>\n",
       "      <td>-0.206326</td>\n",
       "      <td>-0.246683</td>\n",
       "      <td>0.237594</td>\n",
       "      <td>-0.878121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-27</th>\n",
       "      <td>-0.363721</td>\n",
       "      <td>1.052056</td>\n",
       "      <td>-0.024166</td>\n",
       "      <td>-0.010591</td>\n",
       "      <td>0.315892</td>\n",
       "      <td>0.224881</td>\n",
       "      <td>2.192442</td>\n",
       "      <td>1.128111</td>\n",
       "      <td>1.411127</td>\n",
       "      <td>-0.309662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034693</td>\n",
       "      <td>0.474269</td>\n",
       "      <td>0.396878</td>\n",
       "      <td>1.991081</td>\n",
       "      <td>0.941800</td>\n",
       "      <td>-0.265792</td>\n",
       "      <td>1.179504</td>\n",
       "      <td>1.004419</td>\n",
       "      <td>-0.793726</td>\n",
       "      <td>2.193537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-28</th>\n",
       "      <td>-0.048375</td>\n",
       "      <td>0.411934</td>\n",
       "      <td>-0.078409</td>\n",
       "      <td>0.019962</td>\n",
       "      <td>0.022729</td>\n",
       "      <td>0.067776</td>\n",
       "      <td>1.190631</td>\n",
       "      <td>-0.583106</td>\n",
       "      <td>1.407557</td>\n",
       "      <td>-0.141870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.577964</td>\n",
       "      <td>0.645940</td>\n",
       "      <td>-0.749067</td>\n",
       "      <td>0.514492</td>\n",
       "      <td>0.585964</td>\n",
       "      <td>0.026648</td>\n",
       "      <td>1.496052</td>\n",
       "      <td>0.367481</td>\n",
       "      <td>-0.251144</td>\n",
       "      <td>0.527595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ADBE       ADP     GOOGL      GOOG      AMZN       AMD  \\\n",
       "Date                                                                     \n",
       "2023-03-29  0.658925  2.189521  0.106285  0.207821  1.503570  0.444433   \n",
       "2023-03-30  0.273051 -0.221306 -0.389329 -0.434766  0.787034  0.526996   \n",
       "2023-03-31  0.360570  1.136308  1.540732  1.439604  0.531735 -0.056439   \n",
       "2023-04-03 -0.713053 -2.259591  0.252735  0.407399 -0.591892 -0.600163   \n",
       "2023-04-04  0.560730 -1.137430  0.099652  0.013877  0.658632 -0.342217   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2024-03-22 -1.146803 -0.516681  1.151431  1.085070  0.074915  0.081847   \n",
       "2024-03-25  0.659349 -1.221008 -0.372489 -0.341074  0.109668 -0.292706   \n",
       "2024-03-26 -0.032708  0.234343  0.131651  0.109342 -0.556130 -0.244717   \n",
       "2024-03-27 -0.363721  1.052056 -0.024166 -0.010591  0.315892  0.224881   \n",
       "2024-03-28 -0.048375  0.411934 -0.078409  0.019962  0.022729  0.067776   \n",
       "\n",
       "                 AEP      AMGN       ADI      ANSS  ...      TTWO      TMUS  \\\n",
       "Date                                                ...                       \n",
       "2023-03-29  1.020467  0.727398  1.860839  0.356414  ...  0.517876  0.615242   \n",
       "2023-03-30  0.277291  0.076716  1.630406  0.936497  ... -0.109884  0.439972   \n",
       "2023-03-31  0.475365  0.008639  0.935425  1.044069  ...  1.337544  0.117646   \n",
       "2023-04-03 -0.086824  0.759731 -0.328378 -0.555499  ... -0.377687  1.191774   \n",
       "2023-04-04  0.223111  0.872403 -0.399698 -0.127907  ...  1.434972 -0.347689   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "2024-03-22 -0.150701 -0.278013 -0.555224  0.126867  ...  0.046870 -0.246035   \n",
       "2024-03-25 -0.084892  1.184710 -0.959285 -0.282029  ... -2.575803  0.240999   \n",
       "2024-03-26 -0.377797  0.183364 -0.577464  0.317533  ...  0.150958 -0.070195   \n",
       "2024-03-27  2.192442  1.128111  1.411127 -0.309662  ...  0.034693  0.474269   \n",
       "2024-03-28  1.190631 -0.583106  1.407557 -0.141870  ...  0.577964  0.645940   \n",
       "\n",
       "                TSLA       TXN      VRSK      VRTX       WBA       WBD  \\\n",
       "Date                                                                     \n",
       "2023-03-29  0.814986  1.344650  1.105776  0.127319  0.497115  0.429736   \n",
       "2023-03-30  0.233493  1.187056  0.239468 -0.525980  0.691243  0.446100   \n",
       "2023-03-31  2.058867  0.640237  0.325258  0.538943 -0.008816  0.565842   \n",
       "2023-04-03 -2.030038 -0.684867 -0.214460  0.183344  1.205798 -0.547975   \n",
       "2023-04-04 -0.377655 -1.394525 -0.538698 -0.487155  0.553153  0.755053   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2024-03-22 -0.386620 -0.054031 -0.476030 -0.091837 -0.421327 -0.946797   \n",
       "2024-03-25  0.343241 -0.651297 -1.151639 -0.024341  0.166127  0.118796   \n",
       "2024-03-26  0.960796 -1.177043 -0.384593  0.306386 -0.206326 -0.246683   \n",
       "2024-03-27  0.396878  1.991081  0.941800 -0.265792  1.179504  1.004419   \n",
       "2024-03-28 -0.749067  0.514492  0.585964  0.026648  1.496052  0.367481   \n",
       "\n",
       "                WDAY       XEL  \n",
       "Date                            \n",
       "2023-03-29  2.277783  1.269711  \n",
       "2023-03-30  0.389915  0.430715  \n",
       "2023-03-31  1.611597  0.597545  \n",
       "2023-04-03 -0.634285  0.121519  \n",
       "2023-04-04 -0.537767  1.011197  \n",
       "...              ...       ...  \n",
       "2024-03-22  0.106149 -0.002851  \n",
       "2024-03-25 -0.427841  0.321648  \n",
       "2024-03-26  0.237594 -0.878121  \n",
       "2024-03-27 -0.793726  2.193537  \n",
       "2024-03-28 -0.251144  0.527595  \n",
       "\n",
       "[252 rows x 89 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the Z Matrix\n",
    "Z = (df_filtered_new - df_filtered_new.mean()) / df_filtered_new.std()\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Quv2tOhXch4-"
   },
   "source": [
    "---\n",
    "<font color=green>Q14: (1 Mark) </font>\n",
    "<br><font color='green'>\n",
    "Download the `Z_matrix` matrix from the course's GitHub account.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "NmSz-J_oxYQa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADP</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AMD</th>\n",
       "      <th>AEP</th>\n",
       "      <th>AMGN</th>\n",
       "      <th>ADI</th>\n",
       "      <th>ANSS</th>\n",
       "      <th>...</th>\n",
       "      <th>TTWO</th>\n",
       "      <th>TMUS</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>TXN</th>\n",
       "      <th>VRSK</th>\n",
       "      <th>VRTX</th>\n",
       "      <th>WBA</th>\n",
       "      <th>WBD</th>\n",
       "      <th>WDAY</th>\n",
       "      <th>XEL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-03-29</th>\n",
       "      <td>0.658925</td>\n",
       "      <td>2.189521</td>\n",
       "      <td>0.106285</td>\n",
       "      <td>0.207821</td>\n",
       "      <td>1.503570</td>\n",
       "      <td>0.444433</td>\n",
       "      <td>1.020467</td>\n",
       "      <td>0.727398</td>\n",
       "      <td>1.860839</td>\n",
       "      <td>0.356414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517876</td>\n",
       "      <td>0.615242</td>\n",
       "      <td>0.814986</td>\n",
       "      <td>1.344650</td>\n",
       "      <td>1.105776</td>\n",
       "      <td>0.127319</td>\n",
       "      <td>0.497115</td>\n",
       "      <td>0.429736</td>\n",
       "      <td>2.277783</td>\n",
       "      <td>1.269711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-30</th>\n",
       "      <td>0.273051</td>\n",
       "      <td>-0.221306</td>\n",
       "      <td>-0.389329</td>\n",
       "      <td>-0.434766</td>\n",
       "      <td>0.787034</td>\n",
       "      <td>0.526996</td>\n",
       "      <td>0.277291</td>\n",
       "      <td>0.076716</td>\n",
       "      <td>1.630406</td>\n",
       "      <td>0.936497</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109884</td>\n",
       "      <td>0.439972</td>\n",
       "      <td>0.233493</td>\n",
       "      <td>1.187056</td>\n",
       "      <td>0.239468</td>\n",
       "      <td>-0.525980</td>\n",
       "      <td>0.691243</td>\n",
       "      <td>0.446100</td>\n",
       "      <td>0.389915</td>\n",
       "      <td>0.430715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-31</th>\n",
       "      <td>0.360570</td>\n",
       "      <td>1.136308</td>\n",
       "      <td>1.540732</td>\n",
       "      <td>1.439604</td>\n",
       "      <td>0.531735</td>\n",
       "      <td>-0.056439</td>\n",
       "      <td>0.475365</td>\n",
       "      <td>0.008639</td>\n",
       "      <td>0.935425</td>\n",
       "      <td>1.044069</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337544</td>\n",
       "      <td>0.117646</td>\n",
       "      <td>2.058867</td>\n",
       "      <td>0.640237</td>\n",
       "      <td>0.325258</td>\n",
       "      <td>0.538943</td>\n",
       "      <td>-0.008816</td>\n",
       "      <td>0.565842</td>\n",
       "      <td>1.611597</td>\n",
       "      <td>0.597545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-03</th>\n",
       "      <td>-0.713053</td>\n",
       "      <td>-2.259591</td>\n",
       "      <td>0.252735</td>\n",
       "      <td>0.407399</td>\n",
       "      <td>-0.591892</td>\n",
       "      <td>-0.600163</td>\n",
       "      <td>-0.086824</td>\n",
       "      <td>0.759731</td>\n",
       "      <td>-0.328378</td>\n",
       "      <td>-0.555499</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.377687</td>\n",
       "      <td>1.191774</td>\n",
       "      <td>-2.030038</td>\n",
       "      <td>-0.684867</td>\n",
       "      <td>-0.214460</td>\n",
       "      <td>0.183344</td>\n",
       "      <td>1.205798</td>\n",
       "      <td>-0.547975</td>\n",
       "      <td>-0.634285</td>\n",
       "      <td>0.121519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-04</th>\n",
       "      <td>0.560730</td>\n",
       "      <td>-1.137430</td>\n",
       "      <td>0.099652</td>\n",
       "      <td>0.013877</td>\n",
       "      <td>0.658632</td>\n",
       "      <td>-0.342217</td>\n",
       "      <td>0.223111</td>\n",
       "      <td>0.872403</td>\n",
       "      <td>-0.399698</td>\n",
       "      <td>-0.127907</td>\n",
       "      <td>...</td>\n",
       "      <td>1.434972</td>\n",
       "      <td>-0.347689</td>\n",
       "      <td>-0.377655</td>\n",
       "      <td>-1.394525</td>\n",
       "      <td>-0.538698</td>\n",
       "      <td>-0.487155</td>\n",
       "      <td>0.553153</td>\n",
       "      <td>0.755053</td>\n",
       "      <td>-0.537767</td>\n",
       "      <td>1.011197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-22</th>\n",
       "      <td>-1.146803</td>\n",
       "      <td>-0.516681</td>\n",
       "      <td>1.151431</td>\n",
       "      <td>1.085070</td>\n",
       "      <td>0.074915</td>\n",
       "      <td>0.081847</td>\n",
       "      <td>-0.150701</td>\n",
       "      <td>-0.278013</td>\n",
       "      <td>-0.555224</td>\n",
       "      <td>0.126867</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046870</td>\n",
       "      <td>-0.246035</td>\n",
       "      <td>-0.386620</td>\n",
       "      <td>-0.054031</td>\n",
       "      <td>-0.476030</td>\n",
       "      <td>-0.091837</td>\n",
       "      <td>-0.421327</td>\n",
       "      <td>-0.946797</td>\n",
       "      <td>0.106149</td>\n",
       "      <td>-0.002851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-25</th>\n",
       "      <td>0.659349</td>\n",
       "      <td>-1.221008</td>\n",
       "      <td>-0.372489</td>\n",
       "      <td>-0.341074</td>\n",
       "      <td>0.109668</td>\n",
       "      <td>-0.292706</td>\n",
       "      <td>-0.084892</td>\n",
       "      <td>1.184710</td>\n",
       "      <td>-0.959285</td>\n",
       "      <td>-0.282029</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.575803</td>\n",
       "      <td>0.240999</td>\n",
       "      <td>0.343241</td>\n",
       "      <td>-0.651297</td>\n",
       "      <td>-1.151639</td>\n",
       "      <td>-0.024341</td>\n",
       "      <td>0.166127</td>\n",
       "      <td>0.118796</td>\n",
       "      <td>-0.427841</td>\n",
       "      <td>0.321648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-26</th>\n",
       "      <td>-0.032708</td>\n",
       "      <td>0.234343</td>\n",
       "      <td>0.131651</td>\n",
       "      <td>0.109342</td>\n",
       "      <td>-0.556130</td>\n",
       "      <td>-0.244717</td>\n",
       "      <td>-0.377797</td>\n",
       "      <td>0.183364</td>\n",
       "      <td>-0.577464</td>\n",
       "      <td>0.317533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150958</td>\n",
       "      <td>-0.070195</td>\n",
       "      <td>0.960796</td>\n",
       "      <td>-1.177043</td>\n",
       "      <td>-0.384593</td>\n",
       "      <td>0.306386</td>\n",
       "      <td>-0.206326</td>\n",
       "      <td>-0.246683</td>\n",
       "      <td>0.237594</td>\n",
       "      <td>-0.878121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-27</th>\n",
       "      <td>-0.363721</td>\n",
       "      <td>1.052056</td>\n",
       "      <td>-0.024166</td>\n",
       "      <td>-0.010591</td>\n",
       "      <td>0.315892</td>\n",
       "      <td>0.224881</td>\n",
       "      <td>2.192442</td>\n",
       "      <td>1.128111</td>\n",
       "      <td>1.411127</td>\n",
       "      <td>-0.309662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034693</td>\n",
       "      <td>0.474269</td>\n",
       "      <td>0.396878</td>\n",
       "      <td>1.991081</td>\n",
       "      <td>0.941800</td>\n",
       "      <td>-0.265792</td>\n",
       "      <td>1.179504</td>\n",
       "      <td>1.004419</td>\n",
       "      <td>-0.793726</td>\n",
       "      <td>2.193537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-28</th>\n",
       "      <td>-0.048375</td>\n",
       "      <td>0.411934</td>\n",
       "      <td>-0.078409</td>\n",
       "      <td>0.019962</td>\n",
       "      <td>0.022729</td>\n",
       "      <td>0.067776</td>\n",
       "      <td>1.190631</td>\n",
       "      <td>-0.583106</td>\n",
       "      <td>1.407557</td>\n",
       "      <td>-0.141870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.577964</td>\n",
       "      <td>0.645940</td>\n",
       "      <td>-0.749067</td>\n",
       "      <td>0.514492</td>\n",
       "      <td>0.585964</td>\n",
       "      <td>0.026648</td>\n",
       "      <td>1.496052</td>\n",
       "      <td>0.367481</td>\n",
       "      <td>-0.251144</td>\n",
       "      <td>0.527595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ADBE       ADP     GOOGL      GOOG      AMZN       AMD  \\\n",
       "Date                                                                     \n",
       "2023-03-29  0.658925  2.189521  0.106285  0.207821  1.503570  0.444433   \n",
       "2023-03-30  0.273051 -0.221306 -0.389329 -0.434766  0.787034  0.526996   \n",
       "2023-03-31  0.360570  1.136308  1.540732  1.439604  0.531735 -0.056439   \n",
       "2023-04-03 -0.713053 -2.259591  0.252735  0.407399 -0.591892 -0.600163   \n",
       "2023-04-04  0.560730 -1.137430  0.099652  0.013877  0.658632 -0.342217   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2024-03-22 -1.146803 -0.516681  1.151431  1.085070  0.074915  0.081847   \n",
       "2024-03-25  0.659349 -1.221008 -0.372489 -0.341074  0.109668 -0.292706   \n",
       "2024-03-26 -0.032708  0.234343  0.131651  0.109342 -0.556130 -0.244717   \n",
       "2024-03-27 -0.363721  1.052056 -0.024166 -0.010591  0.315892  0.224881   \n",
       "2024-03-28 -0.048375  0.411934 -0.078409  0.019962  0.022729  0.067776   \n",
       "\n",
       "                 AEP      AMGN       ADI      ANSS  ...      TTWO      TMUS  \\\n",
       "Date                                                ...                       \n",
       "2023-03-29  1.020467  0.727398  1.860839  0.356414  ...  0.517876  0.615242   \n",
       "2023-03-30  0.277291  0.076716  1.630406  0.936497  ... -0.109884  0.439972   \n",
       "2023-03-31  0.475365  0.008639  0.935425  1.044069  ...  1.337544  0.117646   \n",
       "2023-04-03 -0.086824  0.759731 -0.328378 -0.555499  ... -0.377687  1.191774   \n",
       "2023-04-04  0.223111  0.872403 -0.399698 -0.127907  ...  1.434972 -0.347689   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "2024-03-22 -0.150701 -0.278013 -0.555224  0.126867  ...  0.046870 -0.246035   \n",
       "2024-03-25 -0.084892  1.184710 -0.959285 -0.282029  ... -2.575803  0.240999   \n",
       "2024-03-26 -0.377797  0.183364 -0.577464  0.317533  ...  0.150958 -0.070195   \n",
       "2024-03-27  2.192442  1.128111  1.411127 -0.309662  ...  0.034693  0.474269   \n",
       "2024-03-28  1.190631 -0.583106  1.407557 -0.141870  ...  0.577964  0.645940   \n",
       "\n",
       "                TSLA       TXN      VRSK      VRTX       WBA       WBD  \\\n",
       "Date                                                                     \n",
       "2023-03-29  0.814986  1.344650  1.105776  0.127319  0.497115  0.429736   \n",
       "2023-03-30  0.233493  1.187056  0.239468 -0.525980  0.691243  0.446100   \n",
       "2023-03-31  2.058867  0.640237  0.325258  0.538943 -0.008816  0.565842   \n",
       "2023-04-03 -2.030038 -0.684867 -0.214460  0.183344  1.205798 -0.547975   \n",
       "2023-04-04 -0.377655 -1.394525 -0.538698 -0.487155  0.553153  0.755053   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2024-03-22 -0.386620 -0.054031 -0.476030 -0.091837 -0.421327 -0.946797   \n",
       "2024-03-25  0.343241 -0.651297 -1.151639 -0.024341  0.166127  0.118796   \n",
       "2024-03-26  0.960796 -1.177043 -0.384593  0.306386 -0.206326 -0.246683   \n",
       "2024-03-27  0.396878  1.991081  0.941800 -0.265792  1.179504  1.004419   \n",
       "2024-03-28 -0.749067  0.514492  0.585964  0.026648  1.496052  0.367481   \n",
       "\n",
       "                WDAY       XEL  \n",
       "Date                            \n",
       "2023-03-29  2.277783  1.269711  \n",
       "2023-03-30  0.389915  0.430715  \n",
       "2023-03-31  1.611597  0.597545  \n",
       "2023-04-03 -0.634285  0.121519  \n",
       "2023-04-04 -0.537767  1.011197  \n",
       "...              ...       ...  \n",
       "2024-03-22  0.106149 -0.002851  \n",
       "2024-03-25 -0.427841  0.321648  \n",
       "2024-03-26  0.237594 -0.878121  \n",
       "2024-03-27 -0.793726  2.193537  \n",
       "2024-03-28 -0.251144  0.527595  \n",
       "\n",
       "[252 rows x 89 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the Z Matrix\n",
    "url = 'https://raw.githubusercontent.com/Jandsy/ml_finance_imperial/main/Coursework/Z_matrix.csv'\n",
    "Z_matrix = pd.read_csv(url,index_col=0)\n",
    "Z_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d-2MrrzatMc"
   },
   "source": [
    "---\n",
    "<font color=green>Q15: (3 Marks) </font>\n",
    "<br><font color='green'>\n",
    "For the specified period from March 29, 2023 ('2023-03-29'), to March 28, 2024 ('2024-03-28'), compute the correlation matrix\n",
    "$C$ using the matrix `Z_matrix`.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "2rRt-HL1xZqA"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADP</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AMD</th>\n",
       "      <th>AEP</th>\n",
       "      <th>AMGN</th>\n",
       "      <th>ADI</th>\n",
       "      <th>ANSS</th>\n",
       "      <th>...</th>\n",
       "      <th>TTWO</th>\n",
       "      <th>TMUS</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>TXN</th>\n",
       "      <th>VRSK</th>\n",
       "      <th>VRTX</th>\n",
       "      <th>WBA</th>\n",
       "      <th>WBD</th>\n",
       "      <th>WDAY</th>\n",
       "      <th>XEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADBE</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.218513</td>\n",
       "      <td>0.397890</td>\n",
       "      <td>0.400601</td>\n",
       "      <td>0.463488</td>\n",
       "      <td>0.444032</td>\n",
       "      <td>-0.035967</td>\n",
       "      <td>0.198781</td>\n",
       "      <td>0.321991</td>\n",
       "      <td>0.387483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257931</td>\n",
       "      <td>0.102167</td>\n",
       "      <td>0.268863</td>\n",
       "      <td>0.326597</td>\n",
       "      <td>0.171580</td>\n",
       "      <td>0.164760</td>\n",
       "      <td>0.033955</td>\n",
       "      <td>0.099841</td>\n",
       "      <td>0.418110</td>\n",
       "      <td>0.019105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.218513</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.294213</td>\n",
       "      <td>0.298841</td>\n",
       "      <td>0.168206</td>\n",
       "      <td>0.045884</td>\n",
       "      <td>0.228457</td>\n",
       "      <td>0.214813</td>\n",
       "      <td>0.279607</td>\n",
       "      <td>0.238355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290311</td>\n",
       "      <td>0.113985</td>\n",
       "      <td>0.178128</td>\n",
       "      <td>0.297954</td>\n",
       "      <td>0.325258</td>\n",
       "      <td>0.176771</td>\n",
       "      <td>0.142369</td>\n",
       "      <td>0.243986</td>\n",
       "      <td>0.320836</td>\n",
       "      <td>0.164682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.397890</td>\n",
       "      <td>0.294213</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997415</td>\n",
       "      <td>0.521199</td>\n",
       "      <td>0.371105</td>\n",
       "      <td>-0.006803</td>\n",
       "      <td>0.118938</td>\n",
       "      <td>0.222252</td>\n",
       "      <td>0.292286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238219</td>\n",
       "      <td>0.086673</td>\n",
       "      <td>0.267941</td>\n",
       "      <td>0.192188</td>\n",
       "      <td>0.178622</td>\n",
       "      <td>0.142447</td>\n",
       "      <td>0.052710</td>\n",
       "      <td>0.042072</td>\n",
       "      <td>0.289137</td>\n",
       "      <td>0.025701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOG</th>\n",
       "      <td>0.400601</td>\n",
       "      <td>0.298841</td>\n",
       "      <td>0.997415</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.525626</td>\n",
       "      <td>0.371568</td>\n",
       "      <td>-0.004037</td>\n",
       "      <td>0.118296</td>\n",
       "      <td>0.223710</td>\n",
       "      <td>0.294542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242111</td>\n",
       "      <td>0.091456</td>\n",
       "      <td>0.268114</td>\n",
       "      <td>0.198044</td>\n",
       "      <td>0.180110</td>\n",
       "      <td>0.146190</td>\n",
       "      <td>0.060822</td>\n",
       "      <td>0.045516</td>\n",
       "      <td>0.293575</td>\n",
       "      <td>0.026392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>0.463488</td>\n",
       "      <td>0.168206</td>\n",
       "      <td>0.521199</td>\n",
       "      <td>0.525626</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.463049</td>\n",
       "      <td>-0.010849</td>\n",
       "      <td>0.123745</td>\n",
       "      <td>0.290872</td>\n",
       "      <td>0.342042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222346</td>\n",
       "      <td>0.120301</td>\n",
       "      <td>0.303368</td>\n",
       "      <td>0.299500</td>\n",
       "      <td>0.144325</td>\n",
       "      <td>0.104962</td>\n",
       "      <td>0.017926</td>\n",
       "      <td>0.162937</td>\n",
       "      <td>0.403757</td>\n",
       "      <td>-0.058870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VRTX</th>\n",
       "      <td>0.164760</td>\n",
       "      <td>0.176771</td>\n",
       "      <td>0.142447</td>\n",
       "      <td>0.146190</td>\n",
       "      <td>0.104962</td>\n",
       "      <td>0.039540</td>\n",
       "      <td>0.239861</td>\n",
       "      <td>0.281759</td>\n",
       "      <td>0.110189</td>\n",
       "      <td>0.142121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180810</td>\n",
       "      <td>0.139184</td>\n",
       "      <td>0.144443</td>\n",
       "      <td>0.198258</td>\n",
       "      <td>0.251863</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.159124</td>\n",
       "      <td>0.062726</td>\n",
       "      <td>0.101851</td>\n",
       "      <td>0.184369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WBA</th>\n",
       "      <td>0.033955</td>\n",
       "      <td>0.142369</td>\n",
       "      <td>0.052710</td>\n",
       "      <td>0.060822</td>\n",
       "      <td>0.017926</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.309717</td>\n",
       "      <td>0.214701</td>\n",
       "      <td>0.208907</td>\n",
       "      <td>0.096813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115189</td>\n",
       "      <td>0.063538</td>\n",
       "      <td>0.168495</td>\n",
       "      <td>0.199627</td>\n",
       "      <td>0.038371</td>\n",
       "      <td>0.159124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.361533</td>\n",
       "      <td>0.010855</td>\n",
       "      <td>0.194839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WBD</th>\n",
       "      <td>0.099841</td>\n",
       "      <td>0.243986</td>\n",
       "      <td>0.042072</td>\n",
       "      <td>0.045516</td>\n",
       "      <td>0.162937</td>\n",
       "      <td>0.092733</td>\n",
       "      <td>0.325463</td>\n",
       "      <td>0.220342</td>\n",
       "      <td>0.310681</td>\n",
       "      <td>0.095919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129023</td>\n",
       "      <td>0.084315</td>\n",
       "      <td>0.282265</td>\n",
       "      <td>0.355450</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>0.062726</td>\n",
       "      <td>0.361533</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.160860</td>\n",
       "      <td>0.183837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WDAY</th>\n",
       "      <td>0.418110</td>\n",
       "      <td>0.320836</td>\n",
       "      <td>0.289137</td>\n",
       "      <td>0.293575</td>\n",
       "      <td>0.403757</td>\n",
       "      <td>0.334587</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.068097</td>\n",
       "      <td>0.315426</td>\n",
       "      <td>0.382360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293949</td>\n",
       "      <td>0.142648</td>\n",
       "      <td>0.277744</td>\n",
       "      <td>0.346161</td>\n",
       "      <td>0.195588</td>\n",
       "      <td>0.101851</td>\n",
       "      <td>0.010855</td>\n",
       "      <td>0.160860</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.019310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XEL</th>\n",
       "      <td>0.019105</td>\n",
       "      <td>0.164682</td>\n",
       "      <td>0.025701</td>\n",
       "      <td>0.026392</td>\n",
       "      <td>-0.058870</td>\n",
       "      <td>-0.214673</td>\n",
       "      <td>0.617318</td>\n",
       "      <td>0.283399</td>\n",
       "      <td>0.029036</td>\n",
       "      <td>0.072063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074596</td>\n",
       "      <td>0.249106</td>\n",
       "      <td>0.023168</td>\n",
       "      <td>0.062496</td>\n",
       "      <td>0.151549</td>\n",
       "      <td>0.184369</td>\n",
       "      <td>0.194839</td>\n",
       "      <td>0.183837</td>\n",
       "      <td>-0.019310</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ADBE       ADP     GOOGL      GOOG      AMZN       AMD       AEP  \\\n",
       "ADBE   1.000000  0.218513  0.397890  0.400601  0.463488  0.444032 -0.035967   \n",
       "ADP    0.218513  1.000000  0.294213  0.298841  0.168206  0.045884  0.228457   \n",
       "GOOGL  0.397890  0.294213  1.000000  0.997415  0.521199  0.371105 -0.006803   \n",
       "GOOG   0.400601  0.298841  0.997415  1.000000  0.525626  0.371568 -0.004037   \n",
       "AMZN   0.463488  0.168206  0.521199  0.525626  1.000000  0.463049 -0.010849   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "VRTX   0.164760  0.176771  0.142447  0.146190  0.104962  0.039540  0.239861   \n",
       "WBA    0.033955  0.142369  0.052710  0.060822  0.017926  0.002629  0.309717   \n",
       "WBD    0.099841  0.243986  0.042072  0.045516  0.162937  0.092733  0.325463   \n",
       "WDAY   0.418110  0.320836  0.289137  0.293575  0.403757  0.334587  0.017659   \n",
       "XEL    0.019105  0.164682  0.025701  0.026392 -0.058870 -0.214673  0.617318   \n",
       "\n",
       "           AMGN       ADI      ANSS  ...      TTWO      TMUS      TSLA  \\\n",
       "ADBE   0.198781  0.321991  0.387483  ...  0.257931  0.102167  0.268863   \n",
       "ADP    0.214813  0.279607  0.238355  ...  0.290311  0.113985  0.178128   \n",
       "GOOGL  0.118938  0.222252  0.292286  ...  0.238219  0.086673  0.267941   \n",
       "GOOG   0.118296  0.223710  0.294542  ...  0.242111  0.091456  0.268114   \n",
       "AMZN   0.123745  0.290872  0.342042  ...  0.222346  0.120301  0.303368   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "VRTX   0.281759  0.110189  0.142121  ...  0.180810  0.139184  0.144443   \n",
       "WBA    0.214701  0.208907  0.096813  ...  0.115189  0.063538  0.168495   \n",
       "WBD    0.220342  0.310681  0.095919  ...  0.129023  0.084315  0.282265   \n",
       "WDAY   0.068097  0.315426  0.382360  ...  0.293949  0.142648  0.277744   \n",
       "XEL    0.283399  0.029036  0.072063  ...  0.074596  0.249106  0.023168   \n",
       "\n",
       "            TXN      VRSK      VRTX       WBA       WBD      WDAY       XEL  \n",
       "ADBE   0.326597  0.171580  0.164760  0.033955  0.099841  0.418110  0.019105  \n",
       "ADP    0.297954  0.325258  0.176771  0.142369  0.243986  0.320836  0.164682  \n",
       "GOOGL  0.192188  0.178622  0.142447  0.052710  0.042072  0.289137  0.025701  \n",
       "GOOG   0.198044  0.180110  0.146190  0.060822  0.045516  0.293575  0.026392  \n",
       "AMZN   0.299500  0.144325  0.104962  0.017926  0.162937  0.403757 -0.058870  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "VRTX   0.198258  0.251863  1.000000  0.159124  0.062726  0.101851  0.184369  \n",
       "WBA    0.199627  0.038371  0.159124  1.000000  0.361533  0.010855  0.194839  \n",
       "WBD    0.355450  0.002990  0.062726  0.361533  1.000000  0.160860  0.183837  \n",
       "WDAY   0.346161  0.195588  0.101851  0.010855  0.160860  1.000000 -0.019310  \n",
       "XEL    0.062496  0.151549  0.184369  0.194839  0.183837 -0.019310  1.000000  \n",
       "\n",
       "[89 rows x 89 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the Correlation Matrix manually by transposing the matrix\n",
    "transpose = Z_matrix.shape[0]\n",
    "corr_matrix = (1 / (transpose - 1))* Z_matrix.T.dot(Z_matrix)\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FnUvUEkjAbG"
   },
   "source": [
    "---\n",
    "<font color=green>Q16: (2 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Refind the correlation matrix from the from March 29, 2023 ('2023-03-29'), to March 28, 2024 ('2024-03-28') using pandas correlation matrix method.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "T_g-VjITxasb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADP</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AMD</th>\n",
       "      <th>AEP</th>\n",
       "      <th>AMGN</th>\n",
       "      <th>ADI</th>\n",
       "      <th>ANSS</th>\n",
       "      <th>...</th>\n",
       "      <th>TTWO</th>\n",
       "      <th>TMUS</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>TXN</th>\n",
       "      <th>VRSK</th>\n",
       "      <th>VRTX</th>\n",
       "      <th>WBA</th>\n",
       "      <th>WBD</th>\n",
       "      <th>WDAY</th>\n",
       "      <th>XEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADBE</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.218513</td>\n",
       "      <td>0.397890</td>\n",
       "      <td>0.400601</td>\n",
       "      <td>0.463488</td>\n",
       "      <td>0.444032</td>\n",
       "      <td>-0.035967</td>\n",
       "      <td>0.198781</td>\n",
       "      <td>0.321991</td>\n",
       "      <td>0.387483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257931</td>\n",
       "      <td>0.102167</td>\n",
       "      <td>0.268863</td>\n",
       "      <td>0.326597</td>\n",
       "      <td>0.171580</td>\n",
       "      <td>0.164760</td>\n",
       "      <td>0.033955</td>\n",
       "      <td>0.099841</td>\n",
       "      <td>0.418110</td>\n",
       "      <td>0.019105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.218513</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.294213</td>\n",
       "      <td>0.298841</td>\n",
       "      <td>0.168206</td>\n",
       "      <td>0.045884</td>\n",
       "      <td>0.228457</td>\n",
       "      <td>0.214813</td>\n",
       "      <td>0.279607</td>\n",
       "      <td>0.238355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290311</td>\n",
       "      <td>0.113985</td>\n",
       "      <td>0.178128</td>\n",
       "      <td>0.297954</td>\n",
       "      <td>0.325258</td>\n",
       "      <td>0.176771</td>\n",
       "      <td>0.142369</td>\n",
       "      <td>0.243986</td>\n",
       "      <td>0.320836</td>\n",
       "      <td>0.164682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.397890</td>\n",
       "      <td>0.294213</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997415</td>\n",
       "      <td>0.521199</td>\n",
       "      <td>0.371105</td>\n",
       "      <td>-0.006803</td>\n",
       "      <td>0.118938</td>\n",
       "      <td>0.222252</td>\n",
       "      <td>0.292286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238219</td>\n",
       "      <td>0.086673</td>\n",
       "      <td>0.267941</td>\n",
       "      <td>0.192188</td>\n",
       "      <td>0.178622</td>\n",
       "      <td>0.142447</td>\n",
       "      <td>0.052710</td>\n",
       "      <td>0.042072</td>\n",
       "      <td>0.289137</td>\n",
       "      <td>0.025701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOG</th>\n",
       "      <td>0.400601</td>\n",
       "      <td>0.298841</td>\n",
       "      <td>0.997415</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.525626</td>\n",
       "      <td>0.371568</td>\n",
       "      <td>-0.004037</td>\n",
       "      <td>0.118296</td>\n",
       "      <td>0.223710</td>\n",
       "      <td>0.294542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242111</td>\n",
       "      <td>0.091456</td>\n",
       "      <td>0.268114</td>\n",
       "      <td>0.198044</td>\n",
       "      <td>0.180110</td>\n",
       "      <td>0.146190</td>\n",
       "      <td>0.060822</td>\n",
       "      <td>0.045516</td>\n",
       "      <td>0.293575</td>\n",
       "      <td>0.026392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>0.463488</td>\n",
       "      <td>0.168206</td>\n",
       "      <td>0.521199</td>\n",
       "      <td>0.525626</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.463049</td>\n",
       "      <td>-0.010849</td>\n",
       "      <td>0.123745</td>\n",
       "      <td>0.290872</td>\n",
       "      <td>0.342042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222346</td>\n",
       "      <td>0.120301</td>\n",
       "      <td>0.303368</td>\n",
       "      <td>0.299500</td>\n",
       "      <td>0.144325</td>\n",
       "      <td>0.104962</td>\n",
       "      <td>0.017926</td>\n",
       "      <td>0.162937</td>\n",
       "      <td>0.403757</td>\n",
       "      <td>-0.058870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VRTX</th>\n",
       "      <td>0.164760</td>\n",
       "      <td>0.176771</td>\n",
       "      <td>0.142447</td>\n",
       "      <td>0.146190</td>\n",
       "      <td>0.104962</td>\n",
       "      <td>0.039540</td>\n",
       "      <td>0.239861</td>\n",
       "      <td>0.281759</td>\n",
       "      <td>0.110189</td>\n",
       "      <td>0.142121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180810</td>\n",
       "      <td>0.139184</td>\n",
       "      <td>0.144443</td>\n",
       "      <td>0.198258</td>\n",
       "      <td>0.251863</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.159124</td>\n",
       "      <td>0.062726</td>\n",
       "      <td>0.101851</td>\n",
       "      <td>0.184369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WBA</th>\n",
       "      <td>0.033955</td>\n",
       "      <td>0.142369</td>\n",
       "      <td>0.052710</td>\n",
       "      <td>0.060822</td>\n",
       "      <td>0.017926</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.309717</td>\n",
       "      <td>0.214701</td>\n",
       "      <td>0.208907</td>\n",
       "      <td>0.096813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115189</td>\n",
       "      <td>0.063538</td>\n",
       "      <td>0.168495</td>\n",
       "      <td>0.199627</td>\n",
       "      <td>0.038371</td>\n",
       "      <td>0.159124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.361533</td>\n",
       "      <td>0.010855</td>\n",
       "      <td>0.194839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WBD</th>\n",
       "      <td>0.099841</td>\n",
       "      <td>0.243986</td>\n",
       "      <td>0.042072</td>\n",
       "      <td>0.045516</td>\n",
       "      <td>0.162937</td>\n",
       "      <td>0.092733</td>\n",
       "      <td>0.325463</td>\n",
       "      <td>0.220342</td>\n",
       "      <td>0.310681</td>\n",
       "      <td>0.095919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129023</td>\n",
       "      <td>0.084315</td>\n",
       "      <td>0.282265</td>\n",
       "      <td>0.355450</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>0.062726</td>\n",
       "      <td>0.361533</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.160860</td>\n",
       "      <td>0.183837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WDAY</th>\n",
       "      <td>0.418110</td>\n",
       "      <td>0.320836</td>\n",
       "      <td>0.289137</td>\n",
       "      <td>0.293575</td>\n",
       "      <td>0.403757</td>\n",
       "      <td>0.334587</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.068097</td>\n",
       "      <td>0.315426</td>\n",
       "      <td>0.382360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293949</td>\n",
       "      <td>0.142648</td>\n",
       "      <td>0.277744</td>\n",
       "      <td>0.346161</td>\n",
       "      <td>0.195588</td>\n",
       "      <td>0.101851</td>\n",
       "      <td>0.010855</td>\n",
       "      <td>0.160860</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.019310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XEL</th>\n",
       "      <td>0.019105</td>\n",
       "      <td>0.164682</td>\n",
       "      <td>0.025701</td>\n",
       "      <td>0.026392</td>\n",
       "      <td>-0.058870</td>\n",
       "      <td>-0.214673</td>\n",
       "      <td>0.617318</td>\n",
       "      <td>0.283399</td>\n",
       "      <td>0.029036</td>\n",
       "      <td>0.072063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074596</td>\n",
       "      <td>0.249106</td>\n",
       "      <td>0.023168</td>\n",
       "      <td>0.062496</td>\n",
       "      <td>0.151549</td>\n",
       "      <td>0.184369</td>\n",
       "      <td>0.194839</td>\n",
       "      <td>0.183837</td>\n",
       "      <td>-0.019310</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ADBE       ADP     GOOGL      GOOG      AMZN       AMD       AEP  \\\n",
       "ADBE   1.000000  0.218513  0.397890  0.400601  0.463488  0.444032 -0.035967   \n",
       "ADP    0.218513  1.000000  0.294213  0.298841  0.168206  0.045884  0.228457   \n",
       "GOOGL  0.397890  0.294213  1.000000  0.997415  0.521199  0.371105 -0.006803   \n",
       "GOOG   0.400601  0.298841  0.997415  1.000000  0.525626  0.371568 -0.004037   \n",
       "AMZN   0.463488  0.168206  0.521199  0.525626  1.000000  0.463049 -0.010849   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "VRTX   0.164760  0.176771  0.142447  0.146190  0.104962  0.039540  0.239861   \n",
       "WBA    0.033955  0.142369  0.052710  0.060822  0.017926  0.002629  0.309717   \n",
       "WBD    0.099841  0.243986  0.042072  0.045516  0.162937  0.092733  0.325463   \n",
       "WDAY   0.418110  0.320836  0.289137  0.293575  0.403757  0.334587  0.017659   \n",
       "XEL    0.019105  0.164682  0.025701  0.026392 -0.058870 -0.214673  0.617318   \n",
       "\n",
       "           AMGN       ADI      ANSS  ...      TTWO      TMUS      TSLA  \\\n",
       "ADBE   0.198781  0.321991  0.387483  ...  0.257931  0.102167  0.268863   \n",
       "ADP    0.214813  0.279607  0.238355  ...  0.290311  0.113985  0.178128   \n",
       "GOOGL  0.118938  0.222252  0.292286  ...  0.238219  0.086673  0.267941   \n",
       "GOOG   0.118296  0.223710  0.294542  ...  0.242111  0.091456  0.268114   \n",
       "AMZN   0.123745  0.290872  0.342042  ...  0.222346  0.120301  0.303368   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "VRTX   0.281759  0.110189  0.142121  ...  0.180810  0.139184  0.144443   \n",
       "WBA    0.214701  0.208907  0.096813  ...  0.115189  0.063538  0.168495   \n",
       "WBD    0.220342  0.310681  0.095919  ...  0.129023  0.084315  0.282265   \n",
       "WDAY   0.068097  0.315426  0.382360  ...  0.293949  0.142648  0.277744   \n",
       "XEL    0.283399  0.029036  0.072063  ...  0.074596  0.249106  0.023168   \n",
       "\n",
       "            TXN      VRSK      VRTX       WBA       WBD      WDAY       XEL  \n",
       "ADBE   0.326597  0.171580  0.164760  0.033955  0.099841  0.418110  0.019105  \n",
       "ADP    0.297954  0.325258  0.176771  0.142369  0.243986  0.320836  0.164682  \n",
       "GOOGL  0.192188  0.178622  0.142447  0.052710  0.042072  0.289137  0.025701  \n",
       "GOOG   0.198044  0.180110  0.146190  0.060822  0.045516  0.293575  0.026392  \n",
       "AMZN   0.299500  0.144325  0.104962  0.017926  0.162937  0.403757 -0.058870  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "VRTX   0.198258  0.251863  1.000000  0.159124  0.062726  0.101851  0.184369  \n",
       "WBA    0.199627  0.038371  0.159124  1.000000  0.361533  0.010855  0.194839  \n",
       "WBD    0.355450  0.002990  0.062726  0.361533  1.000000  0.160860  0.183837  \n",
       "WDAY   0.346161  0.195588  0.101851  0.010855  0.160860  1.000000 -0.019310  \n",
       "XEL    0.062496  0.151549  0.184369  0.194839  0.183837 -0.019310  1.000000  \n",
       "\n",
       "[89 rows x 89 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the correlation matrix using pandas\n",
    "pandas_corr = Z_matrix.corr()\n",
    "pandas_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvsiMvgfaxzW"
   },
   "source": [
    "---\n",
    "<font color=green>Q17: (7 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Conduct Singular Value Decomposition on the correlation matrix $C$. Follow these steps:\n",
    "\n",
    "\n",
    "1.   **Perform SVD**: Decompose the matrix $C$ into its singular values and vectors.\n",
    "2.   **Rank Eigenvalues**: Sort the resulting singular values (often squared to compare to eigenvalues) in descending order.\n",
    "3. **Select Components**: Extract the first 20 components based on the largest singular values.\n",
    "4. **Variance Explained**: Print the variance explained by the first 20 Components and dimensions of differents matrix that you created.\n",
    "\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained by the top 20 components:\n",
      " Component  Variance (%)  Cumulative Variance (%)\n",
      "         1     78.617886                78.617886\n",
      "         2      8.648550                87.266436\n",
      "         3      2.462599                89.729035\n",
      "         4      1.223756                90.952791\n",
      "         5      1.065625                92.018417\n",
      "         6      0.673085                92.691502\n",
      "         7      0.554871                93.246372\n",
      "         8      0.498553                93.744925\n",
      "         9      0.486848                94.231773\n",
      "        10      0.417346                94.649118\n",
      "        11      0.369662                95.018781\n",
      "        12      0.336624                95.355404\n",
      "        13      0.315847                95.671251\n",
      "        14      0.281594                95.952845\n",
      "        15      0.251950                96.204795\n",
      "        16      0.228393                96.433188\n",
      "        17      0.220505                96.653692\n",
      "        18      0.211658                96.865351\n",
      "        19      0.191477                97.056828\n",
      "        20      0.185626                97.242454\n",
      "\n",
      "Dimensions of the matrices created:\n",
      "Shape of correlation matrix C: (89, 89)\n",
      "U matrix shape: (89, 89)\n",
      "Sigma (as a diagonal matrix) shape: (89, 89)\n",
      "Vt matrix shape: (89, 89)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Perform Singular Value Decomposition (SVD) on C\n",
    "U, Sigma, Vt = np.linalg.svd(pandas_corr) #U is eigen vectors\n",
    "\n",
    "# Rank eigenvalues\n",
    "eigenvalues = Sigma**2\n",
    "\n",
    "# Select the top 20 components\n",
    "top_20_indices = np.argsort(eigenvalues)[-20:][::-1]\n",
    "top_20_eigenvalues = eigenvalues[top_20_indices]\n",
    "\n",
    "# Variance Explained\n",
    "total_variance = np.sum(eigenvalues)\n",
    "variance_explained = top_20_eigenvalues / total_variance * 100\n",
    "cumulative_variance_explained = np.cumsum(variance_explained)\n",
    "\n",
    "variance_df = pd.DataFrame({\n",
    "    \"Component\": range(1, 21),\n",
    "    \"Variance (%)\": variance_explained,\n",
    "    \"Cumulative Variance (%)\": cumulative_variance_explained\n",
    "})\n",
    "\n",
    "print(\"Variance explained by the top 20 components:\")\n",
    "print(variance_df.to_string(index=False))\n",
    "\n",
    "top_20_U = U[:, top_20_indices]\n",
    "top_20_Sigma = np.diag(Sigma[top_20_indices])\n",
    "top_20_Vt = Vt[top_20_indices, :]\n",
    "\n",
    "# Print dimensions of the matrices\n",
    "print(\"\\nDimensions of the matrices created:\")\n",
    "print(\"Shape of correlation matrix C:\", pandas_corr.shape)\n",
    "print(f\"U matrix shape: {U.shape}\")\n",
    "print(f\"Sigma (as a diagonal matrix) shape: {np.diag(Sigma).shape}\")\n",
    "print(f\"Vt matrix shape: {Vt.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3sZ7K-Tb6S_"
   },
   "source": [
    "---\n",
    "<font color=green>Q18: (6 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Extract the 20 hidden factors in a matrix F. Check that shape of F is $(252,20)$\n",
    "</font>\n",
    "\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD process\n",
    "U, Sigma, V_t = np.linalg.svd(pandas_corr)\n",
    "\n",
    "# Formula to calculate eignenvalue\n",
    "Eigenvalues = Sigma ** 2 \n",
    "\n",
    "\n",
    "sorted_ev = np.sort(Eigenvalues)[::-1] # Sort in descending order\n",
    "\n",
    "# Find out the top 20 eigenvalues\n",
    "top_eigenvalues = sorted_ev[:20] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.42428276,  -1.13807494,  -1.48029262, ...,   1.23955006,\n",
       "          2.27676229,  -0.13751588],\n",
       "       [ -4.05837617,   0.78059648,  -0.86611386, ...,  -0.36262048,\n",
       "         -0.97011478,  -0.91996453],\n",
       "       [ -7.95739081,  -2.27970763,   1.48389609, ...,   0.05732551,\n",
       "         -0.36311986,  -0.11338118],\n",
       "       ...,\n",
       "       [  0.88061846,   0.13735036,   1.12187707, ...,  -0.17879579,\n",
       "          0.09227425,   0.16559638],\n",
       "       [ -5.48903391,  -4.97318897,  -3.13593018, ...,  -1.29272596,\n",
       "         -0.13181172,   0.74442252],\n",
       "       [ -0.52987874,  -1.28782948,  -0.79107298, ...,  -0.58448879,\n",
       "          0.61863981,   0.69301072]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_first_components = U[:, :20]\n",
    "Sigma_first_components = np.diag(Sigma[:20])\n",
    "\n",
    "# Calculate returns, lambda and R\n",
    "std_returns = df_filtered_new.std()\n",
    "Lambda = V_t.T[:, :20]\n",
    "R = df_filtered_new.values\n",
    "\n",
    "# F matrix\n",
    "F = np.zeros((R.shape[0], 20))\n",
    "\n",
    "# Extract the 20 hidden factors in F matrix\n",
    "for j in range(20):\n",
    "    for t in range(R.shape[0]):\n",
    "        F[t, j] = np.sum((Lambda[:, j] / std_returns.values) * R[t, :])\n",
    "        \n",
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix F is (252, 20)\n"
     ]
    }
   ],
   "source": [
    "# Verify the shape of F\n",
    "print(f\"Shape of matrix F is\", F.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz-ncrUmCAVW"
   },
   "source": [
    "---\n",
    "<font color=green>Q19: (3 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Perform the Regression Analysis of 'AAPL' for the date '2024-03-28', using a historical data length of 252 days using previous $F$ Matrix. Compare the R-squared from the ones obtained at Q11.\n",
    "</font>\n",
    "\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "aapl_returns = Z['AAPL'].values[:252]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.615\n",
      "Model:                            OLS   Adj. R-squared:                  0.582\n",
      "Method:                 Least Squares   F-statistic:                     18.48\n",
      "Date:                Fri, 07 Jun 2024   Prob (F-statistic):           2.29e-37\n",
      "Time:                        15:44:53   Log-Likelihood:                -236.67\n",
      "No. Observations:                 252   AIC:                             515.3\n",
      "Df Residuals:                     231   BIC:                             589.5\n",
      "Df Model:                          20                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0721      0.042     -1.705      0.089      -0.155       0.011\n",
      "x1            -0.1250      0.009    -14.408      0.000      -0.142      -0.108\n",
      "x2             0.0148      0.015      0.981      0.327      -0.015       0.044\n",
      "x3             0.0977      0.021      4.741      0.000       0.057       0.138\n",
      "x4            -0.0234      0.025     -0.954      0.341      -0.072       0.025\n",
      "x5            -0.1106      0.025     -4.351      0.000      -0.161      -0.061\n",
      "x6            -0.0465      0.029     -1.632      0.104      -0.103       0.010\n",
      "x7            -0.0856      0.030     -2.862      0.005      -0.145      -0.027\n",
      "x8            -0.1948      0.031     -6.338      0.000      -0.255      -0.134\n",
      "x9             0.0022      0.031      0.070      0.944      -0.059       0.063\n",
      "x10           -0.1380      0.032     -4.295      0.000      -0.201      -0.075\n",
      "x11            0.1254      0.033      3.786      0.000       0.060       0.191\n",
      "x12           -0.0972      0.034     -2.866      0.005      -0.164      -0.030\n",
      "x13           -0.0029      0.034     -0.085      0.933      -0.071       0.065\n",
      "x14           -0.0305      0.035     -0.860      0.390      -0.100       0.039\n",
      "x15            0.1608      0.036      4.411      0.000       0.089       0.233\n",
      "x16            0.0557      0.037      1.490      0.138      -0.018       0.129\n",
      "x17            0.0453      0.038      1.202      0.231      -0.029       0.120\n",
      "x18            0.0348      0.038      0.915      0.361      -0.040       0.110\n",
      "x19           -0.0444      0.039     -1.138      0.256      -0.121       0.032\n",
      "x20           -0.0336      0.039     -0.853      0.395      -0.111       0.044\n",
      "==============================================================================\n",
      "Omnibus:                       14.605   Durbin-Watson:                   1.977\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               25.419\n",
      "Skew:                          -0.323   Prob(JB):                     3.02e-06\n",
      "Kurtosis:                       4.415   Cond. No.                         5.07\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "aapl_returns = Z['AAPL'].values[:252]\n",
    "X = F\n",
    "#print(X)\n",
    "y = aapl_returns\n",
    "#print(aapl_returns)\n",
    "X = sm.add_constant(X)\n",
    "#print(sm.add_constant(X))\n",
    "\n",
    "# Regression\n",
    "model_hidden_factors = sm.OLS(y, X).fit()\n",
    "print(model_hidden_factors.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of R-squared values:\n",
      "R-squared value from Fama-French model: 0.47510269580608755\n",
      "R-squared value from hidden factors model: 0.6154020479750231\n",
      "The model using hidden factors explains more variance in 'AAPL' returns.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the r squared hidden factors and fama french\n",
    "r_squared_hidden_factors = model_hidden_factors.rsquared\n",
    "r_squared_fama_french = result.rsquared\n",
    "print(\"Comparison of R-squared values:\")\n",
    "print(\"R-squared value from Fama-French model:\", r_squared_fama_french)\n",
    "print(\"R-squared value from hidden factors model:\", r_squared_hidden_factors)\n",
    "\n",
    "# Check to see which value is higher and print out the statement which is correct in this context\n",
    "if r_squared_hidden_factors > r_squared_fama_french:\n",
    "    print(\"The model using hidden factors explains more variance in 'AAPL' returns.\")\n",
    "else:\n",
    "    print(\"The Fama-French model explains more variance in 'AAPL' returns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDU8xmpi_ueR"
   },
   "source": [
    "# Ornstein Uhlenbeck\n",
    "\n",
    "The Ornstein-Uhlenbeck process is defined by the following stochastic differential equation (SDE):\n",
    "\n",
    "$$ dX_t = \\theta (\\mu - X_t) dt + \\sigma dW_t $$\n",
    "\n",
    "where:\n",
    "\n",
    "- **$ X_t $**: The value of the process at time $ t $.\n",
    "- **$ \\mu $**: The long-term mean (equilibrium level) to which the process reverts.\n",
    "- **$ \\theta $**: The speed of reversion or the rate at which the process returns to the mean.\n",
    "- **$ \\sigma $**: The volatility (standard deviation), representing the magnitude of random fluctuations.\n",
    "- **$ W_t $**: A Wiener process or Brownian motion that adds stochastic (random) noise.\n",
    "\n",
    "This equation describes a process where the variable $ X_t $ moves towards the mean $ \\mu $ at a rate determined by $ \\theta $, with random noise added by $ \\sigma dW_t $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HMYOiWsP53c"
   },
   "source": [
    "---\n",
    "<font color=green>Q20: (3 Marks) </font>\n",
    "<br><font color='green'>\n",
    "In the context of mean reversion, which quantity should be modeled using an Ornstein-Uhlenbeck process?\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiO01w7fO_bR"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "The Ornstein-Uhlenbeck (OU) process is typically used to model quantities that exhibit mean-reverting behavior. In finance, the following quantities are often modeled using an OU process: Interest Rates, Asset Returns, Volatility and Equity Prices in Spread Trading.\n",
    "\n",
    "A particularly insightful application of the OU process is in the modeling of residuals from asset pricing models. In the specific context of this problem, the residuals from asset pricing models are the quantities of interest. Residuals represent the deviations of observed market prices from the values predicted by models. These deviations, when modeled as an OU process, help in identifying when the predictions diverge from actual market behaviors, indicating a potential reversion to the mean. This approach not only enhances the accuracy of financial models but also identifies potential market inefficiencies.\n",
    "\n",
    "According to Avellaneda and Lee (2010), the mean-reverting nature of residuals can be captured effectively using the OU process. This method underscores the dynamic nature of financial markets, where deviations from expected behaviors are temporary and likely to revert to a state of equilibrium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t31Q2iWgQgsO"
   },
   "source": [
    "---\n",
    "<font color=green>Q21: (5 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Explain how the parameters $ \\theta $ and $ \\sigma $ can be determined using the following equations. Also, detail the underlying assumptions:\n",
    "$$ E[X] = \\mu $$\n",
    "$$ \\text{Var}[X] = \\frac{\\sigma^2}{2\\theta} $$\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_KjMbQplj4U"
   },
   "source": [
    "**Write your answers here:**\n",
    "\n",
    "\n",
    "- **Estimating \\(\\theta\\) (Speed of Reversion)**:\n",
    "  $$ \\theta = \\frac{\\sigma^2}{2 \\times \\text{Var}[X]} $$\n",
    "  This formula rearranges to solve for \\(\\theta\\) when \\(\\sigma^2\\) and \\(\\text{Var}[X]\\) are known or estimated from the data.\n",
    "\n",
    "- **Estimating \\(\\sigma\\) (Volatility)**:\n",
    "  $$ \\sigma = \\sqrt{2\\theta \\times \\text{Var}[X]} $$\n",
    "  Once \\(\\theta\\) is known, \\(\\sigma\\) can be calculated if \\(\\text{Var}[X]\\) is measured or estimated from the data.\n",
    "\n",
    "### Underlying Assumptions\n",
    "\n",
    "To accurately apply these formulas for \\(\\theta\\) and \\(\\sigma\\), several assumptions are made:\n",
    "1. **Stationarity**: The process \\(X_t\\) is assumed to reach a stationary distribution.\n",
    "2. **Normality of Increments**: The increments driven by the Wiener process (\\(dW_t\\)) are normally distributed.\n",
    "3. **Continuous Observation**: The process is ideally observed continuously, though high-frequency discrete data can approximate this.\n",
    "4. **Independence of Increments**: Increments of the Wiener process are independent.\n",
    "5. **Mean Reversion**: The process exhibits mean-reverting behavior, a defining characteristic of the OU process.\n",
    "\n",
    "These assumptions are critical for the validity of the parameter estimation process, which can be implemented through statistical methods such as maximum likelihood estimation or the method of moments based on the observed data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Parameters of the Ornstein-Uhlenbeck Process\n",
    "\n",
    "### Linear Regression Approach\n",
    "\n",
    "To estimate the parameters of the Ornstein-Uhlenbeck process, we consider a linear regression model where the rate of change of the process, \\(dX_t\\), can be expressed in terms of \\(X_t\\). Specifically, the differential form \\(dX_t = \\theta(\\mu - X_t)dt + \\sigma dW_t\\) is transformed into a regression model:\n",
    "\n",
    "$$\n",
    "\\frac{dX_t}{dt} = a + bX_t + \\epsilon\n",
    "$$\n",
    "\n",
    "Here, the coefficients are related to the parameters of the Ornstein-Uhlenbeck process by:\n",
    "- $$a = \\theta\\mu$$\n",
    "- $$b = -\\theta$$\n",
    "\n",
    "From these relations, the parameters \\(\\theta\\) and \\(\\mu\\) can be extracted as. The standard deviation of the noise, \\(\\sigma\\), is estimated from the standard deviation of the residuals, \\(\\epsilon\\), in the regression.\n",
    "\n",
    "### Variance Estimation\n",
    "\n",
    "The Ornstein-Uhlenbeck process is solved explicitly by defining a transformed variable \\(Y_t = X_t e^{\\theta t}\\), which simplifies using Ito's Lemma.\n",
    "Integrating, the solution for \\(X_t\\) is expressed as:\n",
    "\n",
    "$$\n",
    "X_t = X_0 e^{-\\theta t} + \\mu(1 - e^{-\\theta t}) + \\sigma \\int_0^t e^{\\theta(s-t)} dW_s\n",
    "$$\n",
    "\n",
    "This solution leads to the expected value and variance of \\(X_t\\):\n",
    "- Expected value \\(E[X_t]\\) stabilizes at \\(\\mu\\) as \\(t \\to \\infty\\).\n",
    "- The variance $$\\text{Var}(X_t)$$ is derived through Ito isometry:\n",
    "\n",
    "$$\n",
    "\\text{Var}(X_t) = \\sigma^2 \\int_0^t e^{-2\\theta(t-s)} ds = \\frac{\\sigma^2}{2\\theta}(1 - e^{-2\\theta t})\n",
    "$$\n",
    "\n",
    "This result aligns with the long-term behavior of the process, confirming that \\(\\text{Var}(X_t)\\) approaches \\(\\frac{\\sigma^2}{2\\theta}\\) as \\(t\\) approaches infinity.\n",
    "\n",
    "\n",
    "The derivation and analysis presented above demonstrate a comprehensive approach to estimating the mean-reversion speed theta and the volatility sigma in the Ornstein-Uhlenbeck process. This method combines regression techniques with stochastic calculus, providing a robust framework for parameter estimation in mean-reverting stochastic processes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JvTPsTTSMp4"
   },
   "source": [
    "---\n",
    "<font color=green>Q22: (2 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Create a function named `extract_s_scores` which computes 's scores' for the last element in a list of floating-point numbers. This function calculates the scores using the following formula $ \\text{s scores} = \\frac{X_T - \\mu}{\\sigma} $ where `list_xi` represents a list containing a sequence of floating-point numbers $(X_0, \\cdots, X_T)$.\n",
    "\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "unNHgTj8xkv5"
   },
   "outputs": [],
   "source": [
    "def extract_s_scores(list_xi):\n",
    "    \"\"\"\n",
    "    Computes the 's scores' for the last element in a list of floating-point numbers.\n",
    "\n",
    "    Args:\n",
    "    list_xi (list of float): A list containing a sequence of floating-point numbers (X0, ..., XT).\n",
    "\n",
    "    Returns:\n",
    "    float: The 's score' for the last element in the list.\n",
    "    \"\"\"\n",
    "    if not list_xi:\n",
    "        raise ValueError(\"The list cannot be empty.\")\n",
    "\n",
    "    # Calculate the mean and standard deviation of the list\n",
    "    mu = sum(list_xi) / len(list_xi)\n",
    "    sigma = (sum((x - mu) ** 2 for x in list_xi) / len(list_xi)) ** 0.5\n",
    "\n",
    "    # Last element in the list\n",
    "    x_t = list_xi[-1]\n",
    "\n",
    "    # Calculate s score using the formula\n",
    "    s_score = (x_t - mu) / sigma\n",
    "\n",
    "    return s_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAB_ANF2gCiY"
   },
   "source": [
    "# Autoencoder Analysis\n",
    "\n",
    "Autoencoders are neural networks used for unsupervised learning, particularly for dimensionality reduction and feature extraction. Training an autoencoder on the $Z_i$ matrix aims to identify hidden factors capturing the intrinsic structures in financial data.\n",
    "\n",
    "### Architecture\n",
    "- **Encoder**: Compresses input data into a smaller latent space representation.\n",
    "  - *Input Layer*: Matches the number of features in the $Z_i$ matrix.\n",
    "  - *Hidden Layers*: Compress data through progressively smaller layers.\n",
    "  - *Latent Space*: Encodes the data into hidden factors.\n",
    "- **Decoder**: Reconstructs input data from the latent space.\n",
    "  - *Hidden Layers*: Gradually expand to the original dimension.\n",
    "  - *Output Layer*: Matches the input layer to recreate the original matrix.\n",
    "\n",
    "### Training\n",
    "The autoencoder is trained by minimizing reconstruction loss, usually mean squared error (MSE), between the input $Z_i$ matrix and the decoder's output.\n",
    "\n",
    "### Hidden Factors Extraction\n",
    "After training, the encoder's latent space provides the most important underlying patterns in the stock returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJzNtknXgdqF"
   },
   "source": [
    "---\n",
    "<font color=green>Q23: (2 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Modify the standardized returns matrix `Z_matrix` to reduce the influence of extreme outliers on model trainingby ensuring that all values in the matrix `Z_matrix` do not exceed 3 standard deviations from the mean. Specifically, cap these values at the interval $-3, 3]$. Store the adjusted values in a new matrix, `Z_hat`.\n",
    "</font>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "Ln7vWV0TxmFk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cap_values(Z_matrix):\n",
    "    \"\"\"\n",
    "    Caps all values in the matrix to be within the interval [-3, 3].\n",
    "\n",
    "    Args:\n",
    "    Z_matrix (np.array): The input standardized returns matrix Z.\n",
    "\n",
    "    Returns:\n",
    "    np.array: The adjusted matrix Z_hat with values capped within [-3, 3].\n",
    "    \"\"\"\n",
    "    # Define the limits\n",
    "    lower_cap = -3\n",
    "    upper_cap = 3\n",
    "\n",
    "    # Apply the cap to each element in the Z matrix\n",
    "    Z_hat = np.clip(Z_matrix, lower_cap, upper_cap)\n",
    "\n",
    "    return Z_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNJDLYZ9g_V4"
   },
   "source": [
    "---\n",
    "<font color=green>Q24: (1 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Fetch the `Z_hat` data from GitHub, and we'll proceed with it now.\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "BraH_nivxngd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADP</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AMD</th>\n",
       "      <th>AEP</th>\n",
       "      <th>AMGN</th>\n",
       "      <th>ADI</th>\n",
       "      <th>ANSS</th>\n",
       "      <th>...</th>\n",
       "      <th>TTWO</th>\n",
       "      <th>TMUS</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>TXN</th>\n",
       "      <th>VRSK</th>\n",
       "      <th>VRTX</th>\n",
       "      <th>WBA</th>\n",
       "      <th>WBD</th>\n",
       "      <th>WDAY</th>\n",
       "      <th>XEL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-03-29</th>\n",
       "      <td>0.658925</td>\n",
       "      <td>2.189521</td>\n",
       "      <td>0.106285</td>\n",
       "      <td>0.207821</td>\n",
       "      <td>1.503570</td>\n",
       "      <td>0.444433</td>\n",
       "      <td>1.020467</td>\n",
       "      <td>0.727398</td>\n",
       "      <td>1.860839</td>\n",
       "      <td>0.356414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517876</td>\n",
       "      <td>0.615242</td>\n",
       "      <td>0.814986</td>\n",
       "      <td>1.344650</td>\n",
       "      <td>1.105776</td>\n",
       "      <td>0.127319</td>\n",
       "      <td>0.497115</td>\n",
       "      <td>0.429736</td>\n",
       "      <td>2.277783</td>\n",
       "      <td>1.269711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-30</th>\n",
       "      <td>0.273051</td>\n",
       "      <td>-0.221306</td>\n",
       "      <td>-0.389329</td>\n",
       "      <td>-0.434766</td>\n",
       "      <td>0.787034</td>\n",
       "      <td>0.526996</td>\n",
       "      <td>0.277291</td>\n",
       "      <td>0.076716</td>\n",
       "      <td>1.630406</td>\n",
       "      <td>0.936497</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109884</td>\n",
       "      <td>0.439972</td>\n",
       "      <td>0.233493</td>\n",
       "      <td>1.187056</td>\n",
       "      <td>0.239468</td>\n",
       "      <td>-0.525980</td>\n",
       "      <td>0.691243</td>\n",
       "      <td>0.446100</td>\n",
       "      <td>0.389915</td>\n",
       "      <td>0.430715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-31</th>\n",
       "      <td>0.360570</td>\n",
       "      <td>1.136308</td>\n",
       "      <td>1.540732</td>\n",
       "      <td>1.439604</td>\n",
       "      <td>0.531735</td>\n",
       "      <td>-0.056439</td>\n",
       "      <td>0.475365</td>\n",
       "      <td>0.008639</td>\n",
       "      <td>0.935425</td>\n",
       "      <td>1.044069</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337544</td>\n",
       "      <td>0.117646</td>\n",
       "      <td>2.058867</td>\n",
       "      <td>0.640237</td>\n",
       "      <td>0.325258</td>\n",
       "      <td>0.538943</td>\n",
       "      <td>-0.008816</td>\n",
       "      <td>0.565842</td>\n",
       "      <td>1.611597</td>\n",
       "      <td>0.597545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-03</th>\n",
       "      <td>-0.713053</td>\n",
       "      <td>-2.259591</td>\n",
       "      <td>0.252735</td>\n",
       "      <td>0.407399</td>\n",
       "      <td>-0.591892</td>\n",
       "      <td>-0.600163</td>\n",
       "      <td>-0.086824</td>\n",
       "      <td>0.759731</td>\n",
       "      <td>-0.328378</td>\n",
       "      <td>-0.555499</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.377687</td>\n",
       "      <td>1.191774</td>\n",
       "      <td>-2.030038</td>\n",
       "      <td>-0.684867</td>\n",
       "      <td>-0.214460</td>\n",
       "      <td>0.183344</td>\n",
       "      <td>1.205798</td>\n",
       "      <td>-0.547975</td>\n",
       "      <td>-0.634285</td>\n",
       "      <td>0.121519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-04</th>\n",
       "      <td>0.560730</td>\n",
       "      <td>-1.137430</td>\n",
       "      <td>0.099652</td>\n",
       "      <td>0.013877</td>\n",
       "      <td>0.658632</td>\n",
       "      <td>-0.342217</td>\n",
       "      <td>0.223111</td>\n",
       "      <td>0.872403</td>\n",
       "      <td>-0.399698</td>\n",
       "      <td>-0.127907</td>\n",
       "      <td>...</td>\n",
       "      <td>1.434972</td>\n",
       "      <td>-0.347689</td>\n",
       "      <td>-0.377655</td>\n",
       "      <td>-1.394525</td>\n",
       "      <td>-0.538698</td>\n",
       "      <td>-0.487155</td>\n",
       "      <td>0.553153</td>\n",
       "      <td>0.755053</td>\n",
       "      <td>-0.537767</td>\n",
       "      <td>1.011197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ADBE       ADP     GOOGL      GOOG      AMZN       AMD  \\\n",
       "Date                                                                     \n",
       "2023-03-29  0.658925  2.189521  0.106285  0.207821  1.503570  0.444433   \n",
       "2023-03-30  0.273051 -0.221306 -0.389329 -0.434766  0.787034  0.526996   \n",
       "2023-03-31  0.360570  1.136308  1.540732  1.439604  0.531735 -0.056439   \n",
       "2023-04-03 -0.713053 -2.259591  0.252735  0.407399 -0.591892 -0.600163   \n",
       "2023-04-04  0.560730 -1.137430  0.099652  0.013877  0.658632 -0.342217   \n",
       "\n",
       "                 AEP      AMGN       ADI      ANSS  ...      TTWO      TMUS  \\\n",
       "Date                                                ...                       \n",
       "2023-03-29  1.020467  0.727398  1.860839  0.356414  ...  0.517876  0.615242   \n",
       "2023-03-30  0.277291  0.076716  1.630406  0.936497  ... -0.109884  0.439972   \n",
       "2023-03-31  0.475365  0.008639  0.935425  1.044069  ...  1.337544  0.117646   \n",
       "2023-04-03 -0.086824  0.759731 -0.328378 -0.555499  ... -0.377687  1.191774   \n",
       "2023-04-04  0.223111  0.872403 -0.399698 -0.127907  ...  1.434972 -0.347689   \n",
       "\n",
       "                TSLA       TXN      VRSK      VRTX       WBA       WBD  \\\n",
       "Date                                                                     \n",
       "2023-03-29  0.814986  1.344650  1.105776  0.127319  0.497115  0.429736   \n",
       "2023-03-30  0.233493  1.187056  0.239468 -0.525980  0.691243  0.446100   \n",
       "2023-03-31  2.058867  0.640237  0.325258  0.538943 -0.008816  0.565842   \n",
       "2023-04-03 -2.030038 -0.684867 -0.214460  0.183344  1.205798 -0.547975   \n",
       "2023-04-04 -0.377655 -1.394525 -0.538698 -0.487155  0.553153  0.755053   \n",
       "\n",
       "                WDAY       XEL  \n",
       "Date                            \n",
       "2023-03-29  2.277783  1.269711  \n",
       "2023-03-30  0.389915  0.430715  \n",
       "2023-03-31  1.611597  0.597545  \n",
       "2023-04-03 -0.634285  0.121519  \n",
       "2023-04-04 -0.537767  1.011197  \n",
       "\n",
       "[5 rows x 89 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "url = \"https://raw.githubusercontent.com/Jandsy/ml_finance_imperial/main/Coursework/Z_hat.csv\"\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "z_hat = pd.read_csv(url, index_col=0)\n",
    "z_hat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3MRBtXIiWcN"
   },
   "source": [
    "---\n",
    "<font color=green>Q25: (3 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Segment the standardized and capped returns matrix $\\hat{Z}$ into two subsets for model training and testing. Precisly Allocate 70% of the data in $\\hat{Z}$ to the training set $ \\hat{Z}_{train} $ and Allocate the remaining 30% to the testing set $\\hat{Z}_{test}$. Treat each stock within $\\hat{Z}$ as an individual sample, by flattening temporal dependencies.\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "Ggdgu_3wxobS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 62 stocks\n",
      "Testing set size: 27 stocks\n",
      "Training Set:\n",
      "Date   2023-03-29  2023-03-30  2023-03-31  2023-04-03  2023-04-04  2023-04-05  \\\n",
      "ADBE     0.658925    0.273051    0.360570   -0.713053    0.560730   -0.449491   \n",
      "ADP      2.189521   -0.221306    1.136308   -2.259591   -1.137430   -0.406031   \n",
      "GOOGL    0.106285   -0.389329    1.540732    0.252735    0.099652   -0.240895   \n",
      "GOOG     0.207821   -0.434766    1.439604    0.407399    0.013877   -0.197159   \n",
      "AMZN     1.503570    0.787034    0.531735   -0.591892    0.658632   -1.595048   \n",
      "\n",
      "Date   2023-04-06  2023-04-10  2023-04-11  2023-04-12  ...  2024-03-15  \\\n",
      "ADBE    -0.241966   -0.604917   -0.709950   -0.221744  ...   -3.000000   \n",
      "ADP      1.093156   -0.799022    0.323227    0.091276  ...   -0.465146   \n",
      "GOOGL    2.104351   -1.167096   -0.699080   -0.494813  ...   -0.884422   \n",
      "GOOG     2.091914   -1.147042   -0.555401   -0.597410  ...   -0.979539   \n",
      "AMZN     0.364438   -0.082414   -1.308663   -1.249982  ...   -1.425537   \n",
      "\n",
      "Date   2024-03-18  2024-03-19  2024-03-20  2024-03-21  2024-03-22  2024-03-25  \\\n",
      "ADBE     1.978030    0.604276   -0.252143   -0.782287   -1.146803    0.659349   \n",
      "ADP     -0.148001    0.726723    1.169846    0.352099   -0.516681   -1.221008   \n",
      "GOOGL    2.584556   -0.358402    0.576944   -0.548777    1.151431   -0.372489   \n",
      "GOOG     2.485326   -0.322783    0.590986   -0.469061    1.085070   -0.341074   \n",
      "AMZN    -0.121368    0.292398    0.539395   -0.139629    0.074915    0.109668   \n",
      "\n",
      "Date   2024-03-26  2024-03-27  2024-03-28  \n",
      "ADBE    -0.032708   -0.363721   -0.048375  \n",
      "ADP      0.234343    1.052056    0.411934  \n",
      "GOOGL    0.131651   -0.024166   -0.078409  \n",
      "GOOG     0.109342   -0.010591    0.019962  \n",
      "AMZN    -0.556130    0.315892    0.022729  \n",
      "\n",
      "[5 rows x 252 columns]\n",
      "\n",
      "Testing Set:\n",
      "Date  2023-03-29  2023-03-30  2023-03-31  2023-04-03  2023-04-04  2023-04-05  \\\n",
      "NXPI    1.624397    0.662701    1.283113   -1.321406   -1.734429   -0.987218   \n",
      "ORLY    0.213439    0.823726    0.623793    1.729910   -0.439353   -0.686052   \n",
      "ODFL   -0.170982    0.282329    1.062075   -1.083890   -0.926025   -0.664120   \n",
      "ON      1.514933    0.847784    0.199890   -0.509833   -1.149815   -0.876864   \n",
      "PCAR    0.360310   -0.336688    0.870319   -0.356025   -2.553395   -2.715426   \n",
      "\n",
      "Date  2023-04-06  2023-04-10  2023-04-11  2023-04-12  ...  2024-03-15  \\\n",
      "NXPI   -0.950125    0.608630   -0.243721   -0.580673  ...   -1.386180   \n",
      "ORLY    0.159331    0.715963    0.015008    0.488594  ...   -0.380708   \n",
      "ODFL    0.059627    1.376007    0.741676   -0.058091  ...   -0.736760   \n",
      "ON     -0.556764    1.084733   -0.224972   -0.352948  ...   -1.050097   \n",
      "PCAR   -0.176131    1.153045    1.318380    0.974627  ...    1.901872   \n",
      "\n",
      "Date  2024-03-18  2024-03-19  2024-03-20  2024-03-21  2024-03-22  2024-03-25  \\\n",
      "NXPI   -0.284613   -0.138300    1.197393    0.940280   -0.456547   -0.922165   \n",
      "ORLY   -0.058921    1.428206    0.665413    1.159932    0.217545   -2.251583   \n",
      "ODFL   -0.362514    0.595385    1.185067    1.200104   -0.508751   -0.783420   \n",
      "ON     -0.499009    0.094849    0.712443    0.087817   -0.516168   -0.542341   \n",
      "PCAR    1.123082    0.235729    0.099522    1.699242   -0.602836   -0.405590   \n",
      "\n",
      "Date  2024-03-26  2024-03-27  2024-03-28  \n",
      "NXPI   -0.358961    1.279462    0.399456  \n",
      "ORLY   -0.272465   -0.053309   -0.587612  \n",
      "ODFL   -0.273948   -0.715514    1.232225  \n",
      "ON     -0.317910    1.293988   -0.960720  \n",
      "PCAR   -0.431217    0.722008   -0.540717  \n",
      "\n",
      "[5 rows x 252 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ssl\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "# URL to fetch the Z_hat data\n",
    "url = \"https://raw.githubusercontent.com/Jandsy/ml_finance_imperial/main/Coursework/Z_hat.csv\"\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Read the CSV data into a DataFrame\n",
    "z_hat = pd.read_csv(url, index_col=0)\n",
    "\n",
    "# Check if NVDA stock is in the DataFrame\n",
    "assert 'NVDA' in z_hat.columns, \"NVDA stock not found in the dataset.\"\n",
    "\n",
    "# Transpose the DataFrame to treat each stock as an individual sample\n",
    "z_hat_T = z_hat.T\n",
    "\n",
    "# Extract NVDA stock and drop it from the main dataset\n",
    "nvda_stock = z_hat_T.loc[['NVDA']]\n",
    "remaining_stocks = z_hat_T.drop('NVDA')\n",
    "\n",
    "# Determine the number of stocks for training (62) and testing (27)\n",
    "num_stocks = remaining_stocks.shape[0]\n",
    "train_size = math.floor(z_hat_T.shape[0]*0.7)  # Training set should have 70% stocks\n",
    "test_size = z_hat_T.shape[0]-train_size  # Testing set should have 30% stocks, excluding NVDA\n",
    "\n",
    "# Split the remaining stocks into training and testing sets\n",
    "train_stocks = remaining_stocks.iloc[:train_size]\n",
    "test_stocks = remaining_stocks.iloc[train_size:(train_size + test_size - 1)] # -1 to accommodate NVDA\n",
    "\n",
    "# Append NVDA to the test set\n",
    "test_stocks = pd.concat([test_stocks, nvda_stock])\n",
    "\n",
    "# Check sizes\n",
    "print(f\"Training set size: {train_stocks.shape[0]} stocks\")\n",
    "print(f\"Testing set size: {test_stocks.shape[0]} stocks\")\n",
    "\n",
    "# Ensure the sizes are correct\n",
    "assert train_stocks.shape[0] == train_size, \"Training set size is incorrect.\"\n",
    "assert test_stocks.shape[0] == test_size, \"Testing set size is incorrect.\"\n",
    "\n",
    "# Transpose back to original shape\n",
    "z_hat_train = train_stocks\n",
    "z_hat_test = test_stocks\n",
    "\n",
    "# Print the results of the training and testing set\n",
    "print(\"Training Set:\")\n",
    "print(z_hat_train.head())\n",
    "\n",
    "print(\"\\nTesting Set:\")\n",
    "print(z_hat_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqWeD_efihbH"
   },
   "source": [
    "---\n",
    "<font color=green>Q26: (10 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Please create an autoencoder following the instructions provided in  **[End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture](https://arxiv.org/pdf/2402.08233.pdf)**, Use the model 'Variant 2' in Table 1.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_hat_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "hyf_dfsoxp_X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aads/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.8964 - val_loss: 0.8540\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.8709 - val_loss: 0.8386\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.8543 - val_loss: 0.8244\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.8305 - val_loss: 0.8106\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.8037 - val_loss: 0.7971\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.7870 - val_loss: 0.7835\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.7768 - val_loss: 0.7699\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.7572 - val_loss: 0.7566\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.7375 - val_loss: 0.7435\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.7267 - val_loss: 0.7311\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "Training Residuals:\n",
      "Date   2023-03-29  2023-03-30  2023-03-31  2023-04-03  2023-04-04  2023-04-05  \\\n",
      "ADBE    -0.101145    0.178158    0.014062   -0.768372    0.836269   -0.371291   \n",
      "ADP      1.690736   -0.266394    0.941955   -2.287583   -1.118577   -0.112315   \n",
      "GOOGL   -0.552303   -0.360567    0.940936    0.152554    0.192403   -0.311445   \n",
      "GOOG    -0.451874   -0.411730    0.848319    0.285157    0.101819   -0.257882   \n",
      "AMZN     0.801424    0.681323    0.281602   -0.775716    0.749627   -1.771549   \n",
      "\n",
      "Date   2023-04-06  2023-04-10  2023-04-11  2023-04-12  ...  2024-03-15  \\\n",
      "ADBE    -0.310626   -0.658088   -0.588619    0.136315  ...   -2.574691   \n",
      "ADP      1.164963   -0.931117    0.620689    0.268407  ...   -0.038243   \n",
      "GOOGL    2.128697   -1.074736   -0.442114   -0.163033  ...   -0.520787   \n",
      "GOOG     2.110831   -1.060545   -0.310827   -0.254025  ...   -0.632764   \n",
      "AMZN     0.367020    0.008950   -1.366910   -0.944129  ...   -1.036607   \n",
      "\n",
      "Date   2024-03-18  2024-03-19  2024-03-20  2024-03-21  2024-03-22  2024-03-25  \\\n",
      "ADBE     1.571024    0.276620   -0.494232   -0.713759   -0.728326    0.673119   \n",
      "ADP     -0.130962    0.609036    1.149365    0.263863   -0.427945   -1.196977   \n",
      "GOOGL    2.204500   -0.376279    0.091237   -0.307336    1.666370   -0.324008   \n",
      "GOOG     2.089740   -0.346211    0.113780   -0.227312    1.603672   -0.295485   \n",
      "AMZN    -0.539448   -0.007062    0.249108    0.018317    0.467038    0.310468   \n",
      "\n",
      "Date   2024-03-26  2024-03-27  2024-03-28  \n",
      "ADBE     0.171648   -0.760405   -0.121854  \n",
      "ADP      0.316542    1.018392    0.310383  \n",
      "GOOGL    0.354053   -0.033139    0.019149  \n",
      "GOOG     0.326057   -0.008921    0.131142  \n",
      "AMZN    -0.485287   -0.106333    0.107997  \n",
      "\n",
      "[5 rows x 252 columns]\n",
      "\n",
      "Testing Residuals:\n",
      "Date  2023-03-29  2023-03-30  2023-03-31  2023-04-03  2023-04-04  2023-04-05  \\\n",
      "NXPI    0.861980    0.686795    0.838304   -1.295216   -1.419107   -0.816075   \n",
      "ORLY   -0.128597    0.633712    0.692372    1.590795   -0.514339   -0.808389   \n",
      "ODFL   -0.516941    0.435019    0.772119   -0.895468   -0.719795   -0.539961   \n",
      "ON      0.780629    0.930350   -0.206304   -0.490499   -0.815694   -0.745682   \n",
      "PCAR   -0.203684   -0.337121    0.352499   -0.280044   -2.410717   -2.527488   \n",
      "\n",
      "Date  2023-04-06  2023-04-10  2023-04-11  2023-04-12  ...  2024-03-15  \\\n",
      "NXPI   -0.917033    0.397667    0.087287   -0.131219  ...   -0.928899   \n",
      "ORLY    0.327521    0.632602   -0.113789    0.060309  ...    0.170031   \n",
      "ODFL    0.007942    1.274135    0.884673    0.347872  ...   -0.249526   \n",
      "ON     -0.508617    0.872036    0.082015    0.065354  ...   -0.592334   \n",
      "PCAR    0.200539    0.901972    1.702524    1.481855  ...    2.349458   \n",
      "\n",
      "Date  2024-03-18  2024-03-19  2024-03-20  2024-03-21  2024-03-22  2024-03-25  \\\n",
      "NXPI   -0.579029   -0.439228    1.006077    0.905437   -0.070003   -1.001307   \n",
      "ORLY   -0.413353    1.666488    0.717394    1.014252    0.714109   -2.108182   \n",
      "ODFL   -0.430930    0.509811    1.097577    1.272386   -0.185508   -0.937296   \n",
      "ON     -0.819484   -0.185589    0.528557    0.087038   -0.145569   -0.630893   \n",
      "PCAR    0.997944    0.113551    0.169052    1.548375   -0.367561   -0.576875   \n",
      "\n",
      "Date  2024-03-26  2024-03-27  2024-03-28  \n",
      "NXPI   -0.225214    0.987489    0.391151  \n",
      "ORLY   -0.010040   -0.317451   -0.790857  \n",
      "ODFL   -0.126190   -0.759657    1.062416  \n",
      "ON     -0.215169    0.976948   -0.971417  \n",
      "PCAR   -0.458352    0.597200   -0.494780  \n",
      "\n",
      "[5 rows x 252 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "def create_autoencoder(input_dim):\n",
    "    autoencoder = Sequential()\n",
    "    # Encoder\n",
    "    autoencoder.add(Dense(20, activation='tanh', input_shape=(input_dim,), use_bias=True))\n",
    "    # Decoder\n",
    "    autoencoder.add(Dense(input_dim, activation='tanh', use_bias=True))\n",
    "    return autoencoder\n",
    "\n",
    "# Prepare the data\n",
    "X_train = z_hat_train.values\n",
    "X_test = z_hat_test.values\n",
    "\n",
    "# Create and compile the autoencoder model\n",
    "input_dim = X_train.shape[1]\n",
    "autoencoder = create_autoencoder(input_dim)\n",
    "autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X_train, X_train, epochs=10, batch_size=32, validation_data=(X_test, X_test))\n",
    "\n",
    "# Predict the reconstructed values\n",
    "X_train_pred = autoencoder.predict(X_train)\n",
    "X_test_pred = autoencoder.predict(X_test)\n",
    "\n",
    "# Calculate residuals for the training and testing sets\n",
    "residuals_train = X_train - X_train_pred\n",
    "residuals_test = X_test - X_test_pred\n",
    "\n",
    "# Convert residuals back to DataFrame\n",
    "residuals_train_df = pd.DataFrame(residuals_train, index=train_stocks.index, columns=train_stocks.columns)\n",
    "residuals_test_df = pd.DataFrame(residuals_test, index=test_stocks.index, columns=test_stocks.columns)\n",
    "\n",
    "# Output the first few rows to verify\n",
    "print(\"Training Residuals:\")\n",
    "print(residuals_train_df.head())\n",
    "\n",
    "print(\"\\nTesting Residuals:\")\n",
    "print(residuals_test_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq6o0QBPateR"
   },
   "source": [
    "---\n",
    "<font color=green>Q27 (1 Mark) :\n",
    "\n",
    "Display all the parameters of the deep neural network.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "yLyuzLkGxrAd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,060</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">252</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,292</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │         \u001b[38;5;34m5,060\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m252\u001b[0m)            │         \u001b[38;5;34m5,292\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,058</span> (121.32 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m31,058\u001b[0m (121.32 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,352</span> (40.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,352\u001b[0m (40.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,706</span> (80.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m20,706\u001b[0m (80.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7121 \n",
      "Training Loss: 0.7107928991317749\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.7311\n",
      "Testing Loss: 0.7310886383056641\n"
     ]
    }
   ],
   "source": [
    "# Print the summary of the model\n",
    "autoencoder.summary()\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "train_loss = autoencoder.evaluate(z_hat_train, z_hat_train)\n",
    "print(\"Training Loss:\", train_loss)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "test_loss = autoencoder.evaluate(z_hat_test, z_hat_test)\n",
    "print(\"Testing Loss:\", test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer dense_4 parameters:\n",
      "Parameter 1: shape=(252, 20)\n",
      "[[-0.02079698 -0.08029602  0.02048646 ...  0.06541379  0.0585312\n",
      "   0.04465701]\n",
      " [-0.11479677  0.04092172  0.13644011 ... -0.11095422 -0.01056822\n",
      "  -0.00778408]\n",
      " [-0.12084157 -0.02151833 -0.01658983 ... -0.01212741 -0.12173043\n",
      "  -0.14271016]\n",
      " ...\n",
      " [ 0.12260929  0.02282897  0.02428152 ...  0.13756058  0.09547435\n",
      "   0.06776511]\n",
      " [ 0.00115922  0.13083819 -0.07575481 ... -0.12369613 -0.00642501\n",
      "   0.00854246]\n",
      " [ 0.08665033  0.11284686  0.11662507 ... -0.10251925 -0.02771041\n",
      "  -0.10018809]]\n",
      "Parameter 2: shape=(20,)\n",
      "[ 0.00221739 -0.01543185  0.01330342  0.01587906  0.01856746 -0.00589531\n",
      " -0.00479259  0.01694815  0.01860592 -0.01856731 -0.01366144 -0.01211158\n",
      "  0.0159805  -0.01612386  0.01922866  0.01735154 -0.01830383 -0.0188081\n",
      " -0.01330158 -0.01261664]\n",
      "Layer dense_5 parameters:\n",
      "Parameter 1: shape=(20, 252)\n",
      "[[ 0.1206645   0.10161321  0.04211486 ... -0.04934366 -0.09066212\n",
      "  -0.0396918 ]\n",
      " [-0.0693374  -0.13831618  0.06921664 ...  0.00457862  0.06520031\n",
      "  -0.03523675]\n",
      " [-0.09756935  0.02474296 -0.10237675 ... -0.04729528 -0.02285908\n",
      "  -0.0370387 ]\n",
      " ...\n",
      " [-0.05638397  0.07747176 -0.15203574 ... -0.03630167 -0.10831457\n",
      "  -0.02141962]\n",
      " [-0.13219447  0.08405471 -0.03408089 ... -0.09747484 -0.10351233\n",
      "   0.11735541]\n",
      " [-0.09017031  0.09687277 -0.02905671 ... -0.03998437  0.13009022\n",
      "   0.09760374]]\n",
      "Parameter 2: shape=(252,)\n",
      "[ 0.01808791  0.01949105  0.01958216 -0.01915761 -0.01878596 -0.01852364\n",
      " -0.01332459  0.01763821 -0.01157135 -0.01840782  0.01803198 -0.00720412\n",
      "  0.00923662  0.00853492 -0.01558798 -0.01967614  0.00706279  0.00409396\n",
      " -0.01942884 -0.01901998  0.01910389  0.01587761 -0.00155131 -0.01934239\n",
      " -0.01833809 -0.01550342  0.01911886 -0.01406185 -0.0191636   0.0115505\n",
      " -0.01703159 -0.0102909   0.01459941 -0.01791471  0.0174453   0.01960299\n",
      " -0.0147921  -0.00959087 -0.01963774 -0.01853123 -0.0061797   0.01813389\n",
      " -0.01857311 -0.01947121  0.01626639  0.01953597 -0.01256397 -0.01667483\n",
      " -0.01429261  0.01896103 -0.01939269  0.01887016  0.01830345 -0.01976249\n",
      "  0.01797352 -0.01706218 -0.01802187 -0.01941042  0.01303955 -0.0181499\n",
      "  0.0083879   0.01992743 -0.01320894  0.00184356  0.01922254 -0.00934188\n",
      " -0.01541714 -0.01808169 -0.01642829  0.01989239  0.01543711  0.01124489\n",
      "  0.02014659 -0.00017337  0.01403063  0.00778385 -0.00686322 -0.01781555\n",
      "  0.01110885  0.01358338  0.02012778 -0.00978235 -0.0108073   0.0189042\n",
      " -0.00565373 -0.00803451 -0.01669766 -0.01603048 -0.01800529  0.01961334\n",
      " -0.01972804 -0.019933    0.01898786 -0.01895783  0.01981486 -0.01919534\n",
      " -0.01954092 -0.01765235  0.01430724  0.00962619 -0.00968779  0.01981526\n",
      " -0.01884073  0.01734577  0.01521366  0.02003776 -0.00092838 -0.02018818\n",
      " -0.00779033 -0.019936   -0.01810298 -0.01726972 -0.00060031  0.01993551\n",
      " -0.01791051 -0.02016483  0.01971853 -0.01947487  0.0174792  -0.01879768\n",
      " -0.01965708 -0.01949314  0.00737057 -0.00904816 -0.02001776  0.01837622\n",
      "  0.01740014 -0.01485943 -0.01558924 -0.01877702  0.01983015 -0.01477067\n",
      "  0.01861187  0.00900348  0.01936669  0.00629928 -0.01826367 -0.01762262\n",
      "  0.01856011 -0.00973171 -0.01971152 -0.01890334 -0.01928197 -0.016908\n",
      "  0.01795073 -0.01974534 -0.01919866 -0.01893405  0.01968677  0.01941074\n",
      "  0.01900599  0.01909812  0.01887521 -0.01402336 -0.01467372  0.00682015\n",
      " -0.01905514  0.02000682 -0.01524503  0.01955335 -0.0107714  -0.0077643\n",
      "  0.00954005  0.01634171 -0.01931748  0.00604087 -0.01938035 -0.02000104\n",
      " -0.02009459  0.00351223 -0.00027822  0.01943055 -0.0187951  -0.01865294\n",
      " -0.0190585   0.01540748 -0.00761235  0.01975631  0.01873423  0.01959343\n",
      " -0.00668151 -0.01899464  0.01765867  0.01254283 -0.01953909  0.01546643\n",
      "  0.0167352   0.01964368  0.01489619 -0.01804248 -0.01969718 -0.01883476\n",
      " -0.01943192 -0.0186958  -0.01903243  0.01896972 -0.01915388  0.01326448\n",
      "  0.00792569  0.01504651 -0.01940027 -0.01858673  0.01941618  0.01920613\n",
      "  0.01800398  0.0190688  -0.01648333  0.01959556 -0.00218115  0.01680711\n",
      " -0.01398235 -0.01929707  0.01915298  0.0070579  -0.01960466  0.00799742\n",
      " -0.0054911   0.00585391 -0.01083483 -0.01817904 -0.01911061  0.01919246\n",
      "  0.01780882 -0.01813399 -0.00230978  0.0170057   0.01813541 -0.01308657\n",
      " -0.01193347  0.00530766 -0.01193423  0.01875559  0.0193346  -0.0159342\n",
      " -0.0188324   0.01320852  0.01877054 -0.01093348  0.00416879  0.0145597\n",
      " -0.01828131 -0.018649   -0.01813809  0.01208296 -0.00630916  0.01867612\n",
      "  0.01947427 -0.01281497 -0.01898681 -0.01651406  0.01877039 -0.01861501]\n"
     ]
    }
   ],
   "source": [
    "# Show all parameters of the deep neural network\n",
    "for layer in autoencoder.layers:\n",
    "    layer_params = layer.get_weights()\n",
    "    print(f\"Layer {layer.name} parameters:\")\n",
    "    for i, param in enumerate(layer_params):\n",
    "        print(f\"Parameter {i + 1}: shape={param.shape}\")\n",
    "        print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lNBOw3ait03"
   },
   "source": [
    "---\n",
    "<font color=green>Q28: (3 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Train your model using the Adam optimizer for 20 epochs with a batch size equal to 8 and validation split to 20%. Specify the loss function you've chosen.\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "IBUtUlxIxsrx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aads/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.9035 - val_loss: 0.8537\n",
      "Epoch 2/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8640 - val_loss: 0.8251\n",
      "Epoch 3/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8139 - val_loss: 0.7923\n",
      "Epoch 4/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7798 - val_loss: 0.7570\n",
      "Epoch 5/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7423 - val_loss: 0.7203\n",
      "Epoch 6/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6998 - val_loss: 0.6864\n",
      "Epoch 7/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6595 - val_loss: 0.6600\n",
      "Epoch 8/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6481 - val_loss: 0.6419\n",
      "Epoch 9/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6245 - val_loss: 0.6283\n",
      "Epoch 10/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6215 - val_loss: 0.6177\n",
      "Epoch 11/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5971 - val_loss: 0.6098\n",
      "Epoch 12/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6002 - val_loss: 0.6017\n",
      "Epoch 13/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5942 - val_loss: 0.5941\n",
      "Epoch 14/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5669 - val_loss: 0.5878\n",
      "Epoch 15/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5709 - val_loss: 0.5841\n",
      "Epoch 16/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5623 - val_loss: 0.5813\n",
      "Epoch 17/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5619 - val_loss: 0.5791\n",
      "Epoch 18/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5445 - val_loss: 0.5773\n",
      "Epoch 19/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5558 - val_loss: 0.5751\n",
      "Epoch 20/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5394 - val_loss: 0.5731\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,060</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">252</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,292</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │         \u001b[38;5;34m5,060\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m252\u001b[0m)            │         \u001b[38;5;34m5,292\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,058</span> (121.32 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m31,058\u001b[0m (121.32 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,352</span> (40.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,352\u001b[0m (40.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,706</span> (80.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m20,706\u001b[0m (80.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and compile the autoencoder model\n",
    "input_dim = X_train.shape[1]\n",
    "autoencoder = create_autoencoder(input_dim)\n",
    "autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = autoencoder.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=20,\n",
    "    batch_size=8,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xI8O24djAYO"
   },
   "source": [
    "---\n",
    "<font color=green>Q29: (3 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Predict using the testing set and extract the residuals based on the methodology described in **[End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture](https://arxiv.org/pdf/2402.08233.pdf)**.\n",
    "for 'NVDA' stock.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residuals for 'NVDA' stock:\n",
      "[-4.42358715e-02  3.16490918e-01 -1.16300059e-01  4.30527993e-02\n",
      " -6.81263099e-01 -1.10860710e+00  2.90687046e-01  2.53290708e-01\n",
      " -5.53455764e-01 -6.08467108e-01 -8.82560211e-01  4.85639025e-01\n",
      "  2.05715523e-01  7.90638103e-01  1.32210626e-01 -1.71857322e+00\n",
      " -3.18312800e-01 -6.31756497e-01 -9.14255270e-01  5.04220467e-01\n",
      " -2.29492089e-02  2.29032416e-02  9.89274777e-01 -9.39776648e-01\n",
      " -2.94232476e-01 -5.69367564e-01  8.28468578e-01  6.07920765e-01\n",
      " -7.63215686e-01 -2.89984319e-01 -2.12847363e-01 -5.10609919e-01\n",
      " -1.00535489e-01  1.59970186e-01  6.06814562e-01  1.13923089e+00\n",
      " -6.46688781e-01 -7.20368181e-01 -8.22229310e-01 -1.89728179e-01\n",
      "  2.39672822e+00  1.17298195e-01  7.39184618e-01 -2.00679347e+00\n",
      "  1.44439069e+00 -6.72946069e-01 -4.91156671e-01 -9.16704428e-01\n",
      " -7.02077193e-01  4.22387006e-01  4.91832296e-02  5.91413428e-01\n",
      "  1.23163975e+00  1.03353239e+00 -5.55579454e-01 -1.20084956e-01\n",
      "  1.20603619e+00 -4.87149475e-01 -4.48976196e-01 -4.32271311e-01\n",
      " -1.20024485e+00  6.76387983e-01 -8.05231285e-01 -4.45248235e-01\n",
      "  4.83737608e-01  2.04494291e-01  2.11861478e-01  1.74158537e-01\n",
      "  4.46830913e-01 -2.53617970e-01 -3.53197851e-01  6.08488245e-01\n",
      "  1.76167393e+00 -1.46993970e-01  5.31742778e-03  7.86067244e-01\n",
      " -4.05005370e-01 -5.79168734e-01 -1.38444906e+00  3.88208179e-01\n",
      "  7.10078338e-01  1.91956506e-01  3.30506370e-01  4.14015201e-02\n",
      " -2.68330757e-01 -1.88962610e-01 -1.30426594e+00 -2.06874088e-01\n",
      " -5.84774293e-02  3.14559352e-01 -6.44204941e-01 -1.72271318e+00\n",
      " -1.19892778e-01 -1.34863861e+00  2.01771829e+00  2.25619532e-01\n",
      " -1.50838639e-01  2.95657176e-01 -2.27420793e-02  2.21171505e+00\n",
      " -1.03452796e+00  6.45616449e-01  2.95913056e-01 -1.47661125e+00\n",
      " -1.28590169e-01  1.10708097e+00 -5.80436819e-02 -2.39097830e-01\n",
      " -1.28975535e+00 -4.49485318e-01 -1.24696310e+00 -4.92836970e-01\n",
      " -5.92335441e-01 -3.27592939e-01  2.35696238e-03 -1.06004759e-01\n",
      " -7.94011350e-02 -1.06290174e+00 -3.94956942e-02 -7.82146385e-01\n",
      " -1.18209759e+00 -7.17356379e-01  4.28084867e-02  2.51024767e-01\n",
      " -5.34715870e-01  2.48820306e-01 -5.92356382e-02  2.08470360e-01\n",
      "  5.05371213e-01 -9.10833319e-01  4.07216326e-01  8.31671802e-01\n",
      "  6.17692393e-01 -3.29248373e-01 -3.45683359e-02  3.06965694e-01\n",
      "  3.64447421e-02 -7.13167480e-01 -4.18862440e-01 -1.55647682e+00\n",
      " -1.25471804e+00  6.85292570e-02 -3.83038138e-01  1.12943694e+00\n",
      "  1.52624185e-01 -1.18290422e+00 -1.12174640e+00 -1.06318397e-01\n",
      "  1.64781594e-01 -6.07463890e-01  1.07103321e+00  5.54481444e-01\n",
      "  5.45241475e-01  4.83196290e-01 -3.78916942e-01  1.87586687e-02\n",
      "  4.07430152e-01  6.66553544e-01  1.65740176e-01 -7.66247596e-02\n",
      " -6.46977187e-01  1.91298628e-01 -2.21192957e-02  6.19854689e-02\n",
      " -8.30417185e-01 -1.43326491e+00 -1.28876759e+00  2.39629289e-01\n",
      " -6.74043079e-01  1.06694285e-01 -1.20485208e+00 -4.52028798e-01\n",
      " -9.50613544e-01  8.10267478e-01 -9.57048413e-01  9.81647045e-02\n",
      "  4.91390582e-01 -9.90700360e-01  4.06449751e-01  7.87251602e-02\n",
      " -1.09591879e-01 -2.55495784e-01  5.25386974e-01 -8.85118905e-01\n",
      " -1.42925831e+00 -2.69659283e-01 -2.43222515e-01 -9.69104496e-02\n",
      "  1.36721381e-01 -4.70025498e-01 -3.11471234e-01 -3.38062493e-01\n",
      " -2.79141826e-01 -2.11830858e-02  6.11760238e-01  1.41605540e+00\n",
      "  1.29731846e-01  4.47055136e-01 -1.26550227e-01  1.46765512e-01\n",
      "  4.32231506e-01 -3.52998366e-02  1.06821319e-01  9.74475523e-01\n",
      " -3.55391708e-01  1.52245513e-01  2.29455028e-01  8.09846327e-02\n",
      "  1.67495610e-01  2.42308105e-01  2.11865490e-01 -4.61454399e-01\n",
      "  1.05288710e-01  1.67275101e+00  1.16497385e+00 -5.39631434e-01\n",
      "  3.61632556e-01 -3.50200429e-01  3.64571248e-01 -1.63204156e-01\n",
      "  9.64269209e-02  1.89242711e-01 -3.72051265e-01  1.06097224e-01\n",
      " -1.19672847e+00 -1.12910014e+00  2.64544916e+00 -1.07726004e-01\n",
      "  7.42561889e-02 -1.64231629e-01 -2.44108597e-01  1.42497264e-01\n",
      "  1.06928661e+00  9.52443123e-01  6.69931364e-01  4.37554666e-01\n",
      "  1.05333808e+00 -1.57759658e+00 -7.71471128e-01  1.93230650e+00\n",
      " -4.31438033e-01 -9.35465706e-01  2.30004847e-01 -2.18933719e-01\n",
      "  4.14744893e-02  2.61544228e-01  7.93433099e-02  1.14748373e+00\n",
      "  2.03916679e-01 -1.29028505e+00 -1.48462737e+00  1.22508924e-01]\n"
     ]
    }
   ],
   "source": [
    "# Extract the residuals for NVDA stock\n",
    "nvda_actual = test_stocks.loc['NVDA'].values\n",
    "nvda_pred = X_test_pred[test_stocks.index.get_loc('NVDA')]\n",
    "nvda_residuals = nvda_actual - nvda_pred\n",
    "\n",
    "# Output the residuals\n",
    "print(\"Residuals for 'NVDA' stock:\")\n",
    "print(nvda_residuals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Select the date range with a daily frequency\n",
    "dates = pd.date_range(start='2023-03-29', periods=len(nvda_residuals), freq='B')\n",
    "\n",
    "# Plot the residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dates, nvda_residuals, label='Residuals', color='blue')\n",
    "plt.axhline(0, color='red', linestyle='--', linewidth=0.8)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Residual')\n",
    "plt.title('Residuals for NVDA Stock')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYyVOZu90CIK"
   },
   "source": [
    "<font color=green>Q30: (7 Marks) </font>\n",
    "<br><font color='green'>\n",
    "By reading carefully the paper **[End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture](https://arxiv.org/pdf/2402.08233.pdf)**, answers the following question:\n",
    "1. **Summarize the Key Actions**: Highlight the main experiments and methodologies employed by the authors in Section 5.\n",
    "2. **Reproduction Steps**: Detail the necessary steps required to replicate the authors' approach based on the descriptions provided in the paper.\n",
    "3. **Proposed Improvement**: Suggest one potential enhancement to the methodology that could potentially increase the effectiveness or efficiency of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "na95zQB40hRG"
   },
   "source": [
    "## Key Actions: Main Experiments and Methodologies in Section 5\n",
    "\n",
    "In the paper titled \"End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture,\" Section 5 delves into the core experimental practices and methodologies the researchers adopted. This section is important in understanding how different autoencoder configurations influence the generation of residuals, which are crucial for the proposed trading strategy.\n",
    "\n",
    "### Model Variants\n",
    "The research team has tested various autoencoder configurations to identify the most effective model structure for residual generation. Key variations explored include:\n",
    "- Utilisation of different activation functions, mainly TANH and RELU, to determine their impact on model performance.\n",
    "- Examination of the effects of including or excluding biases within the network layers.\n",
    "- Implementation of dropout layers aimed at preventing overfitting and promoting model robustness.\n",
    "- Adjustments in the architecture, such as changes in the number of hidden layers and nodes, to optimize processing capabilities.\n",
    "\n",
    "### Residual Extraction\n",
    "The methodologies for extracting residuals were diverse, incorporating:\n",
    "- A straightforward approach where predicted standardized returns were subtracted from actual standardized returns.\n",
    "- An advanced method utilizing the encoder portion of the autoencoder to derive factor returns from the preceding 60 days. These were then regressed against actual returns to estimate factor loadings, with predicted factor returns subtracted from actual returns to derive residuals.\n",
    "- A modification of the second method, where returns were scaled by volatility to enhance the model’s sensitivity to market dynamics.\n",
    "\n",
    "### Training and Evaluation\n",
    "The models underwent rigorous training using standardized returns truncated at three standard deviations to minimize the influence of outliers. The training protocol included:\n",
    "- A specified lookback period of Tτ trading days, leveraging historical data for model refinement.\n",
    "- Utilization of the Adam optimizer across 10 epochs aimed at optimizing the weight configurations for ideal performance.\n",
    "\n",
    "### Backtesting Methodology\n",
    "The backtesting of the models was meticulously planned:\n",
    "- Residuals served as inputs for extracting Ornstein-Uhlenbeck (OU) parameters through a 60-day regression analysis.\n",
    "- The training incorporated 1,000 days of historical data, fine-tuned to maximize the Sharpe ratio.\n",
    "- An additional 125 days were reserved for out-of-sample testing to evaluate model performance and readiness for subsequent retraining cycles.\n",
    "\n",
    "### Performance Evaluation Metrics\n",
    "The effectiveness of the models was gauged using:\n",
    "- The Sharpe ratio to assess risk-adjusted returns.\n",
    "- Cumulative returns to measure overall performance over time, providing insights into the profitability of the trading strategies derived from the autoencoder residuals.\n",
    "\n",
    "## Reproduction Steps\n",
    "\n",
    "For researchers and practitioners interested in replicating the study, the following detailed steps are outlined:\n",
    "\n",
    "1. Data Preparation:\n",
    "   - Gather and preprocess historical stock return data, ensuring standardization and capping anomalies at three standard deviations for consistency.\n",
    "\n",
    "2. Model Setup:\n",
    "   - Select an appropriate autoencoder configuration such as one featuring TANH activation, bias inclusion, no dropout, and a singular hidden layer comprising 20 nodes.\n",
    "   \n",
    "   *Encoder*\n",
    "   $$\n",
    "   F=\\operatorname{enc}(Z)=\\operatorname{f_e}\\left(W^{(0)} Z+b^{(0)}\\right)\n",
    "   $$\n",
    "\n",
    "   *Decoder*\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   \\hat{Z} & =\\operatorname{dec}(F) \\\\\n",
    "   & =\\operatorname{f_d} \\left(W^{(1)} F+b^{(1)}\\right) \\\\\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "\n",
    "3. Training Phase:\n",
    "   - Execute the training of the autoencoder using the standardized returns, applying the Adam optimizer and adhering to a Tτ trading day (252 in the paper), with capped at 3 standard deviations lookback period over 10 epochs.\n",
    "\n",
    "4. Residual Calculation:\n",
    "   - Compute residuals directly from the return data or via an advanced encoder-based factor extraction followed by regression analysis.\n",
    "\n",
    "5. Parameter Estimation:\n",
    "   - Conduct a 60-day regression to derive OU parameters from the residuals, using a dataset spanning 1,000 days optimized against a Sharpe ratio target, followed by a performance evaluation over the next 125 days.\n",
    "\n",
    "6. Backtesting and Evaluation:\n",
    "   - Implement backtesting strategies based on the residuals and evaluate the outcomes using the Sharpe ratio and cumulative returns to validate the approach.\n",
    "\n",
    "## Proposed Improvement\n",
    "\n",
    "To further enhance the methodology, the integration of ensemble learning techniques is recommended. This approach involves:\n",
    "- Model Diversity: Training multiple autoencoder models with varied configurations to explore a broader spectrum of predictive capabilities.\n",
    "- Aggregation: Aggregating the predictions from these models using averaging, voting, or stacking methods to formulate a more robust and consistent output.\n",
    "- Regularization: Applying advanced regularization techniques to ensure that the ensemble model remains generalizable to new, unseen data.\n",
    "\n",
    "Adopting ensemble learning could significantly refine the predictive accuracy of the autoencoder, thereby improving the generation of residuals and enhancing the efficacy of the derived trading strategies. This adjustment helps mitigate potential weaknesses inherent in individual models, resulting in a more stable and reliable system performance.\n",
    "\n",
    "Minimizing model turnover is a priority, which can be accomplished by retraining the model less frequently or adjusting the Sharpe policy to slow down the model. Additionally, we recommend adding an extra layer of smoothing during portfolio construction after obtaining weights from the Autoencoder. This could involve using advanced time series prediction methods, such as LSTMs or Transformers, trained appropriately. Transaction cost penalties should also be considered. We have reviewed the paper by Bryan Kelly, and all, which proposes an objective that includes transaction costs. In the future, we aim to replicate this paper to create an objective that is more realistic and aligns with the investor's needs.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
